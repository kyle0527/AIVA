"""
BioNeuron Core V2 - å®Œæ•´ RAG èˆ‡å·¥å…·æ•´åˆç‰ˆæœ¬
æä¾›ç”Ÿç‰©å•Ÿç™¼å¼ç¥ç¶“æ±ºç­–ç¶²è·¯,æ•´åˆå¯¦éš›çš„ RAG æª¢ç´¢èˆ‡ç¨‹å¼æ“ä½œèƒ½åŠ›
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import numpy as np

if TYPE_CHECKING:
    pass


class BiologicalSpikingLayer:
    """æ¨¡æ“¬ç”Ÿç‰©å°–å³°ç¥ç¶“å…ƒè¡Œç‚ºçš„ç¥ç¶“å±¤."""

    def __init__(self, input_size: int, output_size: int) -> None:
        """åˆå§‹åŒ–å°–å³°ç¥ç¶“å±¤.

        Args:
            input_size: è¼¸å…¥ç¶­åº¦
            output_size: è¼¸å‡ºç¶­åº¦
        """
        self.weights = np.random.randn(input_size, output_size) * np.sqrt(
            2.0 / input_size
        )
        self.threshold = 1.0
        self.refractory_period = 0.1
        self.last_spike_time = np.zeros(output_size) - self.refractory_period
        self.params = input_size * output_size

    def forward(self, x: np.ndarray) -> np.ndarray:
        """å‰å‘å‚³æ’­,ç”¢ç”Ÿå°–å³°è¨Šè™Ÿ.

        Args:
            x: è¼¸å…¥è¨Šè™Ÿ

        Returns:
            å°–å³°è¼¸å‡º (0 æˆ– 1)
        """
        import time

        current_time = time.time()
        potential = np.dot(x, self.weights)
        can_spike = (current_time - self.last_spike_time) > self.refractory_period
        spikes = (potential > self.threshold) & can_spike

        self.last_spike_time[spikes] = current_time
        return spikes.astype(np.float32)


class AntiHallucinationModule:
    """æŠ—å¹»è¦ºæ¨¡çµ„,ç¢ºä¿æ±ºç­–çš„å¯é æ€§."""

    def __init__(self, confidence_threshold: float = 0.7) -> None:
        """åˆå§‹åŒ–æŠ—å¹»è¦ºæ¨¡çµ„.

        Args:
            confidence_threshold: ä¿¡å¿ƒåº¦é–¾å€¼
        """
        self.confidence_threshold = confidence_threshold

    def check(self, decision_logits: np.ndarray) -> tuple[bool, float]:
        """æª¢æŸ¥æ±ºç­–çš„ä¿¡å¿ƒåº¦.

        Args:
            decision_logits: æ±ºç­–é‚è¼¯å€¼

        Returns:
            (æ˜¯å¦é€šéæª¢æŸ¥, ä¿¡å¿ƒåº¦åˆ†æ•¸)
        """
        confidence = float(np.max(decision_logits))
        passed = confidence >= self.confidence_threshold
        return passed, confidence


class ScalableBioNet:
    """å¯æ“´å±•çš„ç”Ÿç‰©ç¥ç¶“ç¶²è·¯ (ç´„ 500è¬åƒæ•¸)."""

    def __init__(self, input_size: int, num_tools: int) -> None:
        """åˆå§‹åŒ–æ±ºç­–ç¶²è·¯.

        Args:
            input_size: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            num_tools: å¯ç”¨å·¥å…·æ•¸é‡
        """
        self.fc1 = np.random.randn(input_size, 2048) * np.sqrt(2.0 / input_size)
        self.spiking_layer = BiologicalSpikingLayer(2048, 1024)
        self.fc2 = np.random.randn(1024, num_tools) * np.sqrt(2.0 / 1024)

        total_params = (
            input_size * 2048 + self.spiking_layer.params + 1024 * num_tools
        )
        print(f"[ScalableBioNet] ç¸½åƒæ•¸é‡: {total_params:,}")

    def forward(self, x: np.ndarray) -> np.ndarray:
        """å‰å‘å‚³æ’­.

        Args:
            x: è¼¸å…¥å‘é‡

        Returns:
            å·¥å…·é¸æ“‡çš„é‚è¼¯å€¼
        """
        # ç¬¬ä¸€å±¤å…¨é€£æ¥
        h1 = np.dot(x, self.fc1)
        h1 = np.maximum(0, h1)  # ReLU

        # å°–å³°ç¥ç¶“å±¤
        h2 = self.spiking_layer.forward(h1)

        # è¼¸å‡ºå±¤
        output = np.dot(h2, self.fc2)
        return output


class BioNeuronRAGAgent:
    """ç”Ÿç‰©ç¥ç¶“å…ƒ RAG ä»£ç†,æ•´åˆçŸ¥è­˜æª¢ç´¢èˆ‡å¯¦éš›å·¥å…·åŸ·è¡Œ."""

    def __init__(self, codebase_path: str) -> None:
        """åˆå§‹åŒ– RAG ä»£ç†.

        Args:
            codebase_path: ç¨‹å¼ç¢¼åº«è·¯å¾‘
        """
        from .knowledge_base import KnowledgeBase
        from .tools import (
            CodeAnalyzer,
            CodeReader,
            CodeWriter,
            CommandExecutor,
            ScanTrigger,
            VulnerabilityDetector,
        )

        self.codebase_path = codebase_path
        print("\n" + "="*60)
        print("   BioNeuronRAGAgent åˆå§‹åŒ–")
        print("="*60)
        print(f"ç¨‹å¼ç¢¼åº«: {codebase_path}")

        # åˆå§‹åŒ–çŸ¥è­˜åº«ä¸¦ç´¢å¼•
        print("\n[1/3] æ­£åœ¨ç´¢å¼•ç¨‹å¼ç¢¼åº«...")
        self.knowledge_base = KnowledgeBase(codebase_path)
        self.knowledge_base.index_codebase()

        # åˆå§‹åŒ–å·¥å…·ç³»çµ±
        print("\n[2/3] æ­£åœ¨åˆå§‹åŒ–å·¥å…·ç³»çµ±...")
        self.tool_instances = {
            "CodeReader": CodeReader(codebase_path),
            "CodeWriter": CodeWriter(codebase_path),
            "CodeAnalyzer": CodeAnalyzer(codebase_path),
            "ScanTrigger": ScanTrigger(),
            "XSSDetector": VulnerabilityDetector(),
            "SQLiDetector": VulnerabilityDetector(),
            "SSRFDetector": VulnerabilityDetector(),
            "IDORDetector": VulnerabilityDetector(),
            "CommandExecutor": CommandExecutor(codebase_path),
        }

        self.tools = [
            {"name": "CodeReader", "description": "è®€å–ç¨‹å¼ç¢¼æª”æ¡ˆå…§å®¹"},
            {"name": "CodeWriter", "description": "å¯«å…¥æˆ–ä¿®æ”¹ç¨‹å¼ç¢¼æª”æ¡ˆ"},
            {"name": "CodeAnalyzer", "description": "åˆ†æç¨‹å¼ç¢¼çµæ§‹èˆ‡å“è³ª"},
            {"name": "ScanTrigger", "description": "è§¸ç™¼æ¼æ´æƒæä»»å‹™"},
            {"name": "XSSDetector", "description": "åŸ·è¡Œ XSS æ¼æ´æª¢æ¸¬"},
            {"name": "SQLiDetector", "description": "åŸ·è¡Œ SQL æ³¨å…¥æª¢æ¸¬"},
            {"name": "SSRFDetector", "description": "åŸ·è¡Œ SSRF æ¼æ´æª¢æ¸¬"},
            {"name": "IDORDetector", "description": "åŸ·è¡Œ IDOR æ¼æ´æª¢æ¸¬"},
            {"name": "CommandExecutor", "description": "åŸ·è¡Œç³»çµ±å‘½ä»¤"},
        ]

        print(f"   å·²è¼‰å…¥ {len(self.tools)} å€‹å·¥å…·")

        # åˆå§‹åŒ–æ±ºç­–æ ¸å¿ƒ
        print("\n[3/3] æ­£åœ¨åˆå§‹åŒ–ç”Ÿç‰©ç¥ç¶“æ±ºç­–æ ¸å¿ƒ...")
        self.decision_core = ScalableBioNet(
            input_size=512,
            num_tools=len(self.tools),
        )
        self.anti_hallucination = AntiHallucinationModule(confidence_threshold=0.7)

        # æ­·å²è¨˜éŒ„
        self.history: list[dict[str, Any]] = []

        print("\nBioNeuronRAGAgent åˆå§‹åŒ–å®Œæˆ!")
        print("="*60 + "\n")

    def invoke(self, query: str, **tool_kwargs: Any) -> dict[str, Any]:
        """åŸ·è¡Œ RAG å¢å¼·çš„æ™ºèƒ½æ±ºç­–èˆ‡å·¥å…·åŸ·è¡Œ.

        Args:
            query: ä½¿ç”¨è€…æŸ¥è©¢æˆ–ä»»å‹™æè¿°
            **tool_kwargs: å‚³éçµ¦å·¥å…·çš„åƒæ•¸ (å¦‚ path, target_url, command ç­‰)

        Returns:
            åŸ·è¡Œçµæœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ”¶åˆ°æ–°ä»»å‹™: {query}")
        print(f"{'='*60}\n")

        # ===== æ­¥é©Ÿ 1: RAG æª¢ç´¢ =====
        print("ğŸ“š [æ­¥é©Ÿ 1/5] RAG çŸ¥è­˜æª¢ç´¢")
        retrieved_chunks = self.knowledge_base.search(query, top_k=3)
        print(f"   âœ“ æª¢ç´¢åˆ° {len(retrieved_chunks)} å€‹ç›¸é—œç¨‹å¼ç¢¼ç‰‡æ®µ:")
        for i, chunk in enumerate(retrieved_chunks, 1):
            print(
                f"     {i}. {chunk['path']} - {chunk['name']} (åˆ†æ•¸: {chunk['score']})"
            )

        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_str = "\n\n".join(
            f"# {chunk['path']} - {chunk['name']}\n{chunk['content'][:300]}..."
            for chunk in retrieved_chunks
        )

        # ===== æ­¥é©Ÿ 2: ç·¨ç¢¼ =====
        print("\nğŸ”¢ [æ­¥é©Ÿ 2/5] æŸ¥è©¢èˆ‡ä¸Šä¸‹æ–‡ç·¨ç¢¼")
        # ç°¡åŒ–ç·¨ç¢¼: åŸºæ–¼æŸ¥è©¢å’Œä¸Šä¸‹æ–‡çš„é›œæ¹Š
        query_hash = sum(ord(c) for c in query) % 1000
        context_hash = sum(ord(c) for c in context_str[:1000]) % 1000
        seed = query_hash + context_hash
        np.random.seed(seed)
        query_embedding = np.random.randn(512).astype(np.float32)
        print(f"   âœ“ ç”Ÿæˆ 512 ç¶­åµŒå…¥å‘é‡ (ç¨®å­: {seed})")

        # ===== æ­¥é©Ÿ 3: ç¥ç¶“æ±ºç­– =====
        print("\nğŸ§  [æ­¥é©Ÿ 3/5] ç”Ÿç‰©ç¥ç¶“ç¶²è·¯æ±ºç­–")
        decision_logits = self.decision_core.forward(query_embedding)
        print(f"   âœ“ æ±ºç­–é‚è¼¯å€¼ç¯„åœ: [{decision_logits.min():.2f}, {decision_logits.max():.2f}]")

        # ===== æ­¥é©Ÿ 4: è¨ˆç®—ä¿¡å¿ƒåº¦ (ä¸å¼·åˆ¶æª¢æŸ¥) =====
        print("\nï¿½ [æ­¥é©Ÿ 4/5] è¨ˆç®—æ±ºç­–ä¿¡å¿ƒåº¦")
        _, confidence = self.anti_hallucination.check(decision_logits)
        print(f"   ä¿¡å¿ƒåº¦: {confidence:.2%}")
        print("   â„¹ï¸  å·²åœç”¨ä¿¡å¿ƒåº¦æª¢æŸ¥,æ‰€æœ‰æ±ºç­–éƒ½æœƒåŸ·è¡Œ")

        # ===== æ­¥é©Ÿ 5: å·¥å…·é¸æ“‡èˆ‡åŸ·è¡Œ =====
        print("\nğŸ”§ [æ­¥é©Ÿ 5/5] å·¥å…·é¸æ“‡èˆ‡åŸ·è¡Œ")
        chosen_tool_index = int(np.argmax(decision_logits))
        chosen_tool = self.tools[chosen_tool_index]
        chosen_tool_name = chosen_tool["name"]
        tool_confidence = float(decision_logits[chosen_tool_index])

        print(f"   âœ“ é¸æ“‡å·¥å…·: {chosen_tool_name}")
        print(f"   âœ“ å·¥å…·æè¿°: {chosen_tool['description']}")
        print(f"   âœ“ é¸æ“‡ä¿¡å¿ƒåº¦: {tool_confidence:.2%}")

        # åŸ·è¡Œå·¥å…·
        tool_instance = self.tool_instances.get(chosen_tool_name)
        if tool_instance:
            print("\n   âš™ï¸  æ­£åœ¨åŸ·è¡Œå·¥å…·...")
            tool_result = tool_instance.execute(**tool_kwargs)
            print(f"   âœ“ å·¥å…·åŸ·è¡Œå®Œæˆ: {tool_result.get('status', 'unknown')}")
        else:
            tool_result = {
                "status": "error",
                "error": f"å·¥å…· {chosen_tool_name} æœªå¯¦ä½œ",
            }
            print("   âŒ å·¥å…·æœªå¯¦ä½œ")

        # æ§‹å»ºéŸ¿æ‡‰
        response = {
            "status": "success",
            "query": query,
            "tool_used": chosen_tool_name,
            "confidence": confidence,
            "context": retrieved_chunks,
            "tool_result": tool_result,
        }
        self.history.append(response)

        print(f"\n{'='*60}")
        print("âœ… ä»»å‹™å®Œæˆ!")
        print(f"{'='*60}\n")

        return response

    def get_history(self) -> list[dict[str, Any]]:
        """ç²å–åŸ·è¡Œæ­·å².

        Returns:
            åŸ·è¡Œæ­·å²åˆ—è¡¨
        """
        return self.history

    def get_knowledge_stats(self) -> dict[str, int]:
        """ç²å–çŸ¥è­˜åº«çµ±è¨ˆ.

        Returns:
            çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š
        """
        return {
            "total_chunks": self.knowledge_base.get_chunk_count(),
            "total_keywords": len(self.knowledge_base.index),
        }
