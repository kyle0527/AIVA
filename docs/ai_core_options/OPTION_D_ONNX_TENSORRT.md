# æ–¹æ¡ˆ Dï¼šONNX + TensorRT (ç”¢æ¥­æ¨™æº–æ¨ç†å„ªåŒ–)

## ğŸ“‹ åŸ·è¡Œæ‘˜è¦

**æ ¸å¿ƒç­–ç•¥**ï¼šä½¿ç”¨ ONNX ä½œç‚ºæ¨¡å‹äº¤æ›æ ¼å¼,åœ¨ Python è¨“ç·´å¾Œå°å‡º,é€šé TensorRT é€²è¡Œæ¨ç†å„ªåŒ–,ç²å¾— GPU åŠ é€Ÿèˆ‡ç”¢æ¥­ç´šæ€§èƒ½ã€‚

**é–‹ç™¼æ™‚é–“**ï¼š2-3 é€±  
**éƒ¨ç½²æ™‚é–“**ï¼š1 é€±  
**é ä¼°æˆæœ¬**ï¼šä¸­é«˜ï¼ˆGPU ç¡¬é«” + æˆæ¬Šè€ƒé‡ï¼‰  
**é¢¨éšªç­‰ç´š**ï¼šâ­â­â­ ä¸­ç­‰

---

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒç›®æ¨™

å°‡è¨“ç·´èˆ‡æ¨ç†åˆ†é›¢,ä½¿ç”¨ç”¢æ¥­æ¨™æº–å·¥å…·éˆ:

```
è¨“ç·´éšæ®µï¼šPython + PyTorch/TensorFlow
    â†“ å°å‡º
ä¸­é–“æ ¼å¼ï¼šONNX (é–‹æ”¾ç¥ç¶“ç¶²è·¯äº¤æ›æ ¼å¼)
    â†“ å„ªåŒ–
æ¨ç†å¼•æ“ï¼šTensorRT (NVIDIA GPU åŠ é€Ÿ)
```

### æŠ€è¡“æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         è¨“ç·´éšæ®µ (é›¢ç·š,ä¸€æ¬¡æ€§)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Python + PyTorch                             â”‚  â”‚
â”‚  â”‚                                                â”‚  â”‚
â”‚  â”‚  class AIVANet(nn.Module):                    â”‚  â”‚
â”‚  â”‚      def __init__(self):                      â”‚  â”‚
â”‚  â”‚          self.fc1 = nn.Linear(512, 2048)      â”‚  â”‚
â”‚  â”‚          self.fc2 = nn.Linear(2048, 1024)     â”‚  â”‚
â”‚  â”‚          self.fc3 = nn.Linear(1024, 20)       â”‚  â”‚
â”‚  â”‚                                                â”‚  â”‚
â”‚  â”‚  # è¨“ç·´æ•¸æ“šæ”¶é›†èˆ‡è¨“ç·´                         â”‚  â”‚
â”‚  â”‚  for x, y in dataloader:                      â”‚  â”‚
â”‚  â”‚      loss = train_step(x, y)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â†“ torch.onnx.export()              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ONNX æ¨¡å‹ (model.onnx)                       â”‚  â”‚
â”‚  â”‚  - æ¬Šé‡: 24 MB                                â”‚  â”‚
â”‚  â”‚  - æ ¼å¼: Protocol Buffers                     â”‚  â”‚
â”‚  â”‚  - å¹³å°ç„¡é—œ                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         æ¨ç†éšæ®µ (åœ¨ç·š,é«˜æ€§èƒ½)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TensorRT å„ªåŒ–å™¨                              â”‚  â”‚
â”‚  â”‚  - å±¤èåˆ (Layer Fusion)                      â”‚  â”‚
â”‚  â”‚  - é‡åŒ– (INT8/FP16)                           â”‚  â”‚
â”‚  â”‚  - æ ¸å¿ƒè‡ªå‹•èª¿å„ª                               â”‚  â”‚
â”‚  â”‚  - å‹•æ…‹ Batch è™•ç†                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TensorRT Engine (.trt)                       â”‚  â”‚
â”‚  â”‚  - å„ªåŒ–å¾Œæ¬Šé‡: ~10 MB (INT8)                  â”‚  â”‚
â”‚  â”‚  - GPU å°ˆç”¨äºŒé€²åˆ¶                             â”‚  â”‚
â”‚  â”‚  - æ¨ç†å»¶é²: 0.1 ms (GPU)                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Python Bindings (pycuda)                     â”‚  â”‚
â”‚  â”‚  def predict(features):                       â”‚  â”‚
â”‚  â”‚      return trt_engine.infer(features)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š æŠ€è¡“è¦æ ¼

### ONNX æ¨¡å‹è¦æ ¼

| å±¬æ€§ | å€¼ |
|------|-----|
| **æ ¼å¼ç‰ˆæœ¬** | ONNX Opset 17 |
| **è¼¸å…¥å½¢ç‹€** | [batch_size, 512] |
| **è¼¸å‡ºå½¢ç‹€** | [batch_size, 20] |
| **åƒæ•¸æ•¸é‡** | 3,166,208 |
| **æ¬Šé‡å¤§å°** | 24 MB (FP32) |
| **ç®—å­æ”¯æŒ** | Linear, Tanh, Softmax |

### TensorRT å„ªåŒ–é¸é …

| å„ªåŒ– | FP32 | FP16 | INT8 |
|------|------|------|------|
| **ç²¾åº¦** | 100% | ~99.5% | ~98% |
| **å¤§å°** | 24 MB | 12 MB | 6 MB |
| **é€Ÿåº¦** | 1x | 2x | 4x |
| **GPU è¦æ±‚** | GTX 1050+ | GTX 1060+ | GTX 1080+ |

### æ€§èƒ½å°æ¯”

| æŒ‡æ¨™ | Python | ONNX Runtime | TensorRT FP32 | TensorRT INT8 |
|------|--------|--------------|---------------|---------------|
| **æ¨ç†å»¶é²** | 0.5 ms | 0.3 ms | 0.1 ms | **0.05 ms** |
| **ååé‡** | 2K/s | 3K/s | 10K/s | **20K/s** |
| **GPU åˆ©ç”¨ç‡** | 20% | 40% | 70% | **90%** |
| **æ‰¹æ¬¡å¤§å°** | 1 | 1-32 | 1-128 | 1-256 |

---

## ğŸ”§ å¯¦æ–½è¨ˆç•«

### éšæ®µ 1ï¼šè¨“ç·´èˆ‡å°å‡º (3 å¤©)

**ä»»å‹™ 1.1ï¼šPyTorch è¨“ç·´è…³æœ¬**
```python
# scripts/train_for_onnx.py

import torch
import torch.nn as nn
import torch.onnx

class AIVANet(nn.Module):
    """ONNX å…¼å®¹çš„ AIVA ç¶²è·¯"""
    
    def __init__(self, input_size=512, num_tools=20):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 2048)
        self.fc2 = nn.Linear(2048, 1024)
        self.fc3 = nn.Linear(1024, num_tools)
    
    def forward(self, x):
        x = torch.tanh(self.fc1(x))
        x = torch.tanh(self.fc2(x))
        x = torch.softmax(self.fc3(x), dim=-1)
        return x

def train_model():
    """è¨“ç·´ä¸¦ä¿å­˜æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AIVANet().to(device)
    
    # è¨“ç·´å¾ªç’° (çœç•¥ç´°ç¯€)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(50):
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            loss = nn.functional.cross_entropy(pred, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    # ä¿å­˜ PyTorch æ¬Šé‡
    torch.save(model.state_dict(), 'models/aiva_trained.pth')
    
    return model

if __name__ == '__main__':
    model = train_model()
    print("è¨“ç·´å®Œæˆ")
```

**ä»»å‹™ 1.2ï¼šå°å‡ºåˆ° ONNX**
```python
# scripts/export_to_onnx.py

import torch
import torch.onnx
from train_for_onnx import AIVANet

def export_onnx():
    """å°‡ PyTorch æ¨¡å‹å°å‡ºç‚º ONNX"""
    
    # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AIVANet().to(device)
    model.load_state_dict(torch.load('models/aiva_trained.pth'))
    model.eval()
    
    # æº–å‚™è™›æ“¬è¼¸å…¥
    dummy_input = torch.randn(1, 512, device=device)
    
    # å°å‡º ONNX
    torch.onnx.export(
        model,
        dummy_input,
        'models/aiva.onnx',
        export_params=True,
        opset_version=17,
        do_constant_folding=True,
        input_names=['features'],
        output_names=['probabilities'],
        dynamic_axes={
            'features': {0: 'batch_size'},
            'probabilities': {0: 'batch_size'}
        }
    )
    
    print("ONNX å°å‡ºæˆåŠŸ: models/aiva.onnx")
    
    # é©—è­‰ ONNX æ¨¡å‹
    import onnx
    onnx_model = onnx.load('models/aiva.onnx')
    onnx.checker.check_model(onnx_model)
    print("ONNX æ¨¡å‹é©—è­‰é€šé")

if __name__ == '__main__':
    export_onnx()
```

**ä»»å‹™ 1.3ï¼šONNX Runtime æ¸¬è©¦**
```python
# scripts/test_onnx_runtime.py

import numpy as np
import onnxruntime as ort

def test_onnx_inference():
    """æ¸¬è©¦ ONNX Runtime æ¨ç†"""
    
    # å‰µå»ºæ¨ç†æœƒè©±
    session = ort.InferenceSession(
        'models/aiva.onnx',
        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
    )
    
    # æº–å‚™è¼¸å…¥
    features = np.random.randn(1, 512).astype(np.float32)
    
    # æ¨ç†
    outputs = session.run(
        ['probabilities'],
        {'features': features}
    )
    
    probs = outputs[0][0]
    print(f"è¼¸å‡ºå½¢ç‹€: {probs.shape}")
    print(f"æ©Ÿç‡å’Œ: {probs.sum():.4f}")
    print(f"æœ€é«˜æ©Ÿç‡å·¥å…·: {probs.argmax()}")
    
    # æ€§èƒ½æ¸¬è©¦
    import time
    n_iter = 10000
    start = time.perf_counter()
    for _ in range(n_iter):
        session.run(['probabilities'], {'features': features})
    elapsed = time.perf_counter() - start
    
    print(f"å¹³å‡æ¨ç†æ™‚é–“: {elapsed/n_iter*1000:.3f} ms")
    print(f"ååé‡: {n_iter/elapsed:.0f} æ¬¡/ç§’")

if __name__ == '__main__':
    test_onnx_inference()
```

### éšæ®µ 2ï¼šTensorRT å„ªåŒ– (5 å¤©)

**ä»»å‹™ 2.1ï¼šå®‰è£ TensorRT**
```bash
# ä¸‹è¼‰ TensorRT (éœ€è¦ NVIDIA å¸³è™Ÿ)
# https://developer.nvidia.com/tensorrt

# Windows å®‰è£
# 1. è§£å£“åˆ° C:\TensorRT-8.6.1
# 2. è¨­ç½®ç’°å¢ƒè®Šæ•¸
$env:Path += ";C:\TensorRT-8.6.1\lib"
$env:TENSORRT_DIR = "C:\TensorRT-8.6.1"

# å®‰è£ Python ç¶å®š
pip install tensorrt

# é©—è­‰å®‰è£
python -c "import tensorrt as trt; print(trt.__version__)"
```

**ä»»å‹™ 2.2ï¼šONNX â†’ TensorRT è½‰æ›**
```python
# scripts/convert_to_tensorrt.py

import tensorrt as trt
import numpy as np

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def build_engine(onnx_path, engine_path, fp16=False, int8=False):
    """å°‡ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT å¼•æ“"""
    
    # å‰µå»º Builder
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # è§£æ ONNX
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            raise RuntimeError("ONNX è§£æå¤±æ•—")
    
    # é…ç½®æ§‹å»ºå™¨
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1 GB
    
    # ç²¾åº¦è¨­ç½®
    if fp16 and builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)
        print("å•Ÿç”¨ FP16 ç²¾åº¦")
    
    if int8 and builder.platform_has_fast_int8:
        config.set_flag(trt.BuilderFlag.INT8)
        # INT8 éœ€è¦æ ¡æº–æ•¸æ“š (çœç•¥å¯¦ç¾)
        print("å•Ÿç”¨ INT8 ç²¾åº¦")
    
    # è¨­ç½®å‹•æ…‹å½¢ç‹€ (å¯é¸)
    profile = builder.create_optimization_profile()
    profile.set_shape(
        'features',
        min=(1, 512),
        opt=(16, 512),
        max=(128, 512)
    )
    config.add_optimization_profile(profile)
    
    # æ§‹å»ºå¼•æ“
    print("é–‹å§‹æ§‹å»º TensorRT å¼•æ“...")
    serialized_engine = builder.build_serialized_network(network, config)
    
    # ä¿å­˜å¼•æ“
    with open(engine_path, 'wb') as f:
        f.write(serialized_engine)
    
    print(f"TensorRT å¼•æ“å·²ä¿å­˜: {engine_path}")

if __name__ == '__main__':
    build_engine(
        'models/aiva.onnx',
        'models/aiva_fp32.trt',
        fp16=False,
        int8=False
    )
    
    build_engine(
        'models/aiva.onnx',
        'models/aiva_fp16.trt',
        fp16=True,
        int8=False
    )
```

**ä»»å‹™ 2.3ï¼šTensorRT æ¨ç†åŒ…è£å™¨**
```python
# aiva_bindings/tensorrt_wrapper.py

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

class TensorRTEngine:
    """TensorRT æ¨ç†å¼•æ“åŒ…è£å™¨"""
    
    def __init__(self, engine_path):
        # è¼‰å…¥å¼•æ“
        self.logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, 'rb') as f:
            runtime = trt.Runtime(self.logger)
            self.engine = runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # æº–å‚™è¼¸å…¥è¼¸å‡ºç·©è¡å€
        self.inputs = []
        self.outputs = []
        self.bindings = []
        self.stream = cuda.Stream()
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding))
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            # åˆ†é… GPU å…§å­˜
            device_mem = cuda.mem_alloc(size * np.dtype(dtype).itemsize)
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({
                    'host': None,
                    'device': device_mem,
                    'size': size,
                    'dtype': dtype
                })
            else:
                host_mem = cuda.pagelocked_empty(size, dtype)
                self.outputs.append({
                    'host': host_mem,
                    'device': device_mem,
                    'size': size,
                    'dtype': dtype
                })
    
    def infer(self, features: np.ndarray) -> np.ndarray:
        """åŸ·è¡Œæ¨ç†"""
        # å°‡è¼¸å…¥æ‹·è²åˆ° GPU
        input_data = features.astype(self.inputs[0]['dtype']).ravel()
        cuda.memcpy_htod_async(
            self.inputs[0]['device'],
            input_data,
            self.stream
        )
        
        # åŸ·è¡Œæ¨ç†
        self.context.execute_async_v2(
            bindings=self.bindings,
            stream_handle=self.stream.handle
        )
        
        # å°‡è¼¸å‡ºæ‹·è²å› CPU
        for output in self.outputs:
            cuda.memcpy_dtoh_async(
                output['host'],
                output['device'],
                self.stream
            )
        
        # åŒæ­¥
        self.stream.synchronize()
        
        # è¿”å›çµæœ
        return self.outputs[0]['host'].copy()
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        for inp in self.inputs:
            if inp['device']:
                inp['device'].free()
        for out in self.outputs:
            if out['device']:
                out['device'].free()
```

### éšæ®µ 3ï¼šAIVA æ•´åˆ (5 å¤©)

**ä»»å‹™ 3.1ï¼šæ ¸å¿ƒé¸æ“‡é‚è¼¯**
```python
# services/core/aiva_core/core.py

class AIVACore:
    def __init__(self, engine_type='python'):
        """
        engine_type: 'python', 'onnx', 'tensorrt'
        """
        self.engine_type = engine_type
        
        if engine_type == 'python':
            from .ai_engine.bio_neuron_core import ScalableBioNet
            self.engine = ScalableBioNet(512, 20)
            logger.info("ä½¿ç”¨ Python BioNeuron æ ¸å¿ƒ")
        
        elif engine_type == 'onnx':
            import onnxruntime as ort
            self.session = ort.InferenceSession(
                'models/aiva.onnx',
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            logger.info("ä½¿ç”¨ ONNX Runtime")
        
        elif engine_type == 'tensorrt':
            from aiva_bindings.tensorrt_wrapper import TensorRTEngine
            self.engine = TensorRTEngine('models/aiva_fp16.trt')
            logger.info("ä½¿ç”¨ TensorRT FP16 å¼•æ“")
        
        else:
            raise ValueError(f"Unknown engine type: {engine_type}")
    
    def select_tool(self, scan_result: dict) -> str:
        """é¸æ“‡å·¥å…·"""
        features = self.feature_extractor.extract(scan_result)
        
        if self.engine_type == 'python':
            probs = self.engine.forward(features)
        
        elif self.engine_type == 'onnx':
            features_np = features.astype(np.float32).reshape(1, -1)
            outputs = self.session.run(['probabilities'], {'features': features_np})
            probs = outputs[0][0]
        
        elif self.engine_type == 'tensorrt':
            features_np = features.astype(np.float32).reshape(1, -1)
            probs = self.engine.infer(features_np)
        
        tool_index = np.argmax(probs)
        return self.tools[tool_index]
```

**ä»»å‹™ 3.2ï¼šé…ç½®ç®¡ç†**
```yaml
# config/ai_core.yaml

ai_core:
  # å¼•æ“é¡å‹: python, onnx, tensorrt
  engine: tensorrt
  
  # ONNX é…ç½®
  onnx:
    model_path: models/aiva.onnx
    providers:
      - CUDAExecutionProvider
      - CPUExecutionProvider
  
  # TensorRT é…ç½®
  tensorrt:
    engine_path: models/aiva_fp16.trt
    precision: fp16  # fp32, fp16, int8
    max_batch_size: 32
    workspace_size: 1073741824  # 1 GB
```

---

## ğŸ“ˆ é æœŸæˆæœ

### æ€§èƒ½æå‡

```
æ¨ç†å»¶é²å°æ¯”ï¼š
Python:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.5 ms
ONNX:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.3 ms
TRT FP32:   â–ˆâ–ˆâ–ˆâ–ˆ 0.1 ms
TRT FP16:   â–ˆâ–ˆ 0.07 ms
TRT INT8:   â–ˆ 0.05 ms  â† 10x åŠ é€Ÿ
```

### éƒ¨ç½²å¤§å°

| æ ¼å¼ | å¤§å° | GPU éœ€æ±‚ |
|------|------|----------|
| **PyTorch (.pth)** | 24 MB | å¯é¸ |
| **ONNX (.onnx)** | 24 MB | å¯é¸ |
| **TensorRT FP32** | 24 MB | å¿…é ˆ |
| **TensorRT FP16** | 12 MB | å¿…é ˆ |
| **TensorRT INT8** | 6 MB | å¿…é ˆ |

### GPU åŠ é€Ÿæ•ˆæœ

```
ååé‡ (æ¬¡/ç§’):
CPU (Python):   â–ˆâ–ˆ 2,000
CPU (ONNX):     â–ˆâ–ˆâ–ˆ 3,000
GPU (TRT FP32): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10,000
GPU (TRT INT8): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20,000
```

---

## ğŸ’° æˆæœ¬åˆ†æ

### ç¡¬é«”æˆæœ¬

| GPU å‹è™Ÿ | åƒ¹æ ¼ | FP32 | FP16 | INT8 |
|----------|------|------|------|------|
| **GTX 1650** | $150 | âœ… | âš ï¸ | âŒ |
| **RTX 3060** | $330 | âœ… | âœ… | âš ï¸ |
| **RTX 4070** | $600 | âœ… | âœ… | âœ… |
| **A100 (é›²)** | $2/hr | âœ… | âœ… | âœ… |

### é–‹ç™¼æˆæœ¬

| éšæ®µ | å·¥æ™‚ | æŠ€èƒ½éœ€æ±‚ | æˆæœ¬ |
|------|------|----------|------|
| è¨“ç·´èˆ‡å°å‡º | 3 å¤© | PyTorch | ä½ |
| TensorRT è½‰æ› | 5 å¤© | CUDA/TRT | ä¸­ |
| AIVA æ•´åˆ | 5 å¤© | Python | ä½ |
| å„ªåŒ–èª¿è©¦ | 2-5 å¤© | GPU èª¿å„ª | ä¸­ |
| **ç¸½è¨ˆ** | **15-18 å¤©** | **å¤šæŠ€èƒ½** | **ä¸­** |

### æˆæ¬Šæˆæœ¬

| çµ„ä»¶ | æˆæ¬Š | å•†ç”¨ |
|------|------|------|
| **ONNX** | Apache-2.0 | âœ… å…è²» |
| **ONNX Runtime** | MIT | âœ… å…è²» |
| **TensorRT** | NVIDIA EULA | âš ï¸ æœ‰é™åˆ¶ |

**TensorRT æˆæ¬Šé™åˆ¶**ï¼š
- é–‹ç™¼/æ¸¬è©¦ï¼šå…è²»
- å•†æ¥­éƒ¨ç½²ï¼šéœ€è©•ä¼°å…·é«”ä½¿ç”¨å ´æ™¯
- é›²ç«¯éƒ¨ç½²ï¼šé€šå¸¸å·²åŒ…å«åœ¨ GPU å¯¦ä¾‹æˆæ¬Šä¸­

---

## âš ï¸ é¢¨éšªè©•ä¼°

### æŠ€è¡“é¢¨éšª

| é¢¨éšª | å¯èƒ½æ€§ | å½±éŸ¿ | ç·©è§£æªæ–½ |
|------|--------|------|----------|
| **GPU ä¾è³´** | é«˜ | ä¸­ | æä¾› CPU å¾Œå‚™ (ONNX) |
| **TensorRT ç‰ˆæœ¬å…¼å®¹** | ä¸­ | ä¸­ | é–å®šç‰¹å®šç‰ˆæœ¬ |
| **é‡åŒ–ç²¾åº¦æå¤±** | ä¸­ | ä½ | å……åˆ†æ¸¬è©¦ INT8 |
| **CUDA ç’°å¢ƒè¤‡é›œ** | ä¸­ | ä¸­ | Docker å®¹å™¨åŒ– |
| **æˆæ¬Šåˆè¦** | ä½ | é«˜ | æ³•å‹™å¯©æŸ¥ |

### éƒ¨ç½²é¢¨éšª

```
å¦‚æœå®¢æˆ¶ç’°å¢ƒç„¡ GPUï¼š
- å›é€€åˆ° ONNX Runtime (CPU)
- æ€§èƒ½ä¸‹é™ 3xï¼Œä½†åŠŸèƒ½æ­£å¸¸
```

---

## ğŸ¯ æˆåŠŸæ¨™æº–

### å¿…é ˆé”æˆ

- âœ… ONNX æ¨¡å‹æ­£ç¢ºå°å‡º
- âœ… TensorRT å¼•æ“æˆåŠŸæ§‹å»º
- âœ… æ¨ç†å»¶é² < 0.2 ms (GPU)
- âœ… GPU åˆ©ç”¨ç‡ > 70%
- âœ… CPU å¾Œå‚™å¯ç”¨

### æœŸæœ›é”æˆ

- âœ… INT8 é‡åŒ–æº–ç¢ºç‡ > 95%
- âœ… æ”¯æŒå‹•æ…‹ Batch
- âœ… è·¨å¹³å° ONNX éƒ¨ç½²
- âœ… Docker ä¸€éµéƒ¨ç½²

### æœ€å¥½é”æˆ

- âœ… å¤š GPU ä¸¦è¡Œ
- âœ… æ¨¡å‹ç†±æ›´æ–°
- âœ… æ¨ç†å»¶é² < 0.1 ms
- âœ… AMD GPU æ”¯æŒ (ROCm)

---

## âœ… çµè«–èˆ‡å»ºè­°

### æ ¸å¿ƒå„ªå‹¢

1. **ç”¢æ¥­æ¨™æº–**ï¼šONNX ç”Ÿæ…‹æˆç†Ÿ
2. **æ¥µè‡´æ€§èƒ½**ï¼šGPU åŠ é€Ÿ 10x+
3. **éˆæ´»éƒ¨ç½²**ï¼šONNX è·¨å¹³å°
4. **è¨“ç·´åˆ†é›¢**ï¼šPython è¨“ç·´ï¼Œå„ªåŒ–æ¨ç†
5. **æˆç†Ÿå·¥å…·éˆ**ï¼šNVIDIA å®˜æ–¹æ”¯æŒ

### æ ¸å¿ƒåŠ£å‹¢

1. **GPU ä¾è³´**ï¼šTensorRT éœ€è¦ NVIDIA GPU
2. **ç’°å¢ƒè¤‡é›œ**ï¼šCUDA/cuDNN/TensorRT å®‰è£
3. **æˆæ¬Šè€ƒé‡**ï¼šTensorRT å•†ç”¨éœ€è©•ä¼°
4. **èª¿è©¦å›°é›£**ï¼šGPU éŒ¯èª¤é›£è¿½è¹¤
5. **æˆæœ¬å¢åŠ **ï¼šç¡¬é«”æŠ•è³‡

### é©ç”¨å ´æ™¯

âœ… **å¤§è¦æ¨¡æ¨ç†éƒ¨ç½²**  
âœ… å·²æœ‰ NVIDIA GPU ç’°å¢ƒ  
âœ… è¿½æ±‚æ¥µè‡´æ¨ç†æ€§èƒ½  
âœ… éœ€è¦è·¨å¹³å°æ¨¡å‹äº¤æ›  
âœ… è¨“ç·´èˆ‡æ¨ç†åˆ†é›¢æ¶æ§‹  

### ä¸é©ç”¨å ´æ™¯

âŒ **ç„¡ GPU ç’°å¢ƒ**  
âŒ å°è¦æ¨¡éƒ¨ç½² (<1000 æ¬¡/å¤©)  
âŒ é–‹ç™¼éšæ®µé »ç¹æ”¹å‹•æ¨¡å‹  
âŒ é ç®—æœ‰é™  
âŒ é¿å… NVIDIA ç”Ÿæ…‹é–å®š  

### æœ€çµ‚å»ºè­°

**æ¨è–¦ä½œç‚ºç¬¬äºŒéšæ®µå„ªåŒ–æ–¹æ¡ˆ**

å»ºè­°è·¯ç·šï¼š
```
ç¬¬ 1 éšæ®µï¼šPython è¨“ç·´é©—è­‰ (3 å¤©)
    â†“
ç¬¬ 2 éšæ®µï¼šå°å‡º ONNXï¼Œéƒ¨ç½²åˆ°ç”Ÿç”¢ (1 é€±)
    â†“ (å¯é¸ï¼Œå¦‚æœéœ€è¦æ¥µè‡´æ€§èƒ½)
ç¬¬ 3 éšæ®µï¼šTensorRT å„ªåŒ– (GPU ç’°å¢ƒ)
```

**é—œéµæ±ºç­–é»**ï¼š
- å¦‚æœæœ‰ GPUï¼šå¼·çƒˆæ¨è–¦ TensorRT (10x åŠ é€Ÿ)
- å¦‚æœç„¡ GPUï¼šä½¿ç”¨ ONNX Runtime (ä»æœ‰ 1.7x åŠ é€Ÿ)
- è¨“ç·´éšæ®µï¼šå§‹çµ‚ä½¿ç”¨ Python (éˆæ´»æ€§)

---

**å ±å‘Šç”Ÿæˆæ™‚é–“**ï¼š2025-11-08  
**ç‰ˆæœ¬**ï¼š1.0  
**ç‹€æ…‹**ï¼šå¾…è©•ä¼°
