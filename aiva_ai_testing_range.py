#!/usr/bin/env python3
"""
AIVA AI ç³»çµ±é¶å ´å¯¦æ¸¬è…³æœ¬

æ¸¬è©¦å®Œæ•´çš„ AI æ ¸å¿ƒç³»çµ±ï¼ŒåŒ…æ‹¬ï¼š
- çµ±ä¸€çš„ AI æ¨¡å‹ç®¡ç†å™¨
- æ€§èƒ½å„ªåŒ–çš„ç¥ç¶“ç¶²è·¯
- è¨“ç·´ç³»çµ±æ•´åˆ
- å¯¦éš›æ±ºç­–å ´æ™¯æ¨¡æ“¬
"""

import asyncio
import sys
import os
import json
import time
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'services', 'core'))

async def test_scenario_1_basic_initialization():
    """å ´æ™¯ 1: åŸºæœ¬ç³»çµ±åˆå§‹åŒ–æ¸¬è©¦"""
    print("ğŸ¯ å ´æ™¯ 1: åŸºæœ¬ç³»çµ±åˆå§‹åŒ–æ¸¬è©¦")
    print("=" * 50)
    
    try:
        from aiva_core.ai_engine import AIModelManager, PerformanceConfig
        
        # å‰µå»ºæ€§èƒ½é…ç½®
        config = PerformanceConfig(
            max_concurrent_tasks=10,
            batch_size=16,
            prediction_cache_size=500,
            use_quantized_weights=True
        )
        
        # åˆå§‹åŒ– AI æ¨¡å‹ç®¡ç†å™¨
        manager = AIModelManager(
            model_dir=Path("./test_models")
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        init_result = await manager.initialize_models(
            input_size=64,
            num_tools=8
        )
        
        print(f"âœ… åˆå§‹åŒ–çµæœ: {init_result['status']}")
        if init_result['status'] == 'success':
            print(f"ğŸ“Š ScalableBioNet åƒæ•¸: {init_result.get('scalable_net_params', 'N/A')}")
            print(f"ğŸ§  RAG Agent å°±ç·’: {init_result.get('bio_agent_ready', False)}")
            print(f"ğŸ“… æ¨¡å‹ç‰ˆæœ¬: {init_result.get('model_version', 'N/A')}")
        else:
            print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {init_result.get('error', 'Unknown error')}")
        
        # ç²å–æ¨¡å‹ç‹€æ…‹
        status = await manager.get_model_status()
        print("\nğŸ“‹ æ¨¡å‹ç‹€æ…‹:")
        for key, value in status.items():
            print(f"  â€¢ {key}: {value}")
        
        return True, manager
        
    except Exception as e:
        print(f"âŒ å ´æ™¯ 1 å¤±æ•—: {e}")
        import traceback
        print(traceback.format_exc())
        return False, None


async def test_scenario_2_decision_making(manager):
    """å ´æ™¯ 2: AI æ±ºç­–èƒ½åŠ›æ¸¬è©¦"""
    print("\nğŸ¯ å ´æ™¯ 2: AI æ±ºç­–èƒ½åŠ›æ¸¬è©¦")
    print("=" * 50)
    
    try:
        # æ¸¬è©¦å ´æ™¯åˆ—è¡¨
        test_queries = [
            "åˆ†æé€™å€‹ç³»çµ±çš„å®‰å…¨æ¼æ´",
            "åŸ·è¡Œæ»²é€æ¸¬è©¦è¨ˆç•«",
            "æª¢æ¸¬ç¶²è·¯ç•°å¸¸è¡Œç‚º",
            "ç”Ÿæˆå®‰å…¨è©•ä¼°å ±å‘Š",
            "å»ºè­°é˜²è­·æªæ–½"
        ]
        
        decision_results = []
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ” æ¸¬è©¦ {i}: {query}")
            
            context = {
                "test_id": i,
                "environment": "test_lab",
                "timestamp": time.time()
            }
            
            # åŸ·è¡Œæ±ºç­–
            start_time = time.time()
            result = await manager.make_decision(query, context, use_rag=False)
            decision_time = time.time() - start_time
            
            if result['status'] == 'success':
                confidence = result['result'].get('confidence', 0.0)
                print(f"  âœ… æ±ºç­–æˆåŠŸ (è€—æ™‚: {decision_time:.3f}s, ä¿¡å¿ƒåº¦: {confidence:.2f})")
                decision_results.append({
                    "query": query,
                    "success": True,
                    "confidence": confidence,
                    "time": decision_time
                })
            else:
                print(f"  âŒ æ±ºç­–å¤±æ•—: {result.get('error', 'Unknown error')}")
                decision_results.append({
                    "query": query,
                    "success": False,
                    "time": decision_time
                })
        
        # çµ±è¨ˆçµæœ
        success_count = sum(1 for r in decision_results if r['success'])
        avg_time = sum(r['time'] for r in decision_results) / len(decision_results)
        avg_confidence = sum(r.get('confidence', 0) for r in decision_results if r['success']) / max(success_count, 1)
        
        print(f"\nğŸ“Š æ±ºç­–æ¸¬è©¦çµ±è¨ˆ:")
        print(f"  â€¢ æˆåŠŸç‡: {success_count}/{len(test_queries)} ({success_count/len(test_queries)*100:.1f}%)")
        print(f"  â€¢ å¹³å‡è€—æ™‚: {avg_time:.3f}s")
        print(f"  â€¢ å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.2f}")
        
        return success_count == len(test_queries)
        
    except Exception as e:
        print(f"âŒ å ´æ™¯ 2 å¤±æ•—: {e}")
        return False


async def test_scenario_3_performance_optimization(manager):
    """å ´æ™¯ 3: æ€§èƒ½å„ªåŒ–æ¸¬è©¦"""
    print("\nğŸ¯ å ´æ™¯ 3: æ€§èƒ½å„ªåŒ–æ¸¬è©¦")
    print("=" * 50)
    
    try:
        from aiva_core.ai_engine import OptimizedScalableBioNet, PerformanceConfig
        import numpy as np
        
        # å‰µå»ºå„ªåŒ–çš„ç¥ç¶“ç¶²è·¯
        config = PerformanceConfig(
            max_concurrent_tasks=20,
            batch_size=32,
            prediction_cache_size=1000,
            use_quantized_weights=True
        )
        
        optimized_net = OptimizedScalableBioNet(
            input_size=64,
            num_tools=8,
            config=config
        )
        
        print("ğŸš€ æ¸¬è©¦æ‰¹æ¬¡é æ¸¬æ€§èƒ½...")
        
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        batch_size = 50
        test_inputs = [np.random.randn(64) for _ in range(batch_size)]
        
        # åŸ·è¡Œæ‰¹æ¬¡é æ¸¬
        start_time = time.time()
        results = await optimized_net.predict_batch(test_inputs)
        batch_time = time.time() - start_time
        
        print(f"âœ… æ‰¹æ¬¡é æ¸¬å®Œæˆ:")
        print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  â€¢ ç¸½è€—æ™‚: {batch_time:.3f}s")
        print(f"  â€¢ å–®å€‹é æ¸¬è€—æ™‚: {batch_time/batch_size*1000:.1f}ms")
        print(f"  â€¢ ååé‡: {batch_size/batch_time:.1f} predictions/s")
        
        # ç²å–æ€§èƒ½çµ±è¨ˆ
        stats = optimized_net.get_performance_stats()
        print(f"\nğŸ“ˆ æ€§èƒ½çµ±è¨ˆ:")
        print(f"  â€¢ ç¸½é æ¸¬æ¬¡æ•¸: {stats['predictions']}")
        print(f"  â€¢ å¹³å‡é æ¸¬æ™‚é–“: {stats['avg_prediction_time']*1000:.1f}ms")
        print(f"  â€¢ å¿«å–å‘½ä¸­ç‡: {stats['spiking_layer_cache']['hit_rate']:.2%}")
        print(f"  â€¢ è¨˜æ†¶é«”ä½¿ç”¨: {stats['memory_usage']['total_mb']:.1f}MB")
        
        # æ¸¬è©¦è¨˜æ†¶é«”å„ªåŒ–
        print(f"\nğŸ§¹ åŸ·è¡Œè¨˜æ†¶é«”å„ªåŒ–...")
        optimization_result = optimized_net.optimize_memory()
        print(f"âœ… è¨˜æ†¶é«”å„ªåŒ–å®Œæˆ:")
        print(f"  â€¢ å¿«å–å·²æ¸…ç©º: {optimization_result['caches_cleared']}")
        print(f"  â€¢ GC å›æ”¶ç‰©ä»¶: {optimization_result['gc_collected']}")
        print(f"  â€¢ å„ªåŒ–å¾Œè¨˜æ†¶é«”: {optimization_result['memory_usage']['total_mb']:.1f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ å ´æ™¯ 3 å¤±æ•—: {e}")
        import traceback
        print(traceback.format_exc())
        return False


async def test_scenario_4_training_integration(manager):
    """å ´æ™¯ 4: è¨“ç·´ç³»çµ±æ•´åˆæ¸¬è©¦"""
    print("\nğŸ¯ å ´æ™¯ 4: è¨“ç·´ç³»çµ±æ•´åˆæ¸¬è©¦")
    print("=" * 50)
    
    try:
        from aiva_core.learning import ScalableBioTrainingConfig
        
        # æº–å‚™è¨“ç·´é…ç½®
        training_config = ScalableBioTrainingConfig(
            learning_rate=0.001,
            epochs=3,  # è¼ƒå°‘çš„ epochs ä»¥å¿«é€Ÿæ¸¬è©¦
            batch_size=16,
            early_stopping_patience=2
        )
        
        print("ğŸ“ é–‹å§‹æ¨¡å‹è¨“ç·´æ¸¬è©¦...")
        
        # æ¨¡æ“¬è¨“ç·´æ•¸æ“š
        training_samples = []
        for i in range(100):
            sample = type('Sample', (), {
                'context': f"test_context_{i}",
                'result': f"test_result_{i}",
                'score': 0.8 + (i % 20) * 0.01  # æ¨¡æ“¬ä¸åŒè³ªé‡çš„æ¨£æœ¬
            })()
            training_samples.append(sample)
        
        # åŸ·è¡Œè¨“ç·´
        start_time = time.time()
        training_result = await manager.train_models(
            training_data=training_samples,
            config=training_config,
            use_experience_samples=False
        )
        training_time = time.time() - start_time
        
        if training_result['status'] == 'success':
            print(f"âœ… è¨“ç·´å®Œæˆ (è€—æ™‚: {training_time:.1f}s):")
            print(f"  â€¢ æ¨¡å‹ç‰ˆæœ¬: {training_result['model_version']}")
            print(f"  â€¢ ä½¿ç”¨æ¨£æœ¬: {training_result['samples_used']}")
            print(f"  â€¢ æ¨¡å‹è·¯å¾‘: {training_result['model_path']}")
            
            # è¨“ç·´çµæœç´°ç¯€
            training_details = training_result['training_results']
            print(f"  â€¢ æœ€çµ‚æå¤±: {training_details['final_loss']:.4f}")
            print(f"  â€¢ æœ€çµ‚æº–ç¢ºç‡: {training_details['final_accuracy']:.4f}")
            print(f"  â€¢ è¨“ç·´è¼ªæ•¸: {training_details['epochs_trained']}")
            
            return True
        else:
            print(f"âŒ è¨“ç·´å¤±æ•—: {training_result.get('error', 'Unknown error')}")
            return False
        
    except Exception as e:
        print(f"âŒ å ´æ™¯ 4 å¤±æ•—: {e}")
        import traceback
        print(traceback.format_exc())
        return False


async def test_scenario_5_stress_test(manager):
    """å ´æ™¯ 5: å£“åŠ›æ¸¬è©¦"""
    print("\nğŸ¯ å ´æ™¯ 5: é«˜è² è¼‰å£“åŠ›æ¸¬è©¦")
    print("=" * 50)
    
    try:
        print("âš¡ åŸ·è¡Œä¸¦ç™¼æ±ºç­–æ¸¬è©¦...")
        
        # å‰µå»ºå¤§é‡ä¸¦ç™¼æ±ºç­–ä»»å‹™
        concurrent_tasks = 50
        tasks = []
        
        for i in range(concurrent_tasks):
            query = f"å®‰å…¨åˆ†æä»»å‹™ #{i+1}"
            context = {"task_id": i+1, "priority": "high"}
            task = manager.make_decision(query, context, use_rag=False)
            tasks.append(task)
        
        # åŸ·è¡Œä¸¦ç™¼æ¸¬è©¦
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # åˆ†æçµæœ
        successful_results = [r for r in results if not isinstance(r, Exception) and r.get('status') == 'success']
        failed_results = [r for r in results if isinstance(r, Exception) or r.get('status') != 'success']
        
        print(f"âœ… ä¸¦ç™¼æ¸¬è©¦å®Œæˆ:")
        print(f"  â€¢ ç¸½ä»»å‹™æ•¸: {concurrent_tasks}")
        print(f"  â€¢ æˆåŠŸä»»å‹™: {len(successful_results)}")
        print(f"  â€¢ å¤±æ•—ä»»å‹™: {len(failed_results)}")
        print(f"  â€¢ æˆåŠŸç‡: {len(successful_results)/concurrent_tasks*100:.1f}%")
        print(f"  â€¢ ç¸½è€—æ™‚: {total_time:.2f}s")
        print(f"  â€¢ å¹³å‡è€—æ™‚: {total_time/concurrent_tasks*1000:.1f}ms/task")
        print(f"  â€¢ ååé‡: {concurrent_tasks/total_time:.1f} tasks/s")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸
        if failed_results:
            print(f"\nâš ï¸  ç™¼ç¾ {len(failed_results)} å€‹å¤±æ•—ä»»å‹™:")
            for i, result in enumerate(failed_results[:3]):  # åªé¡¯ç¤ºå‰3å€‹
                if isinstance(result, Exception):
                    print(f"  â€¢ ç•°å¸¸ {i+1}: {type(result).__name__}: {result}")
                else:
                    print(f"  â€¢ å¤±æ•— {i+1}: {result.get('error', 'Unknown error')}")
        
        return len(successful_results) / concurrent_tasks >= 0.8  # 80% æˆåŠŸç‡ç‚ºé€šéæ¨™æº–
        
    except Exception as e:
        print(f"âŒ å ´æ™¯ 5 å¤±æ•—: {e}")
        return False


async def run_complete_test_suite():
    """åŸ·è¡Œå®Œæ•´çš„é¶å ´æ¸¬è©¦å¥—ä»¶"""
    print("ğŸš€ AIVA AI ç³»çµ±é¶å ´å¯¦æ¸¬é–‹å§‹")
    print("=" * 70)
    print("æ¸¬è©¦ç¯„åœ: AI æ ¸å¿ƒçµ±ä¸€ã€æ€§èƒ½å„ªåŒ–ã€è¨“ç·´æ•´åˆã€å¯¦æˆ°å ´æ™¯")
    print("=" * 70)
    
    test_results = []
    manager = None
    
    # å ´æ™¯ 1: åŸºæœ¬åˆå§‹åŒ–
    success_1, manager = await test_scenario_1_basic_initialization()
    test_results.append(("åŸºæœ¬ç³»çµ±åˆå§‹åŒ–", success_1))
    
    if not success_1 or not manager:
        print("\nâŒ åŸºæœ¬åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒå¾ŒçºŒæ¸¬è©¦")
        return
    
    # å ´æ™¯ 2: æ±ºç­–èƒ½åŠ›
    success_2 = await test_scenario_2_decision_making(manager)
    test_results.append(("AI æ±ºç­–èƒ½åŠ›", success_2))
    
    # å ´æ™¯ 3: æ€§èƒ½å„ªåŒ–
    success_3 = await test_scenario_3_performance_optimization(manager)
    test_results.append(("æ€§èƒ½å„ªåŒ–", success_3))
    
    # å ´æ™¯ 4: è¨“ç·´æ•´åˆ
    success_4 = await test_scenario_4_training_integration(manager)
    test_results.append(("è¨“ç·´ç³»çµ±æ•´åˆ", success_4))
    
    # å ´æ™¯ 5: å£“åŠ›æ¸¬è©¦
    success_5 = await test_scenario_5_stress_test(manager)
    test_results.append(("é«˜è² è¼‰å£“åŠ›æ¸¬è©¦", success_5))
    
    # æœ€çµ‚å ±å‘Š
    print("\n" + "=" * 70)
    print("ğŸ é¶å ´å¯¦æ¸¬çµæœç¸½çµ")
    print("=" * 70)
    
    passed_tests = sum(1 for _, success in test_results if success)
    total_tests = len(test_results)
    
    for test_name, success in test_results:
        status = "âœ… é€šé" if success else "âŒ å¤±æ•—"
        print(f"{status} {test_name}")
    
    print(f"\nğŸ“Š æ•´é«”æ¸¬è©¦çµæœ: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼AI ç³»çµ±å·²æº–å‚™å¥½é€²è¡Œå¯¦æˆ°éƒ¨ç½²")
    elif passed_tests >= total_tests * 0.8:
        print("âš ï¸  å¤§éƒ¨åˆ†æ¸¬è©¦é€šéï¼Œç³»çµ±åŸºæœ¬å¯ç”¨ï¼Œä½†éœ€è¦èª¿æ•´éƒ¨åˆ†åŠŸèƒ½")
    else:
        print("âŒ å¤šå€‹æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿è©¦å’Œå„ªåŒ–")
    
    print("=" * 70)


if __name__ == "__main__":
    try:
        asyncio.run(run_complete_test_suite())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦éç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        import traceback
        print(traceback.format_exc())