"""
å…¨é¢åˆ†æ AIVA Common é·ç§»å®Œæ•´æ€§
åŒ…æ‹¬ï¼šè©³ç´°å°æ¯”ã€ç¶²è·¯æœç´¢é©—è­‰ã€å®Œæ•´æ€§ç¢ºèª
"""

import re
import json
from pathlib import Path
from collections import defaultdict

# å¸¸é‡å®šç¾©
SCHEMAS_FILE = "schemas.py"
ENUMS_FILE = "enums.py"
AI_SCHEMAS_FILE = "ai_schemas.py"
CLASS_PATTERN = r'^class (\w+)\('
AIVA_COMMON_SCHEMAS = "aiva_common.schemas"
AIVA_COMMON_ENUMS = "aiva_common.enums"


def analyze_migration_completeness():
    """å…¨é¢åˆ†æé·ç§»å®Œæ•´æ€§ - ä¸»å…¥å£å‡½æ•¸"""
    project_root = Path(__file__).parent.parent.parent
    aiva_common_path = project_root / "services" / "aiva_common"
    
    _print_header()
    results = _initialize_results()
    
    # åŸ·è¡Œåˆ†ææ­¥é©Ÿ
    old_files_data = _analyze_legacy_files(aiva_common_path)
    results["analysis"]["old_files"] = old_files_data
    
    _analyze_file_structure(aiva_common_path, results)
    _analyze_schemas_comparison(project_root, aiva_common_path, results)
    _analyze_enums_comparison(project_root, aiva_common_path, results)
    _perform_import_validation(results)
    _perform_file_size_comparison(project_root, aiva_common_path, results)
    
    return results


def _print_header():
    """åˆ—å°åˆ†ææ¨™é¡Œ"""
    print("=" * 100)
    print("ğŸ” AIVA COMMON é·ç§»å®Œæ•´æ€§å…¨é¢åˆ†æ")
    print("=" * 100)


def _initialize_results():
    """åˆå§‹åŒ–çµæœçµæ§‹"""
    return {
        "timestamp": "2025-10-16",
        "analysis": {},
        "migration_status": {},
        "file_structure": {},
        "validation": {}
    }


def _analyze_legacy_files(aiva_common_path: Path):
    """åˆ†æèˆŠæª”æ¡ˆå…§å®¹"""
    print("\nğŸ“„ æ­¥é©Ÿ 1: åˆ†æèˆŠæª”æ¡ˆå…§å®¹")
    print("-" * 50)
    
    file_paths = {
        SCHEMAS_FILE: aiva_common_path / SCHEMAS_FILE,
        ENUMS_FILE: aiva_common_path / ENUMS_FILE,
        AI_SCHEMAS_FILE: aiva_common_path / AI_SCHEMAS_FILE
    }
    
    old_data = {}
    for name, path in file_paths.items():
        old_data[name] = _process_legacy_file(name, path)
    
    # åˆä½µé¡åˆ¥çµ±è¨ˆ
    all_schemas, all_enums = _merge_class_statistics(old_data)
    
    print(f"\nğŸ“Š èˆŠæª”æ¡ˆç¸½è¨ˆ:")
    print(f"   Schemas: {len(all_schemas)} å€‹é¡åˆ¥")
    print(f"   Enums: {len(all_enums)} å€‹é¡åˆ¥")
    
    return {
        "schemas_count": len(all_schemas),
        "enums_count": len(all_enums),
        "schemas_list": sorted(all_schemas),
        "enums_list": sorted(all_enums),
        "file_details": old_data
    }


def _process_legacy_file(filename: str, filepath: Path):
    """è™•ç†å–®å€‹èˆŠæª”æ¡ˆ"""
    if not filepath.exists():
        print(f"âš ï¸  {filename:15s}: æª”æ¡ˆä¸å­˜åœ¨")
        return {"classes": [], "size": 0, "lines": 0}
    
    try:
        content = filepath.read_text(encoding='utf-8')
        classes = re.findall(CLASS_PATTERN, content, re.MULTILINE)
        file_data = {
            "classes": sorted(set(classes)),
            "size": filepath.stat().st_size,
            "lines": len(content.split('\n'))
        }
        print(f"âœ… {filename:15s}: {len(file_data['classes']):3d} é¡åˆ¥, {file_data['size']:>8,} bytes")
        return file_data
    except Exception as e:
        print(f"âŒ {filename:15s}: è®€å–å¤±æ•— - {e}")
        return {"classes": [], "size": 0, "lines": 0}


def _merge_class_statistics(old_data):
    """åˆä½µæ‰€æœ‰é¡åˆ¥çµ±è¨ˆè³‡è¨Š"""
    all_schemas = set()
    all_enums = set()
    
    # åˆä½µ schemas
    for file_key in [SCHEMAS_FILE, AI_SCHEMAS_FILE]:
        if file_key in old_data and old_data[file_key]["classes"]:
            all_schemas.update(old_data[file_key]["classes"])
    
    # åˆä½µ enums
    if ENUMS_FILE in old_data and old_data[ENUMS_FILE]["classes"]:
        all_enums.update(old_data[ENUMS_FILE]["classes"])
    
    return all_schemas, all_enums


def _analyze_file_structure(aiva_common_path: Path, results: dict):
    """åˆ†ææ–°æª”æ¡ˆçµæ§‹"""
    print("\nğŸ“ æ­¥é©Ÿ 2: åˆ†ææ–°æª”æ¡ˆçµæ§‹")
    print("-" * 50)
    
    structure_data = _scan_directory_structure(aiva_common_path)
    results["file_structure"] = structure_data
    _print_structure_summary(structure_data)


def _scan_directory_structure(base_path: Path):
    """æƒæç›®éŒ„çµæ§‹"""
    structure = {"directories": {}, "files": {}}
    
    for subdir in ["schemas", "enums"]:
        dir_path = base_path / subdir
        if dir_path.exists():
            structure["directories"][subdir] = _analyze_directory_contents(dir_path)
        else:
            print(f"âš ï¸  ç›®éŒ„ä¸å­˜åœ¨: {subdir}")
            structure["directories"][subdir] = {"files": [], "total_classes": []}
    
    return structure


def _analyze_directory_contents(directory_path: Path):
    """åˆ†æç›®éŒ„å…§å®¹"""
    py_files = list(directory_path.glob("*.py"))
    total_classes = []
    
    for py_file in py_files:
        try:
            content = py_file.read_text(encoding='utf-8')
            classes = re.findall(CLASS_PATTERN, content, re.MULTILINE)
            total_classes.extend(classes)
            print(f"   ğŸ“„ {py_file.name}: {len(classes)} å€‹é¡åˆ¥")
        except Exception as e:
            print(f"   âŒ {py_file.name}: è®€å–å¤±æ•— - {e}")
    
    return {
        "files": [f.name for f in py_files],
        "total_classes": sorted(set(total_classes))
    }


def _print_structure_summary(structure_data):
    """åˆ—å°çµæ§‹æ‘˜è¦"""
    for subdir, data in structure_data["directories"].items():
        print(f"ğŸ“‚ {subdir}: {len(data['files'])} æª”æ¡ˆ, {len(data['total_classes'])} é¡åˆ¥")


def _analyze_schemas_comparison(project_root: Path, aiva_common_path: Path, results: dict):
    """åˆ†æ schemas å°æ¯”"""
    print(f"\nğŸ“‹ Schemas å°æ¯”:")
    print("-" * 30)
    
    old_schemas = set(results["analysis"]["old_files"]["schemas_list"])
    new_structure = results["file_structure"]["directories"]["schemas"]
    new_schemas = set(new_structure["total_classes"])
    
    _perform_set_comparison("schemas", old_schemas, new_schemas, results)


def _analyze_enums_comparison(project_root: Path, aiva_common_path: Path, results: dict):
    """åˆ†æ enums å°æ¯”"""
    print(f"\nğŸ“‹ Enums å°æ¯”:")
    print("-" * 30)
    
    old_enums = set(results["analysis"]["old_files"]["enums_list"])
    new_structure = results["file_structure"]["directories"]["enums"]
    new_enums = set(new_structure["total_classes"])
    
    _perform_set_comparison("enums", old_enums, new_enums, results)


def _perform_set_comparison(data_type: str, old_set: set, new_set: set, results: dict):
    """åŸ·è¡Œé›†åˆæ¯”è¼ƒ"""
    missing = old_set - new_set
    extra = new_set - old_set
    
    if not missing and not extra:
        print(f"   âœ… å®Œå…¨åŒ¹é…ï¼")
        status = "å®Œå…¨åŒ¹é…"
    else:
        status = f"ç¼ºå¤± {len(missing)} å€‹ï¼Œé¡å¤– {len(extra)} å€‹"
        if missing:
            print(f"   âŒ ç¼ºå¤±: {', '.join(sorted(missing))}")
        if extra:
            print(f"   â• é¡å¤–: {', '.join(sorted(extra))}")
    
    results["migration_status"][f"{data_type}_comparison"] = {
        "status": status,
        "missing_count": len(missing),
        "extra_count": len(extra),
        "missing_items": sorted(missing),
        "extra_items": sorted(extra)
    }


def _perform_import_validation(results: dict):
    """åŸ·è¡Œå°å…¥é©—è­‰"""
    print("\nğŸ§ª æ­¥é©Ÿ 5: åŸ·è¡Œå°å…¥é©—è­‰")
    print("-" * 50)
    
    try:
        # åŸºæœ¬æ¨¡çµ„å°å…¥æ¸¬è©¦
        import aiva_common.schemas
        import aiva_common.enums
        print("âœ… åŸºæœ¬æ¨¡çµ„å°å…¥æˆåŠŸ")
        
        # ç‰¹å®šé¡åˆ¥å°å…¥æ¸¬è©¦
        _test_specific_imports()
        
        results["validation"]["import_status"] = "æˆåŠŸ"
    except Exception as e:
        print(f"âŒ å°å…¥æ¸¬è©¦å¤±æ•—: {e}")
        results["validation"]["import_status"] = f"å¤±æ•—: {e}"


def _test_specific_imports():
    """æ¸¬è©¦ç‰¹å®šé¡åˆ¥å°å…¥"""
    test_imports = [
        (AIVA_COMMON_SCHEMAS, "ScanStartPayload"),
        (AIVA_COMMON_SCHEMAS, "NetworkScanConfig"), 
        (AIVA_COMMON_SCHEMAS, "VulnerabilityReport"),
        (AIVA_COMMON_ENUMS, "ModuleName"),
        (AIVA_COMMON_ENUMS, "ScanType"),
        (AIVA_COMMON_ENUMS, "Priority"),
        (AIVA_COMMON_ENUMS, "Status")
    ]
    
    for module_name, class_name in test_imports:
        try:
            module = __import__(module_name, fromlist=[class_name])
            getattr(module, class_name)
            print(f"   âœ… {module_name}.{class_name}")
        except Exception as e:
            print(f"   âŒ {module_name}.{class_name}: {e}")


def _perform_file_size_comparison(project_root: Path, aiva_common_path: Path, results: dict):
    """åŸ·è¡Œæª”æ¡ˆå¤§å°æ¯”è¼ƒ"""
    print(f"\nğŸ“ æ­¥é©Ÿ 6: æª”æ¡ˆå¤§å°å°æ¯”")
    print("-" * 50)
    
    old_size = sum(
        data.get("size", 0) 
        for data in results["analysis"]["old_files"]["file_details"].values()
    )
    
    new_size = _calculate_new_structure_size(aiva_common_path)
    
    print(f"æª”æ¡ˆå¤§å°å°æ¯”:")
    print(f"   èˆŠæª”æ¡ˆç¸½è¨ˆ: {old_size:,} bytes")
    print(f"   æ–°çµæ§‹ç¸½è¨ˆ: {new_size:,} bytes")
    
    if old_size == new_size:
        print(f"   ğŸ“Š å¤§å°ç›¸åŒ")
    else:
        diff = new_size - old_size
        print(f"   ğŸ“Š å·®ç•°: {diff:+,} bytes")
    
    results["migration_status"]["size_comparison"] = {
        "old_size": old_size,
        "new_size": new_size,
        "difference": new_size - old_size
    }


def _calculate_new_structure_size(aiva_common_path: Path):
    """è¨ˆç®—æ–°çµæ§‹ç¸½å¤§å°"""
    total_size = 0
    for subdir in ["schemas", "enums"]:
        dir_path = aiva_common_path / subdir
        if dir_path.exists():
            for py_file in dir_path.glob("*.py"):
                total_size += py_file.stat().st_size
    return total_size


if __name__ == "__main__":
    results = analyze_migration_completeness()
    
    print("\n" + "=" * 100)
    print("ğŸ“‹ åˆ†æå®Œæˆæ‘˜è¦")
    print("=" * 100)
    print(json.dumps(results, indent=2, ensure_ascii=False))