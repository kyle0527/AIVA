#!/usr/bin/env python
"""
analyze_codebase.py
-------------------
ç¶œåˆç¨‹å¼ç¢¼åˆ†æå·¥å…· (æ”¯æ´å¤šèªè¨€)
ä½¿ç”¨ CodeAnalyzer å’Œ py2mermaid å°æ•´å€‹ç¨‹å¼ç¢¼åº«é€²è¡Œåˆ†æ
æ”¯æ´ Python, Go, Rust, TypeScript/JavaScript
"""

import ast
from collections import defaultdict
from datetime import datetime
import json
from pathlib import Path
from typing import Any


class CodeAnalyzer:
    """ç¨‹å¼ç¢¼åˆ†æå™¨ - ç¨ç«‹ç‰ˆæœ¬."""

    def __init__(self, codebase_path: str) -> None:
        """åˆå§‹åŒ–ç¨‹å¼ç¢¼åˆ†æå™¨.

        Args:
            codebase_path: ç¨‹å¼ç¢¼åº«æ ¹ç›®éŒ„
        """
        self.codebase_path = Path(codebase_path)

    def execute(self, **kwargs: Any) -> dict[str, Any]:
        """åˆ†æç¨‹å¼ç¢¼.

        Args:
            **kwargs: å·¥å…·åƒæ•¸
                path (str): æª”æ¡ˆè·¯å¾‘
                detailed (bool): æ˜¯å¦é€²è¡Œè©³ç´°åˆ†æ (é è¨­ False)

        Returns:
            åˆ†æçµæœ
        """
        path = kwargs.get("path", "")
        detailed = kwargs.get("detailed", False)

        if not path:
            return {"status": "error", "error": "ç¼ºå°‘å¿…éœ€åƒæ•¸: path"}

        try:
            full_path = self.codebase_path / path
            content = full_path.read_text(encoding="utf-8")

            # åŸºæœ¬çµ±è¨ˆ
            lines = content.splitlines()
            non_empty_lines = [line for line in lines if line.strip()]
            comment_lines = [
                line
                for line in lines
                if line.strip().startswith("#")
                or line.strip().startswith('"""')
                or line.strip().startswith("'''")
            ]

            result = {
                "status": "success",
                "path": path,
                "total_lines": len(lines),
                "code_lines": len(non_empty_lines),
                "comment_lines": len(comment_lines),
                "blank_lines": len(lines) - len(non_empty_lines),
            }

            # å¦‚æœè¦æ±‚è©³ç´°åˆ†æï¼Œä½¿ç”¨ AST
            if detailed:
                try:
                    tree = ast.parse(content)

                    # çµ±è¨ˆå„ç¨®ç¯€é»
                    imports: list[str] = []
                    functions: list[str] = []
                    classes: list[str] = []
                    async_functions: list[str] = []

                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            imports.extend(alias.name for alias in node.names)
                        elif isinstance(node, ast.ImportFrom):
                            module = node.module or ""
                            imports.append(module)
                        elif isinstance(node, ast.FunctionDef):
                            functions.append(node.name)
                        elif isinstance(node, ast.AsyncFunctionDef):
                            async_functions.append(node.name)
                        elif isinstance(node, ast.ClassDef):
                            classes.append(node.name)

                    # è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
                    complexity = self._calculate_complexity(tree)

                    result.update(
                        {
                            "imports": list(set(imports)),
                            "import_count": len(set(imports)),
                            "functions": functions,
                            "function_count": len(functions),
                            "async_functions": async_functions,
                            "async_function_count": len(async_functions),
                            "classes": classes,
                            "class_count": len(classes),
                            "cyclomatic_complexity": complexity,
                            "has_type_hints": self._check_type_hints(tree),
                            "has_docstrings": self._check_docstrings(tree),
                        }
                    )
                except SyntaxError as e:
                    result["syntax_error"] = str(e)
            else:
                # ç°¡å–®çµ±è¨ˆï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                import_count = sum(
                    1 for line in lines if line.strip().startswith("import")
                )
                function_count = sum(
                    1 for line in lines if line.strip().startswith("def ")
                )
                class_count = sum(
                    1 for line in lines if line.strip().startswith("class ")
                )

                result.update(
                    {
                        "imports": import_count,
                        "functions": function_count,
                        "classes": class_count,
                    }
                )

            return result
        except Exception as e:
            return {"status": "error", "path": path, "error": str(e)}

    def _calculate_complexity(self, tree: ast.AST) -> int:
        """è¨ˆç®—å¾ªç’°è¤‡é›œåº¦.

        Args:
            tree: AST æ¨¹

        Returns:
            è¤‡é›œåº¦åˆ†æ•¸
        """
        complexity = 1  # åŸºç¤è¤‡é›œåº¦

        for node in ast.walk(tree):
            # æ¯å€‹æ±ºç­–é»å¢åŠ è¤‡é›œåº¦
            if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

        return complexity

    def _check_type_hints(self, tree: ast.AST) -> bool:
        """æª¢æŸ¥æ˜¯å¦ä½¿ç”¨é¡å‹æç¤º.

        Args:
            tree: AST æ¨¹

        Returns:
            æ˜¯å¦æœ‰é¡å‹æç¤º
        """
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # æª¢æŸ¥åƒæ•¸é¡å‹æç¤º
                if node.args.args:
                    for arg in node.args.args:
                        if arg.annotation is not None:
                            return True
                # æª¢æŸ¥è¿”å›å€¼é¡å‹æç¤º
                if node.returns is not None:
                    return True
        return False

    def _check_docstrings(self, tree: ast.AST) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰æ–‡æª”å­—ä¸².

        Args:
            tree: AST æ¨¹

        Returns:
            æ˜¯å¦æœ‰æ–‡æª”å­—ä¸²
        """
        for node in ast.walk(tree):
            if isinstance(
                node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)
            ):
                docstring = ast.get_docstring(node)
                if docstring:
                    return True
        return False


def analyze_directory(
    root_path: Path,
    output_dir: Path,
    ignore_patterns: list[str] | None = None,
    max_files: int = 1000,
) -> dict[str, Any]:
    """åˆ†ææŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰ Python æª”æ¡ˆ.

    Args:
        root_path: è¦åˆ†æçš„æ ¹ç›®éŒ„
        output_dir: è¼¸å‡ºå ±å‘Šçš„ç›®éŒ„
        ignore_patterns: è¦å¿½ç•¥çš„è·¯å¾‘æ¨¡å¼
        max_files: æœ€å¤§åˆ†ææª”æ¡ˆæ•¸

    Returns:
        åˆ†æçµæœæ‘˜è¦
    """
    if ignore_patterns is None:
        ignore_patterns = [
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            "_out",
            "emoji_backups",
            ".pytest_cache",
            "target",
            "build",
            "dist",
        ]

    analyzer = CodeAnalyzer(str(root_path))

    # çµ±è¨ˆæ•¸æ“š
    stats = {
        "total_files": 0,
        "total_lines": 0,
        "total_code_lines": 0,
        "total_comment_lines": 0,
        "total_blank_lines": 0,
        "total_functions": 0,
        "total_classes": 0,
        "total_imports": 0,
        "files_with_type_hints": 0,
        "files_with_docstrings": 0,
        "total_complexity": 0,
        "syntax_errors": [],
        "file_details": [],
        "top_complex_files": [],
        "module_analysis": defaultdict(
            lambda: {
                "files": 0,
                "lines": 0,
                "functions": 0,
                "classes": 0,
            }
        ),
    }

    # æƒææ‰€æœ‰ Python æª”æ¡ˆ
    py_files = []
    for py_file in root_path.rglob("*.py"):
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²å¿½ç•¥
        if any(pattern in str(py_file) for pattern in ignore_patterns):
            continue
        py_files.append(py_file)
        if len(py_files) >= max_files:
            break

    print(f"æ‰¾åˆ° {len(py_files)} å€‹ Python æª”æ¡ˆ")

    # åˆ†ææ¯å€‹æª”æ¡ˆ
    for i, py_file in enumerate(py_files, 1):
        if i % 20 == 0:
            print(f"é€²åº¦: {i}/{len(py_files)}")

        rel_path = py_file.relative_to(root_path)
        result = analyzer.execute(path=str(rel_path), detailed=True)

        if result["status"] == "success":
            stats["total_files"] += 1
            stats["total_lines"] += result.get("total_lines", 0)
            stats["total_code_lines"] += result.get("code_lines", 0)
            stats["total_comment_lines"] += result.get("comment_lines", 0)
            stats["total_blank_lines"] += result.get("blank_lines", 0)
            stats["total_functions"] += result.get("function_count", 0)
            stats["total_classes"] += result.get("class_count", 0)
            stats["total_imports"] += result.get("import_count", 0)

            if result.get("has_type_hints"):
                stats["files_with_type_hints"] += 1
            if result.get("has_docstrings"):
                stats["files_with_docstrings"] += 1

            complexity = result.get("cyclomatic_complexity", 0)
            stats["total_complexity"] += complexity

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            file_detail = {
                "path": str(rel_path),
                "lines": result.get("total_lines", 0),
                "functions": result.get("function_count", 0),
                "classes": result.get("class_count", 0),
                "complexity": complexity,
                "has_type_hints": result.get("has_type_hints", False),
                "has_docstrings": result.get("has_docstrings", False),
            }
            stats["file_details"].append(file_detail)

            # æŒ‰æ¨¡çµ„çµ±è¨ˆ
            parts = rel_path.parts
            if len(parts) > 0:
                module = parts[0]
                stats["module_analysis"][module]["files"] += 1
                stats["module_analysis"][module]["lines"] += result.get(
                    "total_lines", 0
                )
                stats["module_analysis"][module]["functions"] += result.get(
                    "function_count", 0
                )
                stats["module_analysis"][module]["classes"] += result.get(
                    "class_count", 0
                )

            # æª¢æŸ¥èªæ³•éŒ¯èª¤
            if "syntax_error" in result:
                stats["syntax_errors"].append(
                    {
                        "path": str(rel_path),
                        "error": result["syntax_error"],
                    }
                )

    # æ‰¾å‡ºæœ€è¤‡é›œçš„æª”æ¡ˆï¼ˆå‰ 20ï¼‰
    stats["file_details"].sort(key=lambda x: x["complexity"], reverse=True)
    stats["top_complex_files"] = stats["file_details"][:20]

    # ç”Ÿæˆå ±å‘Š
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆ JSON å ±å‘Š
    json_output = (
        output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(json_output, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ–‡å­—å ±å‘Š
    txt_output = (
        output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    )
    with open(txt_output, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("AIVA ç¨‹å¼ç¢¼åº«åˆ†æå ±å‘Š\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        f.write("æ•´é«”çµ±è¨ˆ\n")
        f.write("-" * 80 + "\n")
        f.write(f"ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}\n")
        f.write(f"ç¸½è¡Œæ•¸: {stats['total_lines']:,}\n")
        f.write(f"  - ç¨‹å¼ç¢¼è¡Œ: {stats['total_code_lines']:,}\n")
        f.write(f"  - è¨»è§£è¡Œ: {stats['total_comment_lines']:,}\n")
        f.write(f"  - ç©ºç™½è¡Œ: {stats['total_blank_lines']:,}\n")
        f.write(f"ç¸½å‡½æ•¸æ•¸: {stats['total_functions']}\n")
        f.write(f"ç¸½é¡åˆ¥æ•¸: {stats['total_classes']}\n")
        f.write(f"ç¸½å°å…¥æ•¸: {stats['total_imports']}\n")
        f.write(
            f"å¹³å‡è¤‡é›œåº¦: {stats['total_complexity'] / max(stats['total_files'], 1):.2f}\n"
        )
        f.write("\n")

        f.write("ç¨‹å¼ç¢¼å“è³ªæŒ‡æ¨™\n")
        f.write("-" * 80 + "\n")
        type_hint_pct = (
            stats["files_with_type_hints"] / max(stats["total_files"], 1)
        ) * 100
        docstring_pct = (
            stats["files_with_docstrings"] / max(stats["total_files"], 1)
        ) * 100
        f.write(
            f"é¡å‹æç¤ºè¦†è“‹ç‡: {type_hint_pct:.1f}% ({stats['files_with_type_hints']}/{stats['total_files']})\n"
        )
        f.write(
            f"æ–‡æª”å­—ä¸²è¦†è“‹ç‡: {docstring_pct:.1f}% ({stats['files_with_docstrings']}/{stats['total_files']})\n"
        )
        f.write("\n")

        if stats["syntax_errors"]:
            f.write("èªæ³•éŒ¯èª¤\n")
            f.write("-" * 80 + "\n")
            for err in stats["syntax_errors"]:
                f.write(f"{err['path']}: {err['error']}\n")
            f.write("\n")

        f.write("æ¨¡çµ„åˆ†æ\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æ¨¡çµ„':<30} {'æª”æ¡ˆæ•¸':>10} {'è¡Œæ•¸':>12} {'å‡½æ•¸':>10} {'é¡åˆ¥':>10}\n")
        f.write("-" * 80 + "\n")
        for module, data in sorted(stats["module_analysis"].items()):
            f.write(
                f"{module:<30} {data['files']:>10} {data['lines']:>12,} "
                f"{data['functions']:>10} {data['classes']:>10}\n"
            )
        f.write("\n")

        f.write("æœ€è¤‡é›œçš„æª”æ¡ˆï¼ˆå‰ 20ï¼‰\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æª”æ¡ˆè·¯å¾‘':<60} {'è¤‡é›œåº¦':>10} {'è¡Œæ•¸':>8}\n")
        f.write("-" * 80 + "\n")
        for file_info in stats["top_complex_files"]:
            path = file_info["path"]
            if len(path) > 58:
                path = "..." + path[-55:]
            f.write(
                f"{path:<60} {file_info['complexity']:>10} {file_info['lines']:>8}\n"
            )
        f.write("\n")

        f.write("=" * 80 + "\n")
        f.write("å ±å‘ŠçµæŸ\n")
        f.write("=" * 80 + "\n")

    print("\nå ±å‘Šå·²ç”Ÿæˆ:")
    print(f"  JSON: {json_output}")
    print(f"  TXT: {txt_output}")

    return stats


def _analyze_file_basic(content: str, comment_prefixes: list[str]) -> dict[str, int]:
    """åˆ†ææª”æ¡ˆåŸºæœ¬çµ±è¨ˆè³‡è¨Šï¼ˆçµ±ä¸€ç‰ˆæœ¬ï¼‰.

    Args:
        content: æª”æ¡ˆå…§å®¹
        comment_prefixes: è¨»è§£å‰ç¶´åˆ—è¡¨ (å¦‚ ["//", "/*", "#"])

    Returns:
        åŸºæœ¬çµ±è¨ˆè³‡è¨Šå­—å…¸ï¼ŒåŒ…å«:
        - total_lines: ç¸½è¡Œæ•¸
        - code_lines: ç¨‹å¼ç¢¼è¡Œæ•¸
        - comment_lines: è¨»è§£è¡Œæ•¸
        - blank_lines: ç©ºç™½è¡Œæ•¸
    """
    lines = content.splitlines()
    non_empty_lines = [line for line in lines if line.strip()]

    comment_lines = []
    for line in lines:
        stripped = line.strip()
        if any(stripped.startswith(prefix) for prefix in comment_prefixes):
            comment_lines.append(line)

    return {
        "total_lines": len(lines),
        "code_lines": len(non_empty_lines),
        "comment_lines": len(comment_lines),
        "blank_lines": len(lines) - len(non_empty_lines),
    }


def _count_go_elements(content: str) -> dict[str, int]:
    """è¨ˆç®— Go ç¨‹å¼ç¢¼å…ƒç´ .

    Args:
        content: Go æª”æ¡ˆå…§å®¹

    Returns:
        åŒ…å« functions, structs, interfaces è¨ˆæ•¸
    """
    import re

    # è¨ˆç®—å‡½æ•¸ (func xxx)
    func_pattern = r"^\s*func\s+(?:\([^)]+\)\s+)?(\w+)"
    functions = len(re.findall(func_pattern, content, re.MULTILINE))

    # è¨ˆç®—çµæ§‹é«” (type xxx struct)
    struct_pattern = r"^\s*type\s+(\w+)\s+struct\s*\{"
    structs = len(re.findall(struct_pattern, content, re.MULTILINE))

    # è¨ˆç®—ä»‹é¢ (type xxx interface)
    interface_pattern = r"^\s*type\s+(\w+)\s+interface\s*\{"
    interfaces = len(re.findall(interface_pattern, content, re.MULTILINE))

    return {
        "functions": functions,
        "structs": structs,
        "interfaces": interfaces,
    }


def _count_rust_elements(content: str) -> dict[str, int]:
    """è¨ˆç®— Rust ç¨‹å¼ç¢¼å…ƒç´ .

    Args:
        content: Rust æª”æ¡ˆå…§å®¹

    Returns:
        åŒ…å« functions, structs, traits, impls è¨ˆæ•¸
    """
    import re

    # è¨ˆç®—å‡½æ•¸ (fn xxx)
    func_pattern = r"^\s*(?:pub\s+)?(?:async\s+)?fn\s+(\w+)"
    functions = len(re.findall(func_pattern, content, re.MULTILINE))

    # è¨ˆç®—çµæ§‹é«” (struct xxx)
    struct_pattern = r"^\s*(?:pub\s+)?struct\s+(\w+)"
    structs = len(re.findall(struct_pattern, content, re.MULTILINE))

    # è¨ˆç®— traits
    trait_pattern = r"^\s*(?:pub\s+)?trait\s+(\w+)"
    traits = len(re.findall(trait_pattern, content, re.MULTILINE))

    # è¨ˆç®— impl å€å¡Š
    impl_pattern = r"^\s*impl\s+"
    impls = len(re.findall(impl_pattern, content, re.MULTILINE))

    return {
        "functions": functions,
        "structs": structs,
        "traits": traits,
        "impls": impls,
    }


def _count_typescript_elements(content: str) -> dict[str, int]:
    """è¨ˆç®— TypeScript ç¨‹å¼ç¢¼å…ƒç´ .

    Args:
        content: TypeScript æª”æ¡ˆå…§å®¹

    Returns:
        åŒ…å« functions, classes, interfaces è¨ˆæ•¸
    """
    import re

    # è¨ˆç®—å‡½æ•¸ (function xxx, const xxx = () =>)
    func_pattern = r"(?:function\s+(\w+)|(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>)"
    functions = len(re.findall(func_pattern, content, re.MULTILINE))

    # è¨ˆç®—é¡åˆ¥
    class_pattern = r"^\s*(?:export\s+)?(?:abstract\s+)?class\s+(\w+)"
    classes = len(re.findall(class_pattern, content, re.MULTILINE))

    # è¨ˆç®—ä»‹é¢
    interface_pattern = r"^\s*(?:export\s+)?interface\s+(\w+)"
    interfaces = len(re.findall(interface_pattern, content, re.MULTILINE))

    # è¨ˆç®—é¡å‹åˆ¥å
    type_pattern = r"^\s*(?:export\s+)?type\s+(\w+)\s*="
    types = len(re.findall(type_pattern, content, re.MULTILINE))

    return {
        "functions": functions,
        "classes": classes,
        "interfaces": interfaces,
        "types": types,
    }


def _generate_multilang_report(stats: dict, output_dir: Path) -> None:
    """ç”Ÿæˆå¤šèªè¨€åˆ†æå ±å‘Š.

    Args:
        stats: çµ±è¨ˆæ•¸æ“š
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆ JSON å ±å‘Š
    json_output = (
        output_dir
        / f"multilang_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(json_output, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ–‡å­—å ±å‘Š
    txt_output = (
        output_dir
        / f"multilang_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    )
    with open(txt_output, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("AIVA å¤šèªè¨€ç¨‹å¼ç¢¼åˆ†æå ±å‘Š\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        # ç¸½è¦½
        f.write("å¤šèªè¨€ç¨‹å¼ç¢¼çµ±è¨ˆç¸½è¦½\n")
        f.write("-" * 80 + "\n")
        for lang, data in stats.items():
            if data["total_files"] > 0:
                f.write(f"\n{lang.upper()}:\n")
                f.write(f"  æª”æ¡ˆæ•¸: {data['total_files']}\n")
                f.write(f"  ç¸½è¡Œæ•¸: {data['total_lines']:,}\n")
                f.write(f"    - ç¨‹å¼ç¢¼è¡Œ: {data['total_code_lines']:,}\n")
                f.write(f"    - è¨»è§£è¡Œ: {data['total_comment_lines']:,}\n")
                f.write(f"    - ç©ºç™½è¡Œ: {data['total_blank_lines']:,}\n")

                # èªè¨€ç‰¹å®šçµ±è¨ˆ
                if lang == "go":
                    f.write(f"  å‡½æ•¸æ•¸: {data['total_functions']}\n")
                    f.write(f"  çµæ§‹é«”æ•¸: {data['total_structs']}\n")
                    f.write(f"  ä»‹é¢æ•¸: {data['total_interfaces']}\n")
                elif lang == "rust":
                    f.write(f"  å‡½æ•¸æ•¸: {data['total_functions']}\n")
                    f.write(f"  çµæ§‹é«”æ•¸: {data['total_structs']}\n")
                    f.write(f"  Traits: {data['total_traits']}\n")
                    f.write(f"  Impls: {data['total_impls']}\n")
                elif lang == "typescript":
                    f.write(f"  å‡½æ•¸æ•¸: {data['total_functions']}\n")
                    f.write(f"  é¡åˆ¥æ•¸: {data['total_classes']}\n")
                    f.write(f"  ä»‹é¢æ•¸: {data['total_interfaces']}\n")
                    f.write(f"  é¡å‹åˆ¥å: {data['total_types']}\n")
                elif lang == "javascript":
                    f.write(f"  å‡½æ•¸æ•¸: {data['total_functions']}\n")
                    f.write(f"  é¡åˆ¥æ•¸: {data['total_classes']}\n")

        # å„èªè¨€æª”æ¡ˆè©³æƒ…ï¼ˆå‰10å¤§æª”æ¡ˆï¼‰
        f.write("\n\næœ€å¤§çš„æª”æ¡ˆï¼ˆå„èªè¨€å‰ 10ï¼‰\n")
        f.write("-" * 80 + "\n")

        for lang, data in stats.items():
            if data["total_files"] > 0:
                f.write(f"\n{lang.upper()}:\n")
                sorted_files = sorted(
                    data["file_details"], key=lambda x: x["lines"], reverse=True
                )[:10]
                f.write(f"{'æª”æ¡ˆè·¯å¾‘':<50} {'è¡Œæ•¸':>10} {'å‡½æ•¸':>8}\n")
                f.write("-" * 70 + "\n")
                for file_info in sorted_files:
                    path = file_info["path"]
                    if len(path) > 48:
                        path = "..." + path[-45:]
                    func_key = "functions" if "functions" in file_info else "N/A"
                    func_count = file_info.get(func_key, 0) if func_key != "N/A" else 0
                    f.write(f"{path:<50} {file_info['lines']:>10} {func_count:>8}\n")

        f.write("\n" + "=" * 80 + "\n")
        f.write("å ±å‘ŠçµæŸ\n")
        f.write("=" * 80 + "\n")

    print("\nå¤šèªè¨€åˆ†æå ±å‘Šå·²ç”Ÿæˆ:")
    print(f"  JSON: {json_output}")
    print(f"  TXT: {txt_output}")


def analyze_multilang_files(
    root_path: Path,
    output_dir: Path,
    ignore_patterns: list[str] | None = None,
) -> dict[str, Any]:
    """åˆ†æå¤šèªè¨€ç¨‹å¼ç¢¼æª”æ¡ˆ (Go, Rust, TypeScript, JavaScript).

    Args:
        root_path: è¦åˆ†æçš„æ ¹ç›®éŒ„
        output_dir: è¼¸å‡ºå ±å‘Šçš„ç›®éŒ„
        ignore_patterns: è¦å¿½ç•¥çš„è·¯å¾‘æ¨¡å¼

    Returns:
        å¤šèªè¨€åˆ†æçµæœæ‘˜è¦ (schema çµ±ä¸€ç‰ˆæœ¬)
    """
    if ignore_patterns is None:
        ignore_patterns = [
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            "_out",
            "emoji_backups",
            ".pytest_cache",
            "target",
            "build",
            "dist",
        ]

    # çµ±ä¸€ schema çµæ§‹ï¼ˆå°é½Š analyze_directoryï¼‰
    multilang_stats = {
        "go": {
            "total_files": 0,
            "total_lines": 0,
            "total_code_lines": 0,
            "total_comment_lines": 0,
            "total_blank_lines": 0,
            "total_functions": 0,
            "total_structs": 0,
            "total_interfaces": 0,
            "file_details": [],
        },
        "rust": {
            "total_files": 0,
            "total_lines": 0,
            "total_code_lines": 0,
            "total_comment_lines": 0,
            "total_blank_lines": 0,
            "total_functions": 0,
            "total_structs": 0,
            "total_traits": 0,
            "total_impls": 0,
            "file_details": [],
        },
        "typescript": {
            "total_files": 0,
            "total_lines": 0,
            "total_code_lines": 0,
            "total_comment_lines": 0,
            "total_blank_lines": 0,
            "total_functions": 0,
            "total_classes": 0,
            "total_interfaces": 0,
            "total_types": 0,
            "file_details": [],
        },
        "javascript": {
            "total_files": 0,
            "total_lines": 0,
            "total_code_lines": 0,
            "total_comment_lines": 0,
            "total_blank_lines": 0,
            "total_functions": 0,
            "total_classes": 0,
            "file_details": [],
        },
    }

    print("é–‹å§‹åˆ†æå¤šèªè¨€æª”æ¡ˆ...")

    # æƒæ Go æª”æ¡ˆï¼ˆçµ±ä¸€ schemaï¼‰
    go_files = [
        f
        for f in root_path.rglob("*.go")
        if not any(pattern in str(f) for pattern in ignore_patterns)
    ]
    print(f"æ‰¾åˆ° {len(go_files)} å€‹ Go æª”æ¡ˆ")

    for i, go_file in enumerate(go_files, 1):
        if i % 10 == 0:
            print(f"  Go é€²åº¦: {i}/{len(go_files)}")
        try:
            content = go_file.read_text(encoding="utf-8")
            rel_path = str(go_file.relative_to(root_path))

            # åŸºæœ¬çµ±è¨ˆ
            basic_stats = _analyze_file_basic(content, ["//", "/*"])
            go_elements = _count_go_elements(content)

            # æ›´æ–°ç¸½è¨ˆ
            multilang_stats["go"]["total_files"] += 1
            multilang_stats["go"]["total_lines"] += basic_stats["total_lines"]
            multilang_stats["go"]["total_code_lines"] += basic_stats["code_lines"]
            multilang_stats["go"]["total_comment_lines"] += basic_stats["comment_lines"]
            multilang_stats["go"]["total_blank_lines"] += basic_stats["blank_lines"]
            multilang_stats["go"]["total_functions"] += go_elements["functions"]
            multilang_stats["go"]["total_structs"] += go_elements["structs"]
            multilang_stats["go"]["total_interfaces"] += go_elements["interfaces"]

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            multilang_stats["go"]["file_details"].append(
                {
                    "path": rel_path,
                    "lines": basic_stats["total_lines"],
                    "functions": go_elements["functions"],
                    "structs": go_elements["structs"],
                    "interfaces": go_elements["interfaces"],
                }
            )
        except Exception as e:
            print(f"  è­¦å‘Š: ç„¡æ³•åˆ†æ {go_file.name}: {e}")

    # æƒæ Rust æª”æ¡ˆï¼ˆçµ±ä¸€ schemaï¼‰
    rust_files = [
        f
        for f in root_path.rglob("*.rs")
        if not any(pattern in str(f) for pattern in ignore_patterns)
    ]
    print(f"æ‰¾åˆ° {len(rust_files)} å€‹ Rust æª”æ¡ˆ")

    for i, rs_file in enumerate(rust_files, 1):
        if i % 10 == 0:
            print(f"  Rust é€²åº¦: {i}/{len(rust_files)}")
        try:
            content = rs_file.read_text(encoding="utf-8")
            rel_path = str(rs_file.relative_to(root_path))

            # åŸºæœ¬çµ±è¨ˆ
            basic_stats = _analyze_file_basic(content, ["//", "/*"])
            rust_elements = _count_rust_elements(content)

            # æ›´æ–°ç¸½è¨ˆ
            multilang_stats["rust"]["total_files"] += 1
            multilang_stats["rust"]["total_lines"] += basic_stats["total_lines"]
            multilang_stats["rust"]["total_code_lines"] += basic_stats["code_lines"]
            multilang_stats["rust"]["total_comment_lines"] += basic_stats[
                "comment_lines"
            ]
            multilang_stats["rust"]["total_blank_lines"] += basic_stats["blank_lines"]
            multilang_stats["rust"]["total_functions"] += rust_elements["functions"]
            multilang_stats["rust"]["total_structs"] += rust_elements["structs"]
            multilang_stats["rust"]["total_traits"] += rust_elements["traits"]
            multilang_stats["rust"]["total_impls"] += rust_elements["impls"]

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            multilang_stats["rust"]["file_details"].append(
                {
                    "path": rel_path,
                    "lines": basic_stats["total_lines"],
                    "functions": rust_elements["functions"],
                    "structs": rust_elements["structs"],
                    "traits": rust_elements["traits"],
                }
            )
        except Exception as e:
            print(f"  è­¦å‘Š: ç„¡æ³•åˆ†æ {rs_file.name}: {e}")

    # æƒæ TypeScript æª”æ¡ˆï¼ˆçµ±ä¸€ schemaï¼‰
    ts_files = [
        f
        for f in root_path.rglob("*.ts")
        if not any(pattern in str(f) for pattern in ignore_patterns)
    ]
    print(f"æ‰¾åˆ° {len(ts_files)} å€‹ TypeScript æª”æ¡ˆ")

    for i, ts_file in enumerate(ts_files, 1):
        if i % 10 == 0:
            print(f"  TypeScript é€²åº¦: {i}/{len(ts_files)}")
        try:
            content = ts_file.read_text(encoding="utf-8")
            rel_path = str(ts_file.relative_to(root_path))

            # åŸºæœ¬çµ±è¨ˆ
            basic_stats = _analyze_file_basic(content, ["//", "/*"])
            ts_elements = _count_typescript_elements(content)

            # æ›´æ–°ç¸½è¨ˆ
            multilang_stats["typescript"]["total_files"] += 1
            multilang_stats["typescript"]["total_lines"] += basic_stats["total_lines"]
            multilang_stats["typescript"]["total_code_lines"] += basic_stats[
                "code_lines"
            ]
            multilang_stats["typescript"]["total_comment_lines"] += basic_stats[
                "comment_lines"
            ]
            multilang_stats["typescript"]["total_blank_lines"] += basic_stats[
                "blank_lines"
            ]
            multilang_stats["typescript"]["total_functions"] += ts_elements["functions"]
            multilang_stats["typescript"]["total_classes"] += ts_elements["classes"]
            multilang_stats["typescript"]["total_interfaces"] += ts_elements[
                "interfaces"
            ]
            multilang_stats["typescript"]["total_types"] += ts_elements["types"]

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            multilang_stats["typescript"]["file_details"].append(
                {
                    "path": rel_path,
                    "lines": basic_stats["total_lines"],
                    "functions": ts_elements["functions"],
                    "classes": ts_elements["classes"],
                    "interfaces": ts_elements["interfaces"],
                }
            )
        except Exception as e:
            print(f"  è­¦å‘Š: ç„¡æ³•åˆ†æ {ts_file.name}: {e}")

    # æƒæ JavaScript æª”æ¡ˆï¼ˆçµ±ä¸€ schemaï¼‰
    js_files = [
        f
        for f in root_path.rglob("*.js")
        if not any(pattern in str(f) for pattern in ignore_patterns)
    ]
    print(f"æ‰¾åˆ° {len(js_files)} å€‹ JavaScript æª”æ¡ˆ")

    for i, js_file in enumerate(js_files, 1):
        if i % 10 == 0:
            print(f"  JavaScript é€²åº¦: {i}/{len(js_files)}")
        try:
            content = js_file.read_text(encoding="utf-8")
            rel_path = str(js_file.relative_to(root_path))

            # åŸºæœ¬çµ±è¨ˆ
            basic_stats = _analyze_file_basic(content, ["//", "/*"])
            # JavaScript ä½¿ç”¨èˆ‡ TypeScript ç›¸åŒçš„å…ƒç´ è¨ˆæ•¸
            js_elements = _count_typescript_elements(content)

            # æ›´æ–°ç¸½è¨ˆ
            multilang_stats["javascript"]["total_files"] += 1
            multilang_stats["javascript"]["total_lines"] += basic_stats["total_lines"]
            multilang_stats["javascript"]["total_code_lines"] += basic_stats[
                "code_lines"
            ]
            multilang_stats["javascript"]["total_comment_lines"] += basic_stats[
                "comment_lines"
            ]
            multilang_stats["javascript"]["total_blank_lines"] += basic_stats[
                "blank_lines"
            ]
            multilang_stats["javascript"]["total_functions"] += js_elements["functions"]
            multilang_stats["javascript"]["total_classes"] += js_elements["classes"]

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            multilang_stats["javascript"]["file_details"].append(
                {
                    "path": rel_path,
                    "lines": basic_stats["total_lines"],
                    "functions": js_elements["functions"],
                    "classes": js_elements["classes"],
                }
            )
        except Exception as e:
            print(f"  è­¦å‘Š: ç„¡æ³•åˆ†æ {js_file.name}: {e}")

    # ç”Ÿæˆå ±å‘Š
    _generate_multilang_report(multilang_stats, output_dir)

    return multilang_stats


def main():
    """ä¸»å‡½æ•¸."""
    # è¨­å®šè·¯å¾‘
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    output_dir = project_root / "_out" / "analysis"

    print("AIVA ç¨‹å¼ç¢¼åº«åˆ†æå·¥å…·")
    print("=" * 80)
    print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {project_root}")
    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    print("=" * 80)
    print()

    # åŸ·è¡Œ Python åˆ†æ
    print("ğŸ“Š éšæ®µ 1: åˆ†æ Python ç¨‹å¼ç¢¼")
    print("-" * 80)
    stats = analyze_directory(
        root_path=project_root / "services",
        output_dir=output_dir,
        max_files=1000,
    )

    # åŸ·è¡Œå¤šèªè¨€åˆ†æ
    print("\nğŸ“Š éšæ®µ 2: åˆ†æå¤šèªè¨€ç¨‹å¼ç¢¼ (Go/Rust/TypeScript/JavaScript)")
    print("-" * 80)
    multilang_stats = analyze_multilang_files(
        root_path=project_root / "services",
        output_dir=output_dir,
    )

    # é¡¯ç¤º Python æ‘˜è¦
    print("\n" + "=" * 80)
    print("âœ… Python åˆ†æå®Œæˆï¼æ‘˜è¦:")
    print("=" * 80)
    print(f"ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
    print(f"ç¸½è¡Œæ•¸: {stats['total_lines']:,}")
    print(f"ç¸½å‡½æ•¸æ•¸: {stats['total_functions']}")
    print(f"ç¸½é¡åˆ¥æ•¸: {stats['total_classes']}")
    print(f"å¹³å‡è¤‡é›œåº¦: {stats['total_complexity'] / max(stats['total_files'], 1):.2f}")

    type_hint_pct = (
        stats["files_with_type_hints"] / max(stats["total_files"], 1)
    ) * 100
    docstring_pct = (
        stats["files_with_docstrings"] / max(stats["total_files"], 1)
    ) * 100
    print(f"é¡å‹æç¤ºè¦†è“‹ç‡: {type_hint_pct:.1f}%")
    print(f"æ–‡æª”å­—ä¸²è¦†è“‹ç‡: {docstring_pct:.1f}%")

    # é¡¯ç¤ºå¤šèªè¨€æ‘˜è¦
    print("\n" + "=" * 80)
    print("âœ… å¤šèªè¨€åˆ†æå®Œæˆï¼æ‘˜è¦:")
    print("=" * 80)
    total_multilang_files = 0
    total_multilang_lines = 0
    for lang, data in multilang_stats.items():
        if data["total_files"] > 0:
            print(
                f"{lang.upper()}: {data['total_files']} æª”æ¡ˆ, "
                f"{data['total_lines']:,} è¡Œ, "
                f"{data['total_functions']} å‡½æ•¸"
            )
            total_multilang_files += data["total_files"]
            total_multilang_lines += data["total_lines"]

    # ç¸½è¨ˆ
    print("\n" + "=" * 80)
    print("ğŸ“ˆ å°ˆæ¡ˆç¸½è¨ˆ:")
    print("=" * 80)
    grand_total_files = stats["total_files"] + total_multilang_files
    grand_total_lines = stats["total_lines"] + total_multilang_lines
    print(f"ç¸½æª”æ¡ˆæ•¸ (æ‰€æœ‰èªè¨€): {grand_total_files}")
    print(f"ç¸½è¡Œæ•¸ (æ‰€æœ‰èªè¨€): {grand_total_lines:,}")
    print(f"  - Python: {stats['total_lines']:,} è¡Œ ({stats['total_files']} æª”æ¡ˆ)")
    print(f"  - å…¶ä»–èªè¨€: {total_multilang_lines:,} è¡Œ ({total_multilang_files} æª”æ¡ˆ)")
    print("=" * 80)


if __name__ == "__main__":
    main()
