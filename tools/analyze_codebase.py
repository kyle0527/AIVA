#!/usr/bin/env python
"""
analyze_codebase.py
-------------------
ç¶œåˆç¨‹å¼ç¢¼åˆ†æå·¥å…· (æ”¯æ´å¤šèªè¨€)
ä½¿ç”¨ CodeAnalyzer å’Œ py2mermaid å°æ•´å€‹ç¨‹å¼ç¢¼åº«é€²è¡Œåˆ†æ
æ”¯æ´ Python, Go, Rust, TypeScript/JavaScript
"""

import ast
from collections import defaultdict
from datetime import datetime
import json
from pathlib import Path
from typing import Any


class CodeAnalyzer:
    """ç¨‹å¼ç¢¼åˆ†æå™¨ - ç¨ç«‹ç‰ˆæœ¬."""

    def __init__(self, codebase_path: str) -> None:
        """åˆå§‹åŒ–ç¨‹å¼ç¢¼åˆ†æå™¨.

        Args:
            codebase_path: ç¨‹å¼ç¢¼åº«æ ¹ç›®éŒ„
        """
        self.codebase_path = Path(codebase_path)

    def execute(self, **kwargs: Any) -> dict[str, Any]:
        """åˆ†æç¨‹å¼ç¢¼.

        Args:
            **kwargs: å·¥å…·åƒæ•¸
                path (str): æª”æ¡ˆè·¯å¾‘
                detailed (bool): æ˜¯å¦é€²è¡Œè©³ç´°åˆ†æ (é è¨­ False)

        Returns:
            åˆ†æçµæœ
        """
        path = kwargs.get("path", "")
        detailed = kwargs.get("detailed", False)

        if not path:
            return {"status": "error", "error": "ç¼ºå°‘å¿…éœ€åƒæ•¸: path"}

        try:
            full_path = self.codebase_path / path
            content = full_path.read_text(encoding="utf-8")

            # åŸºæœ¬çµ±è¨ˆ
            lines = content.splitlines()
            non_empty_lines = [line for line in lines if line.strip()]
            comment_lines = [
                line
                for line in lines
                if line.strip().startswith("#")
                or line.strip().startswith('"""')
                or line.strip().startswith("'''")
            ]

            result = {
                "status": "success",
                "path": path,
                "total_lines": len(lines),
                "code_lines": len(non_empty_lines),
                "comment_lines": len(comment_lines),
                "blank_lines": len(lines) - len(non_empty_lines),
            }

            # å¦‚æœè¦æ±‚è©³ç´°åˆ†æï¼Œä½¿ç”¨ AST
            if detailed:
                try:
                    tree = ast.parse(content)

                    # çµ±è¨ˆå„ç¨®ç¯€é»
                    imports: list[str] = []
                    functions: list[str] = []
                    classes: list[str] = []
                    async_functions: list[str] = []

                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            imports.extend(alias.name for alias in node.names)
                        elif isinstance(node, ast.ImportFrom):
                            module = node.module or ""
                            imports.append(module)
                        elif isinstance(node, ast.FunctionDef):
                            functions.append(node.name)
                        elif isinstance(node, ast.AsyncFunctionDef):
                            async_functions.append(node.name)
                        elif isinstance(node, ast.ClassDef):
                            classes.append(node.name)

                    # è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
                    complexity = self._calculate_complexity(tree)

                    result.update(
                        {
                            "imports": list(set(imports)),
                            "import_count": len(set(imports)),
                            "functions": functions,
                            "function_count": len(functions),
                            "async_functions": async_functions,
                            "async_function_count": len(async_functions),
                            "classes": classes,
                            "class_count": len(classes),
                            "cyclomatic_complexity": complexity,
                            "has_type_hints": self._check_type_hints(tree),
                            "has_docstrings": self._check_docstrings(tree),
                        }
                    )
                except SyntaxError as e:
                    result["syntax_error"] = str(e)
            else:
                # ç°¡å–®çµ±è¨ˆï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                import_count = sum(
                    1 for line in lines if line.strip().startswith("import")
                )
                function_count = sum(
                    1 for line in lines if line.strip().startswith("def ")
                )
                class_count = sum(
                    1 for line in lines if line.strip().startswith("class ")
                )

                result.update(
                    {
                        "imports": import_count,
                        "functions": function_count,
                        "classes": class_count,
                    }
                )

            return result
        except Exception as e:
            return {"status": "error", "path": path, "error": str(e)}

    def _calculate_complexity(self, tree: ast.AST) -> int:
        """è¨ˆç®—å¾ªç’°è¤‡é›œåº¦.

        Args:
            tree: AST æ¨¹

        Returns:
            è¤‡é›œåº¦åˆ†æ•¸
        """
        complexity = 1  # åŸºç¤è¤‡é›œåº¦

        for node in ast.walk(tree):
            # æ¯å€‹æ±ºç­–é»å¢åŠ è¤‡é›œåº¦
            if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

        return complexity

    def _check_type_hints(self, tree: ast.AST) -> bool:
        """æª¢æŸ¥æ˜¯å¦ä½¿ç”¨é¡å‹æç¤º.

        Args:
            tree: AST æ¨¹

        Returns:
            æ˜¯å¦æœ‰é¡å‹æç¤º
        """
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # æª¢æŸ¥åƒæ•¸é¡å‹æç¤º
                if node.args.args:
                    for arg in node.args.args:
                        if arg.annotation is not None:
                            return True
                # æª¢æŸ¥è¿”å›å€¼é¡å‹æç¤º
                if node.returns is not None:
                    return True
        return False

    def _check_docstrings(self, tree: ast.AST) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰æ–‡æª”å­—ä¸².

        Args:
            tree: AST æ¨¹

        Returns:
            æ˜¯å¦æœ‰æ–‡æª”å­—ä¸²
        """
        for node in ast.walk(tree):
            if isinstance(
                node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)
            ):
                docstring = ast.get_docstring(node)
                if docstring:
                    return True
        return False


def analyze_directory(
    root_path: Path,
    output_dir: Path,
    ignore_patterns: list[str] | None = None,
    max_files: int = 1000,
) -> dict:
    """åˆ†ææŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰ Python æª”æ¡ˆ.

    Args:
        root_path: è¦åˆ†æçš„æ ¹ç›®éŒ„
        output_dir: è¼¸å‡ºå ±å‘Šçš„ç›®éŒ„
        ignore_patterns: è¦å¿½ç•¥çš„è·¯å¾‘æ¨¡å¼
        max_files: æœ€å¤§åˆ†ææª”æ¡ˆæ•¸

    Returns:
        åˆ†æçµæœæ‘˜è¦
    """
    if ignore_patterns is None:
        ignore_patterns = [
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            "_out",
            "emoji_backups",
            ".pytest_cache",
            "target",
            "build",
            "dist",
        ]

    analyzer = CodeAnalyzer(str(root_path))

    # çµ±è¨ˆæ•¸æ“š
    stats: dict[str, Any] = {
        "total_files": 0,
        "total_lines": 0,
        "total_code_lines": 0,
        "total_comment_lines": 0,
        "total_blank_lines": 0,
        "total_functions": 0,
        "total_classes": 0,
        "total_imports": 0,
        "files_with_type_hints": 0,
        "files_with_docstrings": 0,
        "total_complexity": 0,
        "syntax_errors": [],
        "file_details": [],
        "top_complex_files": [],
        "module_analysis": defaultdict(
            lambda: {
                "files": 0,
                "lines": 0,
                "functions": 0,
                "classes": 0,
            }
        ),
    }

    # æƒææ‰€æœ‰ Python æª”æ¡ˆ
    py_files = []
    for py_file in root_path.rglob("*.py"):
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²å¿½ç•¥
        if any(pattern in str(py_file) for pattern in ignore_patterns):
            continue
        py_files.append(py_file)
        if len(py_files) >= max_files:
            break

    print(f"æ‰¾åˆ° {len(py_files)} å€‹ Python æª”æ¡ˆ")

    # åˆ†ææ¯å€‹æª”æ¡ˆ
    for i, py_file in enumerate(py_files, 1):
        if i % 20 == 0:
            print(f"é€²åº¦: {i}/{len(py_files)}")

        rel_path = py_file.relative_to(root_path)
        result = analyzer.execute(path=str(rel_path), detailed=True)

        if result["status"] == "success":
            stats["total_files"] += 1
            stats["total_lines"] += result.get("total_lines", 0)
            stats["total_code_lines"] += result.get("code_lines", 0)
            stats["total_comment_lines"] += result.get("comment_lines", 0)
            stats["total_blank_lines"] += result.get("blank_lines", 0)
            stats["total_functions"] += result.get("function_count", 0)
            stats["total_classes"] += result.get("class_count", 0)
            stats["total_imports"] += result.get("import_count", 0)

            if result.get("has_type_hints"):
                stats["files_with_type_hints"] += 1
            if result.get("has_docstrings"):
                stats["files_with_docstrings"] += 1

            complexity = result.get("cyclomatic_complexity", 0)
            stats["total_complexity"] += complexity

            # è¨˜éŒ„æª”æ¡ˆè©³æƒ…
            file_detail = {
                "path": str(rel_path),
                "lines": result.get("total_lines", 0),
                "functions": result.get("function_count", 0),
                "classes": result.get("class_count", 0),
                "complexity": complexity,
                "has_type_hints": result.get("has_type_hints", False),
                "has_docstrings": result.get("has_docstrings", False),
            }
            stats["file_details"].append(file_detail)

            # æŒ‰æ¨¡çµ„çµ±è¨ˆ
            parts = rel_path.parts
            if len(parts) > 0:
                module = parts[0]
                stats["module_analysis"][module]["files"] += 1
                stats["module_analysis"][module]["lines"] += result.get(
                    "total_lines", 0
                )
                stats["module_analysis"][module]["functions"] += result.get(
                    "function_count", 0
                )
                stats["module_analysis"][module]["classes"] += result.get(
                    "class_count", 0
                )

            # æª¢æŸ¥èªæ³•éŒ¯èª¤
            if "syntax_error" in result:
                stats["syntax_errors"].append(
                    {
                        "path": str(rel_path),
                        "error": result["syntax_error"],
                    }
                )

    # æ‰¾å‡ºæœ€è¤‡é›œçš„æª”æ¡ˆï¼ˆå‰ 20ï¼‰
    stats["file_details"].sort(key=lambda x: x["complexity"], reverse=True)
    stats["top_complex_files"] = stats["file_details"][:20]

    # ç”Ÿæˆå ±å‘Š
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆ JSON å ±å‘Š
    json_output = (
        output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(json_output, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆæ–‡å­—å ±å‘Š
    txt_output = (
        output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    )
    with open(txt_output, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("AIVA ç¨‹å¼ç¢¼åº«åˆ†æå ±å‘Š\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        f.write("æ•´é«”çµ±è¨ˆ\n")
        f.write("-" * 80 + "\n")
        f.write(f"ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}\n")
        f.write(f"ç¸½è¡Œæ•¸: {stats['total_lines']:,}\n")
        f.write(f"  - ç¨‹å¼ç¢¼è¡Œ: {stats['total_code_lines']:,}\n")
        f.write(f"  - è¨»è§£è¡Œ: {stats['total_comment_lines']:,}\n")
        f.write(f"  - ç©ºç™½è¡Œ: {stats['total_blank_lines']:,}\n")
        f.write(f"ç¸½å‡½æ•¸æ•¸: {stats['total_functions']}\n")
        f.write(f"ç¸½é¡åˆ¥æ•¸: {stats['total_classes']}\n")
        f.write(f"ç¸½å°å…¥æ•¸: {stats['total_imports']}\n")
        f.write(
            f"å¹³å‡è¤‡é›œåº¦: {stats['total_complexity'] / max(stats['total_files'], 1):.2f}\n"
        )
        f.write("\n")

        f.write("ç¨‹å¼ç¢¼å“è³ªæŒ‡æ¨™\n")
        f.write("-" * 80 + "\n")
        type_hint_pct = (
            stats["files_with_type_hints"] / max(stats["total_files"], 1)
        ) * 100
        docstring_pct = (
            stats["files_with_docstrings"] / max(stats["total_files"], 1)
        ) * 100
        f.write(
            f"é¡å‹æç¤ºè¦†è“‹ç‡: {type_hint_pct:.1f}% ({stats['files_with_type_hints']}/{stats['total_files']})\n"
        )
        f.write(
            f"æ–‡æª”å­—ä¸²è¦†è“‹ç‡: {docstring_pct:.1f}% ({stats['files_with_docstrings']}/{stats['total_files']})\n"
        )
        f.write("\n")

        if stats["syntax_errors"]:
            f.write("èªæ³•éŒ¯èª¤\n")
            f.write("-" * 80 + "\n")
            for err in stats["syntax_errors"]:
                f.write(f"{err['path']}: {err['error']}\n")
            f.write("\n")

        f.write("æ¨¡çµ„åˆ†æ\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æ¨¡çµ„':<30} {'æª”æ¡ˆæ•¸':>10} {'è¡Œæ•¸':>12} {'å‡½æ•¸':>10} {'é¡åˆ¥':>10}\n")
        f.write("-" * 80 + "\n")
        for module, data in sorted(stats["module_analysis"].items()):
            f.write(
                f"{module:<30} {data['files']:>10} {data['lines']:>12,} "
                f"{data['functions']:>10} {data['classes']:>10}\n"
            )
        f.write("\n")

        f.write("æœ€è¤‡é›œçš„æª”æ¡ˆï¼ˆå‰ 20ï¼‰\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æª”æ¡ˆè·¯å¾‘':<60} {'è¤‡é›œåº¦':>10} {'è¡Œæ•¸':>8}\n")
        f.write("-" * 80 + "\n")
        for file_info in stats["top_complex_files"]:
            path = file_info["path"]
            if len(path) > 58:
                path = "..." + path[-55:]
            f.write(
                f"{path:<60} {file_info['complexity']:>10} {file_info['lines']:>8}\n"
            )
        f.write("\n")

        f.write("=" * 80 + "\n")
        f.write("å ±å‘ŠçµæŸ\n")
        f.write("=" * 80 + "\n")

    print("\nå ±å‘Šå·²ç”Ÿæˆ:")
    print(f"  JSON: {json_output}")
    print(f"  TXT: {txt_output}")

    return stats


def analyze_multilang_files(
    root_path: Path,
    ignore_patterns: list[str] | None = None,
) -> dict:
    """åˆ†æå¤šèªè¨€ç¨‹å¼ç¢¼æª”æ¡ˆ (Go, Rust, TypeScript).

    Args:
        root_path: è¦åˆ†æçš„æ ¹ç›®éŒ„
        ignore_patterns: è¦å¿½ç•¥çš„è·¯å¾‘æ¨¡å¼

    Returns:
        å¤šèªè¨€åˆ†æçµæœ
    """
    if ignore_patterns is None:
        ignore_patterns = [
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            "_out",
            "target",
            "build",
            "dist",
        ]

    multilang_stats: dict[str, Any] = {
        "go": {"files": [], "total_lines": 0, "total_files": 0},
        "rust": {"files": [], "total_lines": 0, "total_files": 0},
        "typescript": {"files": [], "total_lines": 0, "total_files": 0},
        "javascript": {"files": [], "total_lines": 0, "total_files": 0},
    }

    # æƒæ Go æª”æ¡ˆ
    for go_file in root_path.rglob("*.go"):
        if any(pattern in str(go_file) for pattern in ignore_patterns):
            continue
        try:
            content = go_file.read_text(encoding="utf-8")
            lines = len(content.splitlines())
            multilang_stats["go"]["files"].append(
                {
                    "path": str(go_file.relative_to(root_path)),
                    "lines": lines,
                }
            )
            multilang_stats["go"]["total_lines"] += lines
            multilang_stats["go"]["total_files"] += 1
        except Exception:
            pass

    # æƒæ Rust æª”æ¡ˆ
    for rs_file in root_path.rglob("*.rs"):
        if any(pattern in str(rs_file) for pattern in ignore_patterns):
            continue
        try:
            content = rs_file.read_text(encoding="utf-8")
            lines = len(content.splitlines())
            multilang_stats["rust"]["files"].append(
                {
                    "path": str(rs_file.relative_to(root_path)),
                    "lines": lines,
                }
            )
            multilang_stats["rust"]["total_lines"] += lines
            multilang_stats["rust"]["total_files"] += 1
        except Exception:
            pass

    # æƒæ TypeScript æª”æ¡ˆ
    for ts_file in root_path.rglob("*.ts"):
        if any(pattern in str(ts_file) for pattern in ignore_patterns):
            continue
        try:
            content = ts_file.read_text(encoding="utf-8")
            lines = len(content.splitlines())
            multilang_stats["typescript"]["files"].append(
                {
                    "path": str(ts_file.relative_to(root_path)),
                    "lines": lines,
                }
            )
            multilang_stats["typescript"]["total_lines"] += lines
            multilang_stats["typescript"]["total_files"] += 1
        except Exception:
            pass

    # æƒæ JavaScript æª”æ¡ˆ
    for js_file in root_path.rglob("*.js"):
        if any(pattern in str(js_file) for pattern in ignore_patterns):
            continue
        try:
            content = js_file.read_text(encoding="utf-8")
            lines = len(content.splitlines())
            multilang_stats["javascript"]["files"].append(
                {
                    "path": str(js_file.relative_to(root_path)),
                    "lines": lines,
                }
            )
            multilang_stats["javascript"]["total_lines"] += lines
            multilang_stats["javascript"]["total_files"] += 1
        except Exception:
            pass

    return multilang_stats


def main():
    """ä¸»å‡½æ•¸."""
    # è¨­å®šè·¯å¾‘
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    output_dir = project_root / "_out" / "analysis"

    print("AIVA ç¨‹å¼ç¢¼åº«åˆ†æå·¥å…· (å¤šèªè¨€æ”¯æ´)")
    print("=" * 80)
    print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {project_root}")
    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    print("=" * 80)
    print()

    # åŸ·è¡Œ Python åˆ†æ
    print("ğŸ“Š åˆ†æ Python ç¨‹å¼ç¢¼...")
    stats = analyze_directory(
        root_path=project_root / "services",
        output_dir=output_dir,
        max_files=1000,
    )

    # åŸ·è¡Œå¤šèªè¨€åˆ†æ
    print("\nğŸ“Š åˆ†æå¤šèªè¨€ç¨‹å¼ç¢¼...")
    multilang_stats = analyze_multilang_files(
        root_path=project_root / "services",
    )

    # å°‡å¤šèªè¨€çµ±è¨ˆåŠ å…¥ä¸»å ±å‘Š
    stats["multilang"] = multilang_stats

    # æ›´æ–° JSON å ±å‘Š
    json_output = (
        output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(json_output, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # é¡¯ç¤ºæ‘˜è¦
    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼æ‘˜è¦:")
    print("=" * 80)
    print("\nğŸ Python:")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
    print(f"  ç¸½è¡Œæ•¸: {stats['total_lines']:,}")
    print(f"  ç¸½å‡½æ•¸æ•¸: {stats['total_functions']}")
    print(f"  ç¸½é¡åˆ¥æ•¸: {stats['total_classes']}")
    print(
        f"  å¹³å‡è¤‡é›œåº¦: {stats['total_complexity'] / max(stats['total_files'], 1):.2f}"
    )

    type_hint_pct = (
        stats["files_with_type_hints"] / max(stats["total_files"], 1)
    ) * 100
    docstring_pct = (
        stats["files_with_docstrings"] / max(stats["total_files"], 1)
    ) * 100
    print(f"  é¡å‹æç¤ºè¦†è“‹ç‡: {type_hint_pct:.1f}%")
    print(f"  æ–‡æª”å­—ä¸²è¦†è“‹ç‡: {docstring_pct:.1f}%")

    # é¡¯ç¤ºå¤šèªè¨€çµ±è¨ˆ
    total_multilang_lines = 0
    total_multilang_files = 0

    if multilang_stats["go"]["total_files"] > 0:
        print("\nğŸ”· Go:")
        print(f"  ç¸½æª”æ¡ˆæ•¸: {multilang_stats['go']['total_files']}")
        print(f"  ç¸½è¡Œæ•¸: {multilang_stats['go']['total_lines']:,}")
        print(
            f"  å¹³å‡è¡Œæ•¸: {multilang_stats['go']['total_lines'] / multilang_stats['go']['total_files']:.1f}"
        )
        total_multilang_lines += multilang_stats["go"]["total_lines"]
        total_multilang_files += multilang_stats["go"]["total_files"]

    if multilang_stats["rust"]["total_files"] > 0:
        print("\nğŸ¦€ Rust:")
        print(f"  ç¸½æª”æ¡ˆæ•¸: {multilang_stats['rust']['total_files']}")
        print(f"  ç¸½è¡Œæ•¸: {multilang_stats['rust']['total_lines']:,}")
        print(
            f"  å¹³å‡è¡Œæ•¸: {multilang_stats['rust']['total_lines'] / multilang_stats['rust']['total_files']:.1f}"
        )
        total_multilang_lines += multilang_stats["rust"]["total_lines"]
        total_multilang_files += multilang_stats["rust"]["total_files"]

    if multilang_stats["typescript"]["total_files"] > 0:
        print("\nğŸ“˜ TypeScript:")
        print(f"  ç¸½æª”æ¡ˆæ•¸: {multilang_stats['typescript']['total_files']}")
        print(f"  ç¸½è¡Œæ•¸: {multilang_stats['typescript']['total_lines']:,}")
        print(
            f"  å¹³å‡è¡Œæ•¸: {multilang_stats['typescript']['total_lines'] / multilang_stats['typescript']['total_files']:.1f}"
        )
        total_multilang_lines += multilang_stats["typescript"]["total_lines"]
        total_multilang_files += multilang_stats["typescript"]["total_files"]

    if multilang_stats["javascript"]["total_files"] > 0:
        print("\nğŸ“œ JavaScript:")
        print(f"  ç¸½æª”æ¡ˆæ•¸: {multilang_stats['javascript']['total_files']}")
        print(f"  ç¸½è¡Œæ•¸: {multilang_stats['javascript']['total_lines']:,}")
        print(
            f"  å¹³å‡è¡Œæ•¸: {multilang_stats['javascript']['total_lines'] / multilang_stats['javascript']['total_files']:.1f}"
        )
        total_multilang_lines += multilang_stats["javascript"]["total_lines"]
        total_multilang_files += multilang_stats["javascript"]["total_files"]

    # ç¸½è¨ˆ
    total_all_lines = stats["total_lines"] + total_multilang_lines
    total_all_files = stats["total_files"] + total_multilang_files

    print("\n" + "=" * 80)
    print("ğŸ“Š å…¨å°ˆæ¡ˆçµ±è¨ˆ:")
    print("=" * 80)
    print(f"ç¸½æª”æ¡ˆæ•¸: {total_all_files}")
    print(f"ç¸½è¡Œæ•¸: {total_all_lines:,}")
    print(
        f"  - Python: {stats['total_lines']:,} ({stats['total_lines']/total_all_lines*100:.1f}%)"
    )
    if total_multilang_lines > 0:
        print(
            f"  - å…¶ä»–èªè¨€: {total_multilang_lines:,} ({total_multilang_lines/total_all_lines*100:.1f}%)"
        )
    print("=" * 80)


if __name__ == "__main__":
    main()
