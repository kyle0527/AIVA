"""
AIVA AI ç³»çµ±æ¢ç´¢å™¨ v3.0 - æ··åˆæ¶æ§‹ç‰ˆæœ¬
åŸºæ–¼ aiva_common è·¨èªè¨€æ¶æ§‹çš„å°ˆæ¥­åˆ†æç³»çµ±

ä¸»è¦ç‰¹é»:
1. åˆ†å±¤åˆ†æç­–ç•¥ (å¿«é€Ÿæƒæ + æ·±åº¦åˆ†æ + è·¨èªè¨€æ•´åˆ)
2. åˆ©ç”¨ AIVA ç¾æœ‰çš„ Schema SOT ç³»çµ±
3. æ•´åˆå°ˆæ¥­åˆ†æå·¥å…· (Go AST, Rust Syn, TypeScript API)
4. ç¬¦åˆ AIVA çµ±ä¸€æ¶ˆæ¯æ ¼å¼å’Œæ¶æ§‹è¨­è¨ˆ
5. æ™ºæ…§åˆ†æç­–ç•¥ (æ ¹æ“šè®Šæ›´ç¨‹åº¦æ±ºå®šåˆ†ææ·±åº¦)

ç‰ˆæœ¬: 3.0.0
ä½œè€…: AIVA Development Team
æ—¥æœŸ: 2025-10-28
"""

import asyncio
import json
import logging
import os
import sys
import subprocess
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
import argparse

# æ·»åŠ  AIVA Common è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "services"))

try:
    # å˜—è©¦å°å…¥ AIVA Common Schema - ä¿®æ­£å°å…¥è·¯å¾‘
    from aiva_common.schemas.base import MessageHeader
    from aiva_common.schemas.findings import Target, Vulnerability, FindingPayload
    from aiva_common.schemas.messaging import AivaMessage
    from aiva_common.enums import ModuleName
    AIVA_SCHEMAS_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… AIVA Common Schemas è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    AIVA_SCHEMAS_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ AIVA Common Schemas è¼‰å…¥å¤±æ•—: {e}")
    logger.info("ğŸ”„ ä½¿ç”¨å…§å»º Schema å®šç¾©")

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(name)s - %(message)s',
    datefmt='%Y-%m-%dT%H:%M:%S%z'
)

@dataclass
class BuiltinMessageHeader:
    """å…§å»ºæ¶ˆæ¯æ¨™é ­å®šç¾©"""
    message_id: str
    trace_id: str
    source_module: str
    timestamp: datetime
    version: str = "3.0.0"

@dataclass  
class LayeredAnalysis:
    """åˆ†å±¤åˆ†æçµæœ"""
    file_path: str
    language: str
    layer1_quick: Dict[str, Any]  # å¿«é€Ÿæƒæçµæœ
    layer2_deep: Optional[Dict[str, Any]] = None  # æ·±åº¦åˆ†æçµæœ
    layer3_cross_lang: Optional[Dict[str, Any]] = None  # è·¨èªè¨€åˆ†æçµæœ
    analysis_time: float = 0.0
    needs_deep_analysis: bool = False

@dataclass
class ProfessionalAnalysisResult:
    """å°ˆæ¥­å·¥å…·åˆ†æçµæœ"""
    functions: List[Dict[str, Any]]
    classes: List[Dict[str, Any]]
    interfaces: List[Dict[str, Any]]
    imports: List[str]
    exports: List[str]
    dependencies: List[str]
    complexity_metrics: Dict[str, float]
    type_information: Dict[str, Any]

class GoASTAnalyzer:
    """Go AST å°ˆæ¥­åˆ†æå™¨"""
    
    def __init__(self):
        self.analyzer_script = "services/features/common/go/analyzer/ast_analyzer.go"
        self.available = self._check_availability()
    
    def _check_availability(self) -> bool:
        """æª¢æŸ¥ Go å·¥å…·éˆæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['go', 'version'], 
                                 capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    async def analyze(self, file_path: str) -> Optional[ProfessionalAnalysisResult]:
        """ä½¿ç”¨ Go AST åˆ†æ Go æª”æ¡ˆ"""
        if not self.available:
            logger.warning("Go å·¥å…·éˆä¸å¯ç”¨ï¼Œè·³éå°ˆæ¥­åˆ†æ")
            return None
        
        try:
            # å‰µå»ºè‡¨æ™‚åˆ†æå™¨è…³æœ¬
            analyzer_code = self._generate_analyzer_script()
            
            with open("temp_go_analyzer.go", 'w') as f:
                f.write(analyzer_code)
            
            # åŸ·è¡Œ Go åˆ†æ
            result = await asyncio.create_subprocess_exec(
                'go', 'run', 'temp_go_analyzer.go', file_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await result.communicate()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            Path("temp_go_analyzer.go").unlink(missing_ok=True)
            
            if result.returncode == 0:
                analysis_data = json.loads(stdout.decode())
                return ProfessionalAnalysisResult(**analysis_data)
            else:
                logger.error(f"Go AST åˆ†æå¤±æ•—: {stderr.decode()}")
                return None
                
        except Exception as e:
            logger.error(f"Go AST åˆ†æç•°å¸¸: {e}")
            return None
    
    def _generate_analyzer_script(self) -> str:
        """ç”Ÿæˆ Go AST åˆ†æå™¨è…³æœ¬"""
        return '''package main

import (
    "encoding/json"
    "fmt"
    "go/ast"
    "go/parser"
    "go/token"
    "os"
    "strings"
)

type Function struct {
    Name       string `json:"name"`
    Parameters []string `json:"parameters"`
    Returns    []string `json:"returns"`
    Line       int    `json:"line"`
}

type Class struct {
    Name   string   `json:"name"`
    Fields []string `json:"fields"`
    Line   int      `json:"line"`
}

type AnalysisResult struct {
    Functions        []Function        `json:"functions"`
    Classes          []Class           `json:"classes"`
    Interfaces       []Class           `json:"interfaces"`
    Imports          []string          `json:"imports"`
    Exports          []string          `json:"exports"`
    Dependencies     []string          `json:"dependencies"`
    ComplexityMetrics map[string]float64 `json:"complexity_metrics"`
    TypeInformation  map[string]interface{} `json:"type_information"`
}

func main() {
    if len(os.Args) < 2 {
        fmt.Fprintf(os.Stderr, "Usage: %s <go-file>\\n", os.Args[0])
        os.Exit(1)
    }
    
    filename := os.Args[1]
    
    fset := token.NewFileSet()
    node, err := parser.ParseFile(fset, filename, nil, parser.ParseComments)
    if err != nil {
        fmt.Fprintf(os.Stderr, "Error parsing file: %v\\n", err)
        os.Exit(1)
    }
    
    result := AnalysisResult{
        Functions:         []Function{},
        Classes:           []Class{},
        Interfaces:        []Class{},
        Imports:           []string{},
        Exports:           []string{},
        Dependencies:      []string{},
        ComplexityMetrics: make(map[string]float64),
        TypeInformation:   make(map[string]interface{}),
    }
    
    // æå–å°å…¥
    for _, imp := range node.Imports {
        path := strings.Trim(imp.Path.Value, "\\"")
        result.Imports = append(result.Imports, path)
        result.Dependencies = append(result.Dependencies, path)
    }
    
    // éæ­· AST ç¯€é»
    ast.Inspect(node, func(n ast.Node) bool {
        switch x := n.(type) {
        case *ast.FuncDecl:
            if x.Name.IsExported() {
                result.Exports = append(result.Exports, x.Name.Name)
            }
            
            function := Function{
                Name:       x.Name.Name,
                Parameters: []string{},
                Returns:    []string{},
                Line:       fset.Position(x.Pos()).Line,
            }
            
            // æå–åƒæ•¸
            if x.Type.Params != nil {
                for _, param := range x.Type.Params.List {
                    for _, name := range param.Names {
                        function.Parameters = append(function.Parameters, name.Name)
                    }
                }
            }
            
            result.Functions = append(result.Functions, function)
            
        case *ast.TypeSpec:
            if structType, ok := x.Type.(*ast.StructType); ok {
                class := Class{
                    Name:   x.Name.Name,
                    Fields: []string{},
                    Line:   fset.Position(x.Pos()).Line,
                }
                
                // æå–çµæ§‹é«”æ¬„ä½
                for _, field := range structType.Fields.List {
                    for _, name := range field.Names {
                        class.Fields = append(class.Fields, name.Name)
                    }
                }
                
                result.Classes = append(result.Classes, class)
                
                if x.Name.IsExported() {
                    result.Exports = append(result.Exports, x.Name.Name)
                }
            }
            
            if _, ok := x.Type.(*ast.InterfaceType); ok {
                interface_ := Class{
                    Name:   x.Name.Name,
                    Fields: []string{},
                    Line:   fset.Position(x.Pos()).Line,
                }
                
                result.Interfaces = append(result.Interfaces, interface_)
                
                if x.Name.IsExported() {
                    result.Exports = append(result.Exports, x.Name.Name)
                }
            }
        }
        return true
    })
    
    // è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
    result.ComplexityMetrics["function_count"] = float64(len(result.Functions))
    result.ComplexityMetrics["struct_count"] = float64(len(result.Classes))
    result.ComplexityMetrics["interface_count"] = float64(len(result.Interfaces))
    
    // è¼¸å‡º JSON çµæœ
    output, err := json.Marshal(result)
    if err != nil {
        fmt.Fprintf(os.Stderr, "Error marshaling result: %v\\n", err)
        os.Exit(1)
    }
    
    fmt.Print(string(output))
}'''

class RustSynAnalyzer:
    """Rust Syn å°ˆæ¥­åˆ†æå™¨"""
    
    def __init__(self):
        self.available = self._check_availability()
    
    def _check_availability(self) -> bool:
        """æª¢æŸ¥ Rust å·¥å…·éˆæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['rustc', '--version'], 
                                 capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    async def analyze(self, file_path: str) -> Optional[ProfessionalAnalysisResult]:
        """ä½¿ç”¨ Rust Syn åˆ†æ Rust æª”æ¡ˆ"""
        if not self.available:
            logger.warning("Rust å·¥å…·éˆä¸å¯ç”¨ï¼Œè·³éå°ˆæ¥­åˆ†æ")
            return None
        
        try:
            # å‰µå»ºè‡¨æ™‚ Rust åˆ†æå™¨å°ˆæ¡ˆ
            analyzer_code = self._generate_analyzer_code()
            cargo_toml = self._generate_cargo_toml()
            
            temp_dir = Path("temp_rust_analyzer")
            temp_dir.mkdir(exist_ok=True)
            src_dir = temp_dir / "src"
            src_dir.mkdir(exist_ok=True)
            
            with open(temp_dir / "Cargo.toml", 'w') as f:
                f.write(cargo_toml)
            
            with open(src_dir / "main.rs", 'w') as f:
                f.write(analyzer_code)
            
            # åŸ·è¡Œ Rust åˆ†æ
            result = await asyncio.create_subprocess_exec(
                'cargo', 'run', '--manifest-path', str(temp_dir / "Cargo.toml"), 
                '--', file_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=temp_dir
            )
            
            stdout, stderr = await result.communicate()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            if result.returncode == 0:
                analysis_data = json.loads(stdout.decode())
                return ProfessionalAnalysisResult(**analysis_data)
            else:
                logger.error(f"Rust Syn åˆ†æå¤±æ•—: {stderr.decode()}")
                return None
                
        except Exception as e:
            logger.error(f"Rust Syn åˆ†æç•°å¸¸: {e}")
            return None
    
    def _generate_cargo_toml(self) -> str:
        """ç”Ÿæˆ Cargo.toml"""
        return '''[package]
name = "rust_analyzer"
version = "0.1.0"
edition = "2021"

[dependencies]
syn = { version = "2.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
'''
    
    def _generate_analyzer_code(self) -> str:
        """ç”Ÿæˆ Rust åˆ†æå™¨ä»£ç¢¼"""
        return '''use std::env;
use std::fs;
use syn::{parse_file, Item, ItemFn, ItemStruct, ItemEnum, ItemTrait, ItemUse};
use serde::{Serialize, Deserialize};
use std::collections::HashMap;

#[derive(Serialize, Deserialize)]
struct Function {
    name: String,
    parameters: Vec<String>,
    returns: Vec<String>,
    line: usize,
}

#[derive(Serialize, Deserialize)]
struct Class {
    name: String,
    fields: Vec<String>,
    line: usize,
}

#[derive(Serialize, Deserialize)]
struct AnalysisResult {
    functions: Vec<Function>,
    classes: Vec<Class>,
    interfaces: Vec<Class>,
    imports: Vec<String>,
    exports: Vec<String>,
    dependencies: Vec<String>,
    complexity_metrics: HashMap<String, f64>,
    type_information: HashMap<String, serde_json::Value>,
}

fn main() {
    let args: Vec<String> = env::args().collect();
    if args.len() < 2 {
        eprintln!("Usage: {} <rust-file>", args[0]);
        std::process::exit(1);
    }
    
    let filename = &args[1];
    let content = match fs::read_to_string(filename) {
        Ok(content) => content,
        Err(e) => {
            eprintln!("Error reading file: {}", e);
            std::process::exit(1);
        }
    };
    
    let ast = match parse_file(&content) {
        Ok(ast) => ast,
        Err(e) => {
            eprintln!("Error parsing file: {}", e);
            std::process::exit(1);
        }
    };
    
    let mut result = AnalysisResult {
        functions: Vec::new(),
        classes: Vec::new(),
        interfaces: Vec::new(),
        imports: Vec::new(),
        exports: Vec::new(),
        dependencies: Vec::new(),
        complexity_metrics: HashMap::new(),
        type_information: HashMap::new(),
    };
    
    for item in ast.items {
        match item {
            Item::Fn(ItemFn { sig, .. }) => {
                let function = Function {
                    name: sig.ident.to_string(),
                    parameters: sig.inputs.iter().map(|_| "param".to_string()).collect(),
                    returns: vec!["return_type".to_string()],
                    line: 0,
                };
                result.functions.push(function);
            }
            Item::Struct(ItemStruct { ident, fields, .. }) => {
                let class = Class {
                    name: ident.to_string(),
                    fields: match fields {
                        syn::Fields::Named(fields) => {
                            fields.named.iter().map(|f| f.ident.as_ref().unwrap().to_string()).collect()
                        }
                        _ => Vec::new(),
                    },
                    line: 0,
                };
                result.classes.push(class);
            }
            Item::Trait(ItemTrait { ident, .. }) => {
                let interface = Class {
                    name: ident.to_string(),
                    fields: Vec::new(),
                    line: 0,
                };
                result.interfaces.push(interface);
            }
            Item::Use(ItemUse { tree, .. }) => {
                let use_path = quote::quote!(#tree).to_string();
                result.imports.push(use_path.clone());
                result.dependencies.push(use_path);
            }
            _ => {}
        }
    }
    
    // è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
    result.complexity_metrics.insert("function_count".to_string(), result.functions.len() as f64);
    result.complexity_metrics.insert("struct_count".to_string(), result.classes.len() as f64);
    result.complexity_metrics.insert("trait_count".to_string(), result.interfaces.len() as f64);
    
    let output = serde_json::to_string(&result).unwrap();
    println!("{}", output);
}'''

class TypeScriptAPIAnalyzer:
    """TypeScript Compiler API å°ˆæ¥­åˆ†æå™¨"""
    
    def __init__(self):
        self.available = self._check_availability()
    
    def _check_availability(self) -> bool:
        """æª¢æŸ¥ Node.js å’Œ TypeScript æ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['node', '--version'], 
                                 capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    async def analyze(self, file_path: str) -> Optional[ProfessionalAnalysisResult]:
        """ä½¿ç”¨ TypeScript API åˆ†æ TypeScript/JavaScript æª”æ¡ˆ"""
        if not self.available:
            logger.warning("Node.js ä¸å¯ç”¨ï¼Œè·³é TypeScript å°ˆæ¥­åˆ†æ")
            return None
        
        try:
            # å‰µå»ºè‡¨æ™‚ TypeScript åˆ†æå™¨
            analyzer_code = self._generate_analyzer_script()
            
            with open("temp_ts_analyzer.js", 'w') as f:
                f.write(analyzer_code)
            
            # åŸ·è¡Œ TypeScript åˆ†æ
            result = await asyncio.create_subprocess_exec(
                'node', 'temp_ts_analyzer.js', file_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await result.communicate()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            Path("temp_ts_analyzer.js").unlink(missing_ok=True)
            
            if result.returncode == 0:
                analysis_data = json.loads(stdout.decode())
                return ProfessionalAnalysisResult(**analysis_data)
            else:
                logger.error(f"TypeScript API åˆ†æå¤±æ•—: {stderr.decode()}")
                return None
                
        except Exception as e:
            logger.error(f"TypeScript API åˆ†æç•°å¸¸: {e}") 
            return None
    
    def _generate_analyzer_script(self) -> str:
        """ç”Ÿæˆ TypeScript/JavaScript åˆ†æå™¨è…³æœ¬"""
        return '''const fs = require('fs');

function analyzeFile(filename) {
    try {
        const content = fs.readFileSync(filename, 'utf8');
        
        const result = {
            functions: [],
            classes: [],
            interfaces: [],
            imports: [],
            exports: [],
            dependencies: [],
            complexity_metrics: {},
            type_information: {}
        };
        
        // ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼åˆ†æ (åœ¨å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨ TypeScript Compiler API)
        
        // æå–å‡½æ•¸
        const functionRegex = /(?:function\\s+|const\\s+\\w+\\s*=\\s*(?:async\\s+)?(?:\\([^)]*\\)\\s*=>|function\\s*\\([^)]*\\)))|(?:async\\s+function\\s+)/g;
        let match;
        while ((match = functionRegex.exec(content)) !== null) {
            result.functions.push({
                name: "extracted_function",
                parameters: [],
                returns: [],
                line: content.substring(0, match.index).split('\\n').length
            });
        }
        
        // æå–é¡åˆ¥
        const classRegex = /class\\s+(\\w+)/g;
        while ((match = classRegex.exec(content)) !== null) {
            result.classes.push({
                name: match[1],
                fields: [],
                line: content.substring(0, match.index).split('\\n').length
            });
        }
        
        // æå–ä»‹é¢
        const interfaceRegex = /interface\\s+(\\w+)/g;
        while ((match = interfaceRegex.exec(content)) !== null) {
            result.interfaces.push({
                name: match[1],
                fields: [],
                line: content.substring(0, match.index).split('\\n').length
            });
        }
        
        // æå–å°å…¥
        const importRegex = /import\\s+.*?from\\s+['"]([^'"]+)['"]/g;
        while ((match = importRegex.exec(content)) !== null) {
            result.imports.push(match[1]);
            result.dependencies.push(match[1]);
        }
        
        // æå– require
        const requireRegex = /require\\s*\\(\\s*['"]([^'"]+)['"]\\s*\\)/g;
        while ((match = requireRegex.exec(content)) !== null) {
            result.imports.push(match[1]);
            result.dependencies.push(match[1]);
        }
        
        // è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
        result.complexity_metrics.function_count = result.functions.length;
        result.complexity_metrics.class_count = result.classes.length;
        result.complexity_metrics.interface_count = result.interfaces.length;
        
        console.log(JSON.stringify(result));
        
    } catch (error) {
        console.error('Error analyzing file:', error);
        process.exit(1);
    }
}

if (process.argv.length < 3) {
    console.error('Usage: node analyzer.js <file>');
    process.exit(1);
}

analyzeFile(process.argv[2]);'''

class HybridSystemExplorer:
    """æ··åˆæ¶æ§‹ç³»çµ±æ¢ç´¢å™¨"""
    
    def __init__(self, workspace_root: str = None):
        self.workspace_root = Path(workspace_root or os.getcwd())
        
        # åˆå§‹åŒ–å°ˆæ¥­åˆ†æå™¨
        self.go_analyzer = GoASTAnalyzer()
        self.rust_analyzer = RustSynAnalyzer()
        self.ts_analyzer = TypeScriptAPIAnalyzer()
        
        # å¾ v2.0 ç³»çµ±ç¹¼æ‰¿åŸºç¤åŠŸèƒ½
        from ai_system_explorer_v2 import IncrementalSystemExplorer
        self.base_explorer = IncrementalSystemExplorer(workspace_root)
        
        # AIVA Schema æ•´åˆ
        self.message_header_template = self._create_message_header_template()
        
    def _create_message_header_template(self) -> Union[Any, BuiltinMessageHeader]:
        """å‰µå»ºæ¶ˆæ¯æ¨™é ­æ¨¡æ¿"""
        if AIVA_SCHEMAS_AVAILABLE:
            return MessageHeader
        else:
            return BuiltinMessageHeader
    
    async def hybrid_explore(self, detailed: bool = False, 
                           force_professional: bool = False) -> Dict[str, Any]:
        """åŸ·è¡Œæ··åˆæ¶æ§‹æ¢ç´¢"""
        logger.info("ğŸ” å•Ÿå‹•æ··åˆæ¶æ§‹ç³»çµ±æ¢ç´¢...")
        
        start_time = datetime.now()
        
        # Layer 1: å¿«é€Ÿæƒæ (ä½¿ç”¨ v2.0 ç³»çµ±)
        logger.info("ğŸš€ Layer 1: åŸ·è¡Œå¿«é€Ÿæƒæ...")
        base_results = await self.base_explorer.incremental_explore()
        
        # Layer 2: å°ˆæ¥­å·¥å…·æ·±åº¦åˆ†æ
        enhanced_results = {}
        if detailed or force_professional:
            logger.info("ğŸ”¬ Layer 2: åŸ·è¡Œå°ˆæ¥­å·¥å…·æ·±åº¦åˆ†æ...")
            enhanced_results = await self._professional_analysis(base_results)
        
        # Layer 3: è·¨èªè¨€æ•´åˆåˆ†æ
        if AIVA_SCHEMAS_AVAILABLE:
            logger.info("ğŸŒ Layer 3: åŸ·è¡Œè·¨èªè¨€æ•´åˆåˆ†æ...")
            integrated_results = await self._cross_language_integration(
                base_results, enhanced_results)
        else:
            integrated_results = enhanced_results
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = self._generate_hybrid_report(
            base_results, enhanced_results, integrated_results, duration)
        
        logger.info(f"âœ… æ··åˆæ¶æ§‹æ¢ç´¢å®Œæˆ (è€—æ™‚: {duration:.2f}ç§’)")
        return final_report
    
    async def _professional_analysis(self, base_results) -> Dict[str, Any]:
        """åŸ·è¡Œå°ˆæ¥­å·¥å…·åˆ†æ"""
        professional_results = {}
        
        for module_id, module_analysis in base_results.modules.items():
            logger.info(f"ğŸ”¬ å°ˆæ¥­åˆ†ææ¨¡çµ„: {module_analysis.name}")
            
            module_professional = {
                'go_analysis': [],
                'rust_analysis': [],
                'typescript_analysis': [],
                'enhanced_metrics': {}
            }
            
            # åˆ†æå„èªè¨€æª”æ¡ˆ
            for file_analysis in module_analysis.files:
                file_path = file_analysis.path
                language = file_analysis.language
                
                if language == 'Go' and self.go_analyzer.available:
                    go_result = await self.go_analyzer.analyze(file_path)
                    if go_result:
                        module_professional['go_analysis'].append({
                            'file': file_path,
                            'analysis': asdict(go_result)
                        })
                
                elif language == 'Rust' and self.rust_analyzer.available:
                    rust_result = await self.rust_analyzer.analyze(file_path)
                    if rust_result:
                        module_professional['rust_analysis'].append({
                            'file': file_path,
                            'analysis': asdict(rust_result)
                        })
                
                elif language in ['TypeScript', 'JavaScript'] and self.ts_analyzer.available:
                    ts_result = await self.ts_analyzer.analyze(file_path)
                    if ts_result:
                        module_professional['typescript_analysis'].append({
                            'file': file_path,
                            'analysis': asdict(ts_result)
                        })
            
            # è¨ˆç®—å¢å¼·æŒ‡æ¨™
            module_professional['enhanced_metrics'] = self._calculate_enhanced_metrics(
                module_professional)
            
            professional_results[module_id] = module_professional
        
        return professional_results
    
    async def _cross_language_integration(self, base_results, professional_results) -> Dict[str, Any]:
        """åŸ·è¡Œè·¨èªè¨€æ•´åˆåˆ†æ"""
        integration_results = {}
        
        # åˆ†æè·¨èªè¨€ä¾è³´é—œä¿‚
        cross_lang_dependencies = self._analyze_cross_language_dependencies(
            base_results, professional_results)
        
        # ç”Ÿæˆ AIVA æ¨™æº–æ ¼å¼å ±å‘Š
        aiva_findings = []
        for module_id, module_analysis in base_results.modules.items():
            finding = self._create_aiva_finding(
                module_id, module_analysis, 
                professional_results.get(module_id, {}))
            aiva_findings.append(finding)
        
        integration_results = {
            'cross_language_dependencies': cross_lang_dependencies,
            'aiva_findings': aiva_findings,
            'schema_compliance': True,
            'integration_health': self._calculate_integration_health(
                cross_lang_dependencies)
        }
        
        return integration_results
    
    def _analyze_cross_language_dependencies(self, base_results, professional_results) -> Dict[str, Any]:
        """åˆ†æè·¨èªè¨€ä¾è³´é—œä¿‚"""
        dependencies = {
            'python_to_go': [],
            'python_to_rust': [],
            'python_to_typescript': [],
            'go_to_python': [],
            'rust_to_python': [],
            'typescript_to_python': [],
            'ffi_calls': [],
            'subprocess_calls': [],
            'schema_mappings': []
        }
        
        # åŸºæ–¼å°ˆæ¥­åˆ†æçµæœæª¢æ¸¬è·¨èªè¨€èª¿ç”¨
        for module_id, professional_data in professional_results.items():
            # æª¢æ¸¬ Go ä¸­çš„ Python èª¿ç”¨
            for go_analysis in professional_data.get('go_analysis', []):
                for dep in go_analysis['analysis']['dependencies']:
                    if 'python' in dep.lower():
                        dependencies['go_to_python'].append({
                            'module': module_id,
                            'file': go_analysis['file'],
                            'dependency': dep
                        })
            
            # æª¢æ¸¬ Rust ä¸­çš„ Python ç¶å®š
            for rust_analysis in professional_data.get('rust_analysis', []):
                for dep in rust_analysis['analysis']['dependencies']:
                    if 'pyo3' in dep.lower() or 'python' in dep.lower():
                        dependencies['rust_to_python'].append({
                            'module': module_id,
                            'file': rust_analysis['file'],
                            'dependency': dep
                        })
        
        return dependencies
    
    def _create_aiva_finding(self, module_id: str, module_analysis, professional_data: Dict) -> Dict[str, Any]:
        """å‰µå»º AIVA æ¨™æº–æ ¼å¼çš„ Finding"""
        if AIVA_SCHEMAS_AVAILABLE:
            # ä½¿ç”¨çœŸæ­£çš„ AIVA Schema
            header = MessageHeader(
                message_id=f"exploration_{module_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                trace_id=f"trace_{hashlib.md5(module_id.encode()).hexdigest()[:8]}",
                source_module=ModuleName.CORE,  # ä½¿ç”¨æœ‰æ•ˆçš„æ¨¡çµ„åç¨±
                timestamp=datetime.now()
            )
        else:
            # ä½¿ç”¨å…§å»º Schema
            header = BuiltinMessageHeader(
                message_id=f"exploration_{module_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                trace_id=f"trace_{hashlib.md5(module_id.encode()).hexdigest()[:8]}",
                source_module="CoreModule",  # ä½¿ç”¨å­—ä¸²å½¢å¼çš„æœ‰æ•ˆæ¨¡çµ„åç¨±
                timestamp=datetime.now()
            )
        
        finding = {
            'header': header.model_dump() if AIVA_SCHEMAS_AVAILABLE else asdict(header),
            'module_id': module_id,
            'module_name': module_analysis.name,
            'health_score': module_analysis.health_score,
            'analysis_type': 'system_exploration',
            'professional_analysis': professional_data,
            'findings': {
                'language_distribution': self._calculate_language_distribution(module_analysis),
                'complexity_metrics': self._extract_complexity_metrics(professional_data),
                'cross_language_calls': module_analysis.cross_language_calls,
                'issues': module_analysis.issues,
                'warnings': module_analysis.warnings
            },
            'recommendations': self._generate_recommendations(module_analysis, professional_data)
        }
        
        return finding
    
    def _calculate_enhanced_metrics(self, professional_data: Dict) -> Dict[str, float]:
        """è¨ˆç®—å¢å¼·æŒ‡æ¨™"""
        metrics = {}
        
        # Go æŒ‡æ¨™
        go_functions = sum(len(analysis['analysis']['functions']) 
                          for analysis in professional_data['go_analysis'])
        go_structs = sum(len(analysis['analysis']['classes']) 
                        for analysis in professional_data['go_analysis'])
        
        # Rust æŒ‡æ¨™
        rust_functions = sum(len(analysis['analysis']['functions']) 
                           for analysis in professional_data['rust_analysis'])
        rust_structs = sum(len(analysis['analysis']['classes']) 
                         for analysis in professional_data['rust_analysis'])
        
        # TypeScript æŒ‡æ¨™
        ts_functions = sum(len(analysis['analysis']['functions']) 
                         for analysis in professional_data['typescript_analysis'])
        ts_classes = sum(len(analysis['analysis']['classes']) 
                        for analysis in professional_data['typescript_analysis'])
        
        metrics.update({
            'go_function_count': go_functions,
            'go_struct_count': go_structs,
            'rust_function_count': rust_functions,
            'rust_struct_count': rust_structs,
            'typescript_function_count': ts_functions,
            'typescript_class_count': ts_classes,
            'total_professional_functions': go_functions + rust_functions + ts_functions,
            'total_professional_types': go_structs + rust_structs + ts_classes
        })
        
        return metrics
    
    def _calculate_language_distribution(self, module_analysis) -> Dict[str, int]:
        """è¨ˆç®—èªè¨€åˆ†å¸ƒ"""
        distribution = {}
        for file_analysis in module_analysis.files:
            lang = file_analysis.language
            distribution[lang] = distribution.get(lang, 0) + file_analysis.line_count
        return distribution
    
    def _extract_complexity_metrics(self, professional_data: Dict) -> Dict[str, Any]:
        """æå–è¤‡é›œåº¦æŒ‡æ¨™"""
        complexity = {}
        
        for analysis_type in ['go_analysis', 'rust_analysis', 'typescript_analysis']:
            for analysis in professional_data.get(analysis_type, []):
                if 'complexity_metrics' in analysis['analysis']:
                    for metric, value in analysis['analysis']['complexity_metrics'].items():
                        key = f"{analysis_type}_{metric}"
                        complexity[key] = complexity.get(key, 0) + value
        
        return complexity
    
    def _generate_recommendations(self, module_analysis, professional_data: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼å¥åº·åˆ†æ•¸çš„å»ºè­°
        if module_analysis.health_score < 0.8:
            recommendations.append("æ¨¡çµ„å¥åº·åˆ†æ•¸åä½ï¼Œå»ºè­°æª¢æŸ¥ç¨‹å¼ç¢¼å“è³ª")
        
        # åŸºæ–¼å°ˆæ¥­åˆ†æçš„å»ºè­°
        enhanced_metrics = professional_data.get('enhanced_metrics', {})
        
        if enhanced_metrics.get('total_professional_functions', 0) > 100:
            recommendations.append("å‡½æ•¸æ•¸é‡è¼ƒå¤šï¼Œå»ºè­°è€ƒæ…®æ¨¡çµ„åŒ–é‡æ§‹")
        
        if len(module_analysis.cross_language_calls) > 10:
            recommendations.append("è·¨èªè¨€èª¿ç”¨è¼ƒå¤šï¼Œå»ºè­°æª¢æŸ¥æ•´åˆè¤‡é›œåº¦")
        
        # åŸºæ–¼èªè¨€åˆ†å¸ƒçš„å»ºè­°
        lang_dist = self._calculate_language_distribution(module_analysis)
        if len(lang_dist) > 3:
            recommendations.append("ä½¿ç”¨å¤šç¨®ç¨‹å¼èªè¨€ï¼Œå»ºè­°çµ±ä¸€æŠ€è¡“æ£§")
        
        return recommendations
    
    def _calculate_integration_health(self, cross_lang_deps: Dict) -> float:
        """è¨ˆç®—æ•´åˆå¥åº·åº¦"""
        total_deps = sum(len(deps) for deps in cross_lang_deps.values())
        
        if total_deps == 0:
            return 1.0
        elif total_deps < 10:
            return 0.9
        elif total_deps < 20:
            return 0.8
        else:
            return 0.7
    
    def _generate_hybrid_report(self, base_results, professional_results, 
                               integration_results, duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ··åˆæ¶æ§‹å ±å‘Š"""
        report = {
            'report_id': f"hybrid_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'version': '3.0.0',
            'analysis_type': 'hybrid_architecture',
            'duration_seconds': duration,
            
            # Layer 1 çµæœ
            'layer1_quick_scan': {
                'overall_health': base_results.overall_health,
                'total_files': base_results.total_files,
                'total_lines': base_results.total_lines,
                'language_distribution': base_results.language_distribution,
                'modules_analyzed': len(base_results.modules)
            },
            
            # Layer 2 çµæœ
            'layer2_professional_analysis': {
                'tools_used': {
                    'go_ast': self.go_analyzer.available,
                    'rust_syn': self.rust_analyzer.available,
                    'typescript_api': self.ts_analyzer.available
                },
                'professional_results': professional_results
            },
            
            # Layer 3 çµæœ
            'layer3_integration': integration_results,
            
            # æ•´åˆå»ºè­°
            'recommendations': self._generate_system_recommendations(
                base_results, professional_results, integration_results),
            
            # AIVA ç›¸å®¹æ€§
            'aiva_compatibility': {
                'schemas_available': AIVA_SCHEMAS_AVAILABLE,
                'follows_aiva_patterns': True,
                'integration_ready': bool(integration_results)
            }
        }
        
        return report
    
    def _generate_system_recommendations(self, base_results, professional_results, 
                                       integration_results) -> List[str]:
        """ç”Ÿæˆç³»çµ±ç´šå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ•´é«”å¥åº·ç‹€æ…‹
        if base_results.overall_health < 0.8:
            recommendations.append("ğŸ¥ ç³»çµ±æ•´é«”å¥åº·ç‹€æ…‹éœ€è¦æ”¹å–„")
        
        # åŸºæ–¼è·¨èªè¨€æ•´åˆ
        if integration_results:
            integration_health = integration_results.get('integration_health', 1.0)
            if integration_health < 0.8:
                recommendations.append("ğŸ”— è·¨èªè¨€æ•´åˆè¤‡é›œåº¦éé«˜ï¼Œå»ºè­°ç°¡åŒ–")
        
        # åŸºæ–¼å°ˆæ¥­åˆ†æå·¥å…·å¯ç”¨æ€§
        tools_available = 0
        if self.go_analyzer.available:
            tools_available += 1
        if self.rust_analyzer.available:
            tools_available += 1  
        if self.ts_analyzer.available:
            tools_available += 1
        
        if tools_available < 2:
            recommendations.append("ğŸ› ï¸ å»ºè­°å®‰è£æ›´å¤šå°ˆæ¥­åˆ†æå·¥å…·ä»¥ç²å¾—æ›´æ·±å…¥çš„åˆ†æ")
        
        # AIVA Schema å»ºè­°
        if not AIVA_SCHEMAS_AVAILABLE:
            recommendations.append("ğŸ“‹ å»ºè­°ä¿®å¾© AIVA Common Schemas å°å…¥ä»¥ç²å¾—å®Œæ•´åŠŸèƒ½")
        
        return recommendations

async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="AIVA AI ç³»çµ±æ¢ç´¢å™¨ v3.0 - æ··åˆæ¶æ§‹ç‰ˆæœ¬")
    parser.add_argument("--workspace", "-w", type=str, help="å·¥ä½œç›®éŒ„è·¯å¾‘")
    parser.add_argument("--detailed", "-d", action="store_true", help="å•Ÿç”¨å°ˆæ¥­å·¥å…·æ·±åº¦åˆ†æ")
    parser.add_argument("--force-professional", "-f", action="store_true", help="å¼·åˆ¶ä½¿ç”¨å°ˆæ¥­å·¥å…·")
    parser.add_argument("--output", "-o", type=str, help="è¼¸å‡ºæ ¼å¼", default="both")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ··åˆæ¢ç´¢å™¨
    workspace = args.workspace or os.getcwd()
    explorer = HybridSystemExplorer(workspace)
    
    print("ğŸ” AIVA æ··åˆæ¶æ§‹ç³»çµ±æ¢ç´¢å™¨ v3.0")
    print(f"ğŸ“ å·¥ä½œç›®éŒ„: {workspace}")
    print(f"ğŸ§¬ AIVA Schemas: {'âœ… å¯ç”¨' if AIVA_SCHEMAS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print(f"ğŸ› ï¸ å°ˆæ¥­å·¥å…·: Go AST({'âœ…' if explorer.go_analyzer.available else 'âŒ'}), "
          f"Rust Syn({'âœ…' if explorer.rust_analyzer.available else 'âŒ'}), "
          f"TypeScript API({'âœ…' if explorer.ts_analyzer.available else 'âŒ'})")
    
    try:
        # åŸ·è¡Œæ··åˆæ¢ç´¢
        report = await explorer.hybrid_explore(
            detailed=args.detailed,
            force_professional=args.force_professional
        )
        
        # ä¿å­˜å ±å‘Š
        if args.output in ["json", "both"]:
            json_path = f"reports/ai_diagnostics/hybrid_report_{report['report_id']}.json"
            Path(json_path).parent.mkdir(parents=True, exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, default=str, ensure_ascii=False)
            print(f"ğŸ“„ JSON å ±å‘Š: {json_path}")
        
        if args.output in ["text", "both"]:
            text_path = f"reports/ai_diagnostics/hybrid_summary_{report['report_id']}.txt"
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write("AIVA æ··åˆæ¶æ§‹ç³»çµ±æ¢ç´¢å ±å‘Š\n")
                f.write("=" * 50 + "\n")
                f.write(f"å ±å‘Š ID: {report['report_id']}\n")
                f.write(f"åˆ†æé¡å‹: {report['analysis_type']}\n")
                f.write(f"æ¢ç´¢è€—æ™‚: {report['duration_seconds']:.2f}ç§’\n\n")
                
                # Layer 1 æ‘˜è¦
                layer1 = report['layer1_quick_scan']
                f.write("ğŸ“Š Layer 1 - å¿«é€Ÿæƒæçµæœ:\n")
                f.write("-" * 30 + "\n")
                f.write(f"æ•´é«”å¥åº·åº¦: {layer1['overall_health']:.2f}\n")
                f.write(f"ç¸½æª”æ¡ˆæ•¸: {layer1['total_files']}\n")
                f.write(f"ç¸½ç¨‹å¼ç¢¼è¡Œæ•¸: {layer1['total_lines']}\n")
                f.write(f"æ¨¡çµ„æ•¸é‡: {layer1['modules_analyzed']}\n\n")
                
                # èªè¨€åˆ†å¸ƒ
                f.write("ğŸŒ èªè¨€åˆ†å¸ƒ:\n")
                for lang, lines in layer1['language_distribution'].items():
                    percentage = (lines / layer1['total_lines']) * 100 if layer1['total_lines'] > 0 else 0
                    f.write(f"  {lang}: {lines} è¡Œ ({percentage:.1f}%)\n")
                f.write("\n")
                
                # Layer 2 æ‘˜è¦
                layer2 = report['layer2_professional_analysis']
                f.write("ğŸ”¬ Layer 2 - å°ˆæ¥­å·¥å…·åˆ†æ:\n")
                f.write("-" * 30 + "\n")
                for tool, available in layer2['tools_used'].items():
                    status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
                    f.write(f"  {tool}: {status}\n")
                f.write("\n")
                
                # Layer 3 æ‘˜è¦
                if 'layer3_integration' in report:
                    layer3 = report['layer3_integration']
                    f.write("ğŸŒ Layer 3 - è·¨èªè¨€æ•´åˆ:\n")
                    f.write("-" * 30 + "\n")
                    f.write(f"Schema ç›¸å®¹æ€§: {'âœ…' if layer3.get('schema_compliance', False) else 'âŒ'}\n")
                    f.write(f"æ•´åˆå¥åº·åº¦: {layer3.get('integration_health', 0.0):.2f}\n\n")
                
                # å»ºè­°
                f.write("ğŸ’¡ æ”¹é€²å»ºè­°:\n")
                f.write("-" * 30 + "\n")
                for rec in report['recommendations']:
                    f.write(f"  â€¢ {rec}\n")
            
            print(f"ğŸ“‹ æ–‡å­—å ±å‘Š: {text_path}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print(f"\nâœ… æ··åˆæ¶æ§‹æ¢ç´¢å®Œæˆ!")
        print(f"â±ï¸ æ¢ç´¢è€—æ™‚: {report['duration_seconds']:.2f}ç§’")
        print(f"ğŸ¥ æ•´é«”å¥åº·åº¦: {report['layer1_quick_scan']['overall_health']:.2f}")
        print(f"ğŸ“Š åˆ†ææª”æ¡ˆ: {report['layer1_quick_scan']['total_files']} å€‹")
        print(f"ğŸ“ ç¨‹å¼ç¢¼è¡Œæ•¸: {report['layer1_quick_scan']['total_lines']} è¡Œ")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ ä¸»è¦å»ºè­°:")
            for rec in report['recommendations'][:3]:  # é¡¯ç¤ºå‰3å€‹å»ºè­°
                print(f"  â€¢ {rec}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ¢ç´¢")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ¢ç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())