#!/usr/bin/env python3
"""
å‘é‡å­˜å„²çµ±ä¸€é·ç§»è…³æœ¬

å°‡ç¾æœ‰çš„æ–‡ä»¶å¼å‘é‡å­˜å„²ï¼ˆFAISS/numpyï¼‰é·ç§»åˆ° PostgreSQL + pgvector
è§£æ±ºæ•¸æ“šå­¤å³¶å•é¡Œï¼Œå¯¦ç¾çµ±ä¸€çš„å‘é‡å­˜å„²ç®¡ç†
"""

import asyncio
import logging
import sys
from pathlib import Path
from typing import Any

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from services.core.aiva_core.rag import UnifiedVectorStore, VectorStore

logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class VectorStorageMigration:
    """å‘é‡å­˜å„²é·ç§»ç®¡ç†å™¨"""

    def __init__(
        self,
        database_url: str = "postgresql://postgres:aiva123@localhost:5432/aiva_db",
        legacy_data_dirs: list[Path] | None = None,
    ):
        self.database_url = database_url
        self.legacy_data_dirs = legacy_data_dirs or [
            Path("./data/knowledge/vectors"),
            Path("./data/vectors"),
            Path("./services/core/aiva_core/rag/data"),
        ]

    async def scan_legacy_data(self) -> list[tuple[Path, dict[str, Any]]]:
        """æƒæèˆŠçš„å‘é‡å­˜å„²æ•¸æ“š"""
        logger.info("ğŸ” æƒæç¾æœ‰çš„å‘é‡å­˜å„²æ•¸æ“š...")
        
        found_stores = []
        
        for data_dir in self.legacy_data_dirs:
            if not data_dir.exists():
                logger.debug(f"ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
                continue
                
            # æª¢æŸ¥æ˜¯å¦æœ‰å‘é‡å­˜å„²æ–‡ä»¶
            vectors_file = data_dir / "vectors.npy"
            data_file = data_dir / "data.json"
            
            if vectors_file.exists() and data_file.exists():
                # åŠ è¼‰ä¸¦æª¢æŸ¥æ•¸æ“š
                try:
                    legacy_store = VectorStore(
                        backend="memory",
                        persist_directory=data_dir,
                    )
                    legacy_store.load()
                    
                    stats = legacy_store.get_statistics()
                    logger.info(f"ğŸ“ ç™¼ç¾å‘é‡å­˜å„²: {data_dir}")
                    logger.info(f"   - æ–‡æª”æ•¸é‡: {stats['total_documents']}")
                    logger.info(f"   - å¾Œç«¯é¡å‹: {stats['backend']}")
                    
                    found_stores.append((data_dir, stats))
                    
                except Exception as e:
                    logger.warning(f"è¼‰å…¥å‘é‡å­˜å„²å¤±æ•— {data_dir}: {str(e)}")
        
        logger.info(f"âœ… æƒæå®Œæˆï¼Œæ‰¾åˆ° {len(found_stores)} å€‹å‘é‡å­˜å„²")
        return found_stores

    async def migrate_single_store(
        self,
        source_dir: Path,
        target_table: str = "knowledge_vectors",
    ) -> int:
        """é·ç§»å–®å€‹å‘é‡å­˜å„²"""
        logger.info(f"ğŸšš é–‹å§‹é·ç§»å‘é‡å­˜å„²: {source_dir} -> {target_table}")
        
        try:
            # å‰µå»ºçµ±ä¸€å‘é‡å­˜å„²
            unified_store = UnifiedVectorStore(
                database_url=self.database_url,
                table_name=target_table,
                legacy_persist_directory=source_dir,
            )
            
            await unified_store.initialize()
            
            # ç²å–é·ç§»å¾Œçš„çµ±è¨ˆä¿¡æ¯
            stats = await unified_store.get_statistics()
            migrated_count = stats["total_documents"]
            
            logger.info(f"âœ… é·ç§»å®Œæˆ: {migrated_count} å€‹æ–‡æª”")
            
            await unified_store.close()
            return migrated_count
            
        except Exception as e:
            logger.error(f"âŒ é·ç§»å¤±æ•— {source_dir}: {str(e)}")
            raise

    async def create_backup(self, source_dir: Path) -> Path:
        """å‰µå»ºé·ç§»å‰çš„å‚™ä»½"""
        backup_dir = source_dir.parent / f"{source_dir.name}_backup"
        
        logger.info(f"ğŸ’¾ å‰µå»ºå‚™ä»½: {source_dir} -> {backup_dir}")
        
        import shutil
        shutil.copytree(source_dir, backup_dir, dirs_exist_ok=True)
        
        logger.info(f"âœ… å‚™ä»½å®Œæˆ: {backup_dir}")
        return backup_dir

    async def verify_migration(self, target_table: str = "knowledge_vectors") -> bool:
        """é©—è­‰é·ç§»çµæœ"""
        logger.info(f"ğŸ” é©—è­‰é·ç§»çµæœ: {target_table}")
        
        try:
            unified_store = UnifiedVectorStore(
                database_url=self.database_url,
                table_name=target_table,
            )
            
            await unified_store.initialize()
            
            # ç²å–çµ±è¨ˆä¿¡æ¯
            stats = await unified_store.get_statistics()
            logger.info(f"ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
            logger.info(f"   - ç¸½æ–‡æª”æ•¸: {stats['total_documents']}")
            logger.info(f"   - å¾Œç«¯é¡å‹: {stats['backend']}")
            logger.info(f"   - è¡¨åç¨±: {stats['table_name']}")
            
            # æ¸¬è©¦æœç´¢åŠŸèƒ½
            if stats['total_documents'] > 0:
                logger.info("ğŸ” æ¸¬è©¦æœç´¢åŠŸèƒ½...")
                results = await unified_store.search("test query", top_k=3)
                logger.info(f"   - æœç´¢çµæœæ•¸: {len(results)}")
                
                if results:
                    logger.info(f"   - é¦–å€‹çµæœåˆ†æ•¸: {results[0]['score']:.4f}")
            
            await unified_store.close()
            
            logger.info("âœ… é·ç§»é©—è­‰é€šé")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é·ç§»é©—è­‰å¤±æ•—: {str(e)}")
            return False

    async def full_migration(
        self,
        create_backups: bool = True,
        target_table: str = "knowledge_vectors",
    ) -> dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å‘é‡å­˜å„²é·ç§»"""
        logger.info("ğŸš€ é–‹å§‹å®Œæ•´çš„å‘é‡å­˜å„²é·ç§»...")
        
        migration_summary = {
            "total_stores": 0,
            "migrated_stores": 0,
            "total_documents": 0,
            "failed_stores": [],
            "backups_created": [],
        }
        
        try:
            # 1. æƒæç¾æœ‰æ•¸æ“š
            found_stores = await self.scan_legacy_data()
            migration_summary["total_stores"] = len(found_stores)
            
            if not found_stores:
                logger.info("âŒ æ²’æœ‰æ‰¾åˆ°éœ€è¦é·ç§»çš„å‘é‡å­˜å„²æ•¸æ“š")
                return migration_summary
            
            # 2. é·ç§»æ¯å€‹å‘é‡å­˜å„²
            for source_dir, _stats in found_stores:
                try:
                    # å‰µå»ºå‚™ä»½
                    if create_backups:
                        backup_dir = await self.create_backup(source_dir)
                        migration_summary["backups_created"].append(str(backup_dir))
                    
                    # åŸ·è¡Œé·ç§»
                    migrated_count = await self.migrate_single_store(
                        source_dir, target_table
                    )
                    
                    migration_summary["migrated_stores"] += 1
                    migration_summary["total_documents"] += migrated_count
                    
                except Exception as e:
                    logger.error(f"é·ç§»å¤±æ•— {source_dir}: {str(e)}")
                    migration_summary["failed_stores"].append({
                        "path": str(source_dir),
                        "error": str(e),
                    })
            
            # 3. é©—è­‰é·ç§»çµæœ
            verification_passed = await self.verify_migration(target_table)
            migration_summary["verification_passed"] = verification_passed
            
            # 4. è¼¸å‡ºç¸½çµ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ å‘é‡å­˜å„²é·ç§»ç¸½çµ")
            logger.info("="*60)
            logger.info(f"æ‰¾åˆ°çš„å­˜å„²: {migration_summary['total_stores']}")
            logger.info(f"æˆåŠŸé·ç§»: {migration_summary['migrated_stores']}")
            logger.info(f"ç¸½æ–‡æª”æ•¸: {migration_summary['total_documents']}")
            logger.info(f"å¤±æ•—æ•¸é‡: {len(migration_summary['failed_stores'])}")
            logger.info(f"é©—è­‰çµæœ: {'é€šé' if verification_passed else 'å¤±æ•—'}")
            
            if migration_summary["failed_stores"]:
                logger.error("å¤±æ•—çš„å­˜å„²:")
                for failed in migration_summary["failed_stores"]:
                    logger.error(f"  - {failed['path']}: {failed['error']}")
            
            if migration_summary["migrated_stores"] > 0:
                logger.info("ğŸ‰ å‘é‡å­˜å„²çµ±ä¸€é·ç§»å®Œæˆï¼")
            else:
                logger.warning("âš ï¸  æ²’æœ‰æˆåŠŸé·ç§»ä»»ä½•å‘é‡å­˜å„²")
                
            return migration_summary
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡å­˜å„²é·ç§»éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            migration_summary["global_error"] = str(e)
            return migration_summary


async def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ å•Ÿå‹•å‘é‡å­˜å„²çµ±ä¸€é·ç§»...")
    
    # å‰µå»ºé·ç§»ç®¡ç†å™¨
    migrator = VectorStorageMigration(
        database_url="postgresql://postgres:aiva123@localhost:5432/aiva_db",
        legacy_data_dirs=[
            Path("./data/knowledge/vectors"),
            Path("./data/vectors"),
            Path("./services/core/aiva_core/rag/data"),
            Path("./data/training/vectors"),  # å¯èƒ½çš„å…¶ä»–ä½ç½®
        ],
    )
    
    # åŸ·è¡Œå®Œæ•´é·ç§»
    result = await migrator.full_migration(
        create_backups=True,
        target_table="knowledge_vectors",
    )
    
    # æ ¹æ“šçµæœè¿”å›é€€å‡ºç¢¼
    if result.get("verification_passed", False) and result["migrated_stores"] > 0:
        logger.info("âœ… é·ç§»æˆåŠŸå®Œæˆ")
        return 0
    else:
        logger.error("âŒ é·ç§»æœªèƒ½æˆåŠŸå®Œæˆ")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)