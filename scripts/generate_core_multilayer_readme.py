#!/usr/bin/env python3
"""
AIVA Core æ¨¡çµ„å¤šå±¤æ¬¡ README ç”Ÿæˆå™¨
åŸºæ–¼ generate_multilayer_readme.py ç‚º Core æ¨¡çµ„å‰µå»ºå®Œæ•´çš„æ–‡ä»¶æ¶æ§‹
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from collections import defaultdict

class CoreMultiLayerReadmeGenerator:
    """Core æ¨¡çµ„å¤šå±¤æ¬¡ README ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base_dir = Path("services/core")
        self.output_dir = self.base_dir / "docs"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¼‰å…¥åˆ†æè³‡æ–™
        analysis_file = Path("_out/core_module_analysis_detailed.json")
        if analysis_file.exists():
            with open(analysis_file, 'r', encoding='utf-8') as f:
                self.analysis_data = json.load(f)
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°åˆ†ææ•¸æ“šï¼Œè«‹å…ˆé‹è¡Œ analyze_core_modules.py")
            self.analysis_data = []
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = self._calculate_statistics()
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        if not self.analysis_data:
            return {}
        
        total_files = len(self.analysis_data)
        total_code_lines = sum(item['code_lines'] for item in self.analysis_data)
        total_classes = sum(item['classes'] for item in self.analysis_data)
        total_functions = sum(item['functions'] for item in self.analysis_data)
        total_async = sum(item['async_functions'] for item in self.analysis_data)
        avg_complexity = sum(item['complexity_score'] for item in self.analysis_data) / total_files
        
        # æŒ‰åŠŸèƒ½åˆ†é¡
        ai_files = [f for f in self.analysis_data if 'ai_' in f['file'] or 'bio_neuron' in f['file'] or 'nlg' in f['file']]
        execution_files = [f for f in self.analysis_data if any(k in f['file'] for k in ['execution', 'task_', 'plan_'])]
        analysis_files = [f for f in self.analysis_data if 'analysis' in f['file'] or 'decision' in f['file']]
        storage_files = [f for f in self.analysis_data if any(k in f['file'] for k in ['storage', 'state', 'session'])]
        learning_files = [f for f in self.analysis_data if any(k in f['file'] for k in ['learning', 'training'])]
        
        # ä¾è³´åˆ†æ
        all_imports = defaultdict(int)
        for item in self.analysis_data:
            for module in item.get('import_modules', []):
                if module and not module.startswith('.'):
                    all_imports[module] += 1
        
        return {
            'total_files': total_files,
            'total_code_lines': total_code_lines,
            'total_classes': total_classes,
            'total_functions': total_functions,
            'total_async_functions': total_async,
            'avg_complexity': round(avg_complexity, 1),
            'ai_components': len(ai_files),
            'execution_components': len(execution_files),
            'analysis_components': len(analysis_files),
            'storage_components': len(storage_files),
            'learning_components': len(learning_files),
            'top_dependencies': sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    
    def generate_main_readme(self) -> str:
        """ç”Ÿæˆä¸» README - æ¶æ§‹ç¸½è¦½èˆ‡å°èˆª"""
        
        stats = self.stats
        
        template = f"""# AIVA Core æ¨¡çµ„ - AIé©…å‹•æ ¸å¿ƒå¼•æ“æ¶æ§‹

> **ğŸ¯ å¿«é€Ÿå°èˆª**: é¸æ“‡æ‚¨çš„è§’è‰²å’Œéœ€æ±‚ï¼Œæ‰¾åˆ°æœ€é©åˆçš„æ–‡ä»¶
> 
> - ğŸ‘¨â€ğŸ’¼ **æ¶æ§‹å¸«/PM**: é–±è®€ [æ ¸å¿ƒæ¶æ§‹ç¸½è¦½](#æ ¸å¿ƒæ¶æ§‹ç¸½è¦½)
> - ğŸ **Python é–‹ç™¼è€…**: æŸ¥çœ‹ [é–‹ç™¼æŒ‡å—](docs/README_DEVELOPMENT.md)
> - ğŸ¤– **AI å·¥ç¨‹å¸«**: æŸ¥çœ‹ [AI å¼•æ“æŒ‡å—](docs/README_AI_ENGINE.md)
> - âš¡ **æ€§èƒ½å·¥ç¨‹å¸«**: æŸ¥çœ‹ [åŸ·è¡Œå¼•æ“æŒ‡å—](docs/README_EXECUTION.md)
> - ğŸ§  **ML å·¥ç¨‹å¸«**: æŸ¥çœ‹ [å­¸ç¿’ç³»çµ±æŒ‡å—](docs/README_LEARNING.md)

---

## ğŸ“Š **æ¨¡çµ„è¦æ¨¡ä¸€è¦½**

### **ğŸ—ï¸ æ•´é«”çµ±è¨ˆ**
- **ç¸½æª”æ¡ˆæ•¸**: **{stats['total_files']}** å€‹ Python æ¨¡çµ„
- **ä»£ç¢¼è¡Œæ•¸**: **{stats['total_code_lines']:,}** è¡Œ
- **é¡åˆ¥æ•¸é‡**: **{stats['total_classes']}** å€‹é¡åˆ¥
- **å‡½æ•¸æ•¸é‡**: **{stats['total_functions']}** å€‹å‡½æ•¸ (å« {stats['total_async_functions']} å€‹ç•°æ­¥å‡½æ•¸)
- **å¹³å‡è¤‡é›œåº¦**: **{stats['avg_complexity']}** / 100
- **è¤‡é›œåº¦ç­‰ç´š**: â­â­â­â­â­ (æœ€é«˜ç´šåˆ¥)

### **ğŸ“ˆ åŠŸèƒ½åˆ†ä½ˆ**
```
ğŸ¤– AI å¼•æ“        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ {stats['ai_components']} çµ„ä»¶
âš¡ åŸ·è¡Œå¼•æ“        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ {stats['execution_components']} çµ„ä»¶
ğŸ§  å­¸ç¿’ç³»çµ±        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ {stats['learning_components']} çµ„ä»¶
ğŸ“Š åˆ†ææ±ºç­–        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ {stats['analysis_components']} çµ„ä»¶
ğŸ’¾ å­˜å„²ç‹€æ…‹        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ {stats['storage_components']} çµ„ä»¶
```

---

## ğŸ—ï¸ **æ ¸å¿ƒæ¶æ§‹ç¸½è¦½**

### **äº”å±¤æ ¸å¿ƒæ¶æ§‹**

```mermaid
flowchart TD
    CORE["ğŸ¯ AIVA Core Engine<br/>{stats['total_files']} çµ„ä»¶"]
    
    AI["ğŸ¤– AI å¼•æ“å±¤<br/>{stats['ai_components']} çµ„ä»¶<br/>æ™ºèƒ½æ±ºç­–èˆ‡æ§åˆ¶"]
    EXEC["âš¡ åŸ·è¡Œå¼•æ“å±¤<br/>{stats['execution_components']} çµ„ä»¶<br/>ä»»å‹™èª¿åº¦èˆ‡åŸ·è¡Œ"]
    LEARN["ğŸ§  å­¸ç¿’ç³»çµ±å±¤<br/>{stats['learning_components']} çµ„ä»¶<br/>æŒçºŒå­¸ç¿’èˆ‡å„ªåŒ–"]
    ANALYSIS["ğŸ“Š åˆ†ææ±ºç­–å±¤<br/>{stats['analysis_components']} çµ„ä»¶<br/>é¢¨éšªè©•ä¼°èˆ‡ç­–ç•¥"]
    STORAGE["ğŸ’¾ å­˜å„²ç®¡ç†å±¤<br/>{stats['storage_components']} çµ„ä»¶<br/>ç‹€æ…‹èˆ‡æ•¸æ“šç®¡ç†"]
    
    CORE --> AI
    CORE --> EXEC
    CORE --> LEARN
    CORE --> ANALYSIS
    CORE --> STORAGE
    
    AI <--> EXEC
    EXEC <--> LEARN
    LEARN <--> ANALYSIS
    ANALYSIS <--> STORAGE
    
    classDef aiStyle fill:#9333ea,color:#fff
    classDef execStyle fill:#dc2626,color:#fff
    classDef learnStyle fill:#2563eb,color:#fff
    classDef analysisStyle fill:#059669,color:#fff
    classDef storageStyle fill:#ea580c,color:#fff
    
    class AI aiStyle
    class EXEC execStyle
    class LEARN learnStyle
    class ANALYSIS analysisStyle
    class STORAGE storageStyle
```

### **ğŸ¯ å„å±¤æ ¸å¿ƒè·è²¬**

| åŠŸèƒ½å±¤ | ä¸»è¦è·è²¬ | é—œéµæ¨¡çµ„ | ä»£ç¢¼è¦æ¨¡ |
|--------|----------|----------|----------|
| ğŸ¤– **AI å¼•æ“** | AIæ¨¡å‹ç®¡ç†ã€ç¥ç¶“ç¶²çµ¡ã€åå¹»è¦º | bio_neuron_core, ai_controller | 2,000+ è¡Œ |
| âš¡ **åŸ·è¡Œå¼•æ“** | ä»»å‹™èª¿åº¦ã€è¨ˆåŠƒåŸ·è¡Œã€ç‹€æ…‹ç›£æ§ | plan_executor, task_dispatcher | 1,500+ è¡Œ |
| ğŸ§  **å­¸ç¿’ç³»çµ±** | æ¨¡å‹è¨“ç·´ã€ç¶“é©—ç®¡ç†ã€å ´æ™¯è¨“ç·´ | model_trainer, scenario_manager | 1,700+ è¡Œ |
| ğŸ“Š **åˆ†ææ±ºç­–** | é¢¨éšªè©•ä¼°ã€ç­–ç•¥ç”Ÿæˆã€æ±ºç­–æ”¯æŒ | enhanced_decision_agent, strategy_generator | 800+ è¡Œ |
| ğŸ’¾ **å­˜å„²ç®¡ç†** | ç‹€æ…‹ç®¡ç†ã€æ•¸æ“šæŒä¹…åŒ–ã€æœƒè©±ç®¡ç† | session_state_manager, storage_manager | 600+ è¡Œ |

---

## ğŸ“š **æ–‡ä»¶å°èˆªåœ°åœ–**

### **ğŸ“ æŒ‰åŠŸèƒ½æŸ¥çœ‹**
- ğŸ¤– [**AI å¼•æ“è©³è§£**](docs/README_AI_ENGINE.md) - ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ã€AIæ§åˆ¶å™¨ã€åå¹»è¦ºæ¨¡çµ„
- âš¡ [**åŸ·è¡Œå¼•æ“è©³è§£**](docs/README_EXECUTION.md) - ä»»å‹™èª¿åº¦ã€è¨ˆåŠƒåŸ·è¡Œã€ç›£æ§è¿½è¹¤
- ğŸ§  [**å­¸ç¿’ç³»çµ±è©³è§£**](docs/README_LEARNING.md) - æ¨¡å‹è¨“ç·´ã€ç¶“é©—ç®¡ç†ã€å ´æ™¯è¨“ç·´
- ğŸ“Š [**åˆ†ææ±ºç­–è©³è§£**](docs/README_ANALYSIS.md) - é¢¨éšªè©•ä¼°ã€ç­–ç•¥ç”Ÿæˆã€æ±ºç­–ä»£ç†
- ğŸ’¾ [**å­˜å„²ç®¡ç†è©³è§£**](docs/README_STORAGE.md) - ç‹€æ…‹ç®¡ç†ã€æ•¸æ“šæŒä¹…åŒ–ã€æœƒè©±æ§åˆ¶

### **ğŸ’» é–‹ç™¼æ–‡æª”**
- ğŸ [**é–‹ç™¼æŒ‡å—**](docs/README_DEVELOPMENT.md) - Python é–‹ç™¼è¦ç¯„ã€æœ€ä½³å¯¦è¸
- ğŸ”§ [**API åƒè€ƒ**](docs/README_API.md) - æ ¸å¿ƒ API æ–‡æª”èˆ‡ä½¿ç”¨ç¯„ä¾‹
- ğŸ§ª [**æ¸¬è©¦æŒ‡å—**](docs/README_TESTING.md) - å–®å…ƒæ¸¬è©¦ã€æ•´åˆæ¸¬è©¦ç­–ç•¥

---

## ğŸš€ **å¿«é€Ÿé–‹å§‹æŒ‡å—**

### **ğŸ” æˆ‘éœ€è¦ä»€éº¼ï¼Ÿ**

**å ´æ™¯ 1: äº†è§£ AI å¼•æ“** ğŸ¤–  
```
â†’ é–±è®€æœ¬æ–‡ä»¶çš„æ ¸å¿ƒæ¶æ§‹ç¸½è¦½
â†’ æŸ¥çœ‹ docs/README_AI_ENGINE.md
â†’ æª¢è¦– bio_neuron_core.py å’Œ ai_controller.py
```

**å ´æ™¯ 2: é–‹ç™¼ä»»å‹™åŸ·è¡ŒåŠŸèƒ½** âš¡  
```
â†’ é–±è®€ docs/README_EXECUTION.md
â†’ æŸ¥çœ‹ plan_executor.py å’Œ task_dispatcher.py
â†’ è·Ÿéš¨åŸ·è¡Œå¼•æ“é–‹ç™¼æ¨¡å¼
```

**å ´æ™¯ 3: å¯¦ç¾å­¸ç¿’åŠŸèƒ½** ğŸ§   
```  
â†’ é–±è®€ docs/README_LEARNING.md
â†’ æŸ¥çœ‹ model_trainer.py å’Œ scenario_manager.py
â†’ è·Ÿéš¨å­¸ç¿’ç³»çµ±é–‹ç™¼æŒ‡å—
```

**å ´æ™¯ 4: ç³»çµ±æ•´åˆèˆ‡éƒ¨ç½²** ğŸ”§  
```
â†’ é–±è®€ docs/README_DEVELOPMENT.md  
â†’ æŸ¥çœ‹æ•´åˆæ¸¬è©¦ç¯„ä¾‹
â†’ åƒè€ƒéƒ¨ç½²å’Œç›£æ§æœ€ä½³å¯¦è¸
```

### **ğŸ› ï¸ ç’°å¢ƒè¨­å®š**
```bash
# 1. é€²å…¥ Core æ¨¡çµ„
cd services/core

# 2. å®‰è£ä¾è³´
pip install -r requirements.txt

# 3. é…ç½®ç’°å¢ƒè®Šé‡
cp .env.example .env

# 4. åŸ·è¡Œæ¸¬è©¦
python -m pytest tests/ -v

# 5. å•Ÿå‹•é–‹ç™¼æœå‹™å™¨
python -m aiva_core.app
```

---

## âš ï¸ **é‡è¦æ³¨æ„äº‹é …**

### **ğŸ”´ é—œéµæ¶æ§‹åŸå‰‡**
1. **AI å„ªå…ˆ**: Core æ¨¡çµ„ä»¥ AI å¼•æ“ç‚ºæ ¸å¿ƒ
2. **ç•°æ­¥å„ªå…ˆ**: å¤§é‡ä½¿ç”¨ç•°æ­¥ç·¨ç¨‹æå‡æ€§èƒ½
3. **ç‹€æ…‹ç®¡ç†**: åš´æ ¼çš„ç‹€æ…‹ç®¡ç†å’ŒæŒä¹…åŒ–ç­–ç•¥
4. **æ¨¡çµ„åŒ–è¨­è¨ˆ**: æ¸…æ™°çš„å±¤æ¬¡çµæ§‹å’Œä¾è³´é—œä¿‚

### **ğŸš¨ é–‹ç™¼ç´„æŸ**
- âœ… **å¿…é ˆ**: éµå¾ª Python é¡å‹æ¨™è¨»å’Œæ–‡æª”å­—ç¬¦ä¸²è¦ç¯„
- âœ… **å¿…é ˆ**: å¯¦ç¾å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„
- âš ï¸ **é¿å…**: è·¨å±¤ç›´æ¥èª¿ç”¨ï¼Œæ‡‰é€šéå®šç¾©çš„ä»‹é¢
- âš ï¸ **é¿å…**: é˜»å¡æ“ä½œï¼Œå„ªå…ˆä½¿ç”¨ç•°æ­¥æ–¹æ³•

---

## ğŸ“ˆ **æŠ€è¡“å‚µå‹™èˆ‡å„ªåŒ–å»ºè­°**

### **ğŸš¨ é«˜è¤‡é›œåº¦æ¨¡çµ„ (éœ€è¦é‡æ§‹)**
åŸºæ–¼ä»£ç¢¼åˆ†æï¼Œä»¥ä¸‹æ¨¡çµ„è¤‡é›œåº¦è¼ƒé«˜ï¼Œå»ºè­°å„ªå…ˆé‡æ§‹ï¼š

1. **bio_neuron_core.py** (è¤‡é›œåº¦: 97)
   - å»ºè­°æ‹†åˆ†ç‚ºå¤šå€‹å°ˆé–€æ¨¡çµ„
   - æœ€é•·å‡½æ•¸ 118 è¡Œï¼Œéœ€è¦åˆ†è§£

2. **ai_controller.py** (è¤‡é›œåº¦: 77)
   - çµ±ä¸€æ§åˆ¶å™¨é‚è¼¯éæ–¼é¾å¤§
   - å»ºè­°å¼•å…¥æ›´å¤šå§”è¨—æ¨¡å¼

3. **enhanced_decision_agent.py** (è¤‡é›œåº¦: 75)
   - æ±ºç­–é‚è¼¯è¤‡é›œåº¦é«˜
   - å»ºè­°å¼•å…¥ç­–ç•¥æ¨¡å¼ç°¡åŒ–

### **âš¡ æ€§èƒ½å„ªåŒ–æ©Ÿæœƒ**
- å¢åŠ ç•°æ­¥å‡½æ•¸ä½¿ç”¨ç‡ï¼ˆç•¶å‰ {stats['total_async_functions']} / {stats['total_functions']}ï¼‰
- å¯¦ç¾æ›´å®Œå–„çš„ç·©å­˜ç­–ç•¥
- å„ªåŒ–æ•¸æ“šåº«æŸ¥è©¢å’Œæ‰¹é‡æ“ä½œ

---

## ğŸ”— **æ ¸å¿ƒä¾è³´é—œä¿‚**

### **ğŸ“¦ ä¸»è¦å¤–éƒ¨ä¾è³´**
{self._format_dependencies()}

---

## ğŸ“ **æ”¯æ´èˆ‡è¯ç¹«**

### **ğŸ‘¥ åœ˜éšŠåˆ†å·¥**
- ğŸ¤– **AI å¼•æ“åœ˜éšŠ**: ç¥ç¶“ç¶²çµ¡ã€æ¨¡å‹ç®¡ç†
- âš¡ **åŸ·è¡Œå¼•æ“åœ˜éšŠ**: ä»»å‹™èª¿åº¦ã€æ€§èƒ½å„ªåŒ–
- ğŸ§  **å­¸ç¿’ç³»çµ±åœ˜éšŠ**: è¨“ç·´ç®¡é“ã€ç¶“é©—ç®¡ç†
- ğŸ“Š **åˆ†æåœ˜éšŠ**: æ±ºç­–ç³»çµ±ã€é¢¨éšªè©•ä¼°

### **ğŸ“Š ç›¸é—œå ±å‘Š**
- ğŸ“ˆ [æ ¸å¿ƒæ¨¡çµ„ä»£ç¢¼åˆ†æ](_out/core_module_analysis_detailed.json)
- ğŸ” [æ¶æ§‹å„ªåŒ–å»ºè­°](reports/ANALYSIS_REPORTS/core_module_comprehensive_analysis.md)

---

**ğŸ“ æ–‡ä»¶ç‰ˆæœ¬**: v1.0 - Core Module Multi-Layer Documentation  
**ğŸ”„ æœ€å¾Œæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d')}  
**ğŸ“ˆ è¤‡é›œåº¦ç­‰ç´š**: â­â­â­â­â­ (æœ€é«˜) - æ ¸å¿ƒå¼•æ“ç³»çµ±  
**ğŸ‘¥ ç¶­è­·åœ˜éšŠ**: AIVA Core Architecture Team

*é€™æ˜¯ AIVA Core æ¨¡çµ„çš„ä¸»è¦å°èˆªæ–‡ä»¶ã€‚æ ¹æ“šæ‚¨çš„è§’è‰²å’Œéœ€æ±‚ï¼Œé¸æ“‡é©åˆçš„å°ˆæ¥­æ–‡ä»¶æ·±å…¥äº†è§£ã€‚*
"""
        return template
    
    def _format_dependencies(self) -> str:
        """æ ¼å¼åŒ–ä¾è³´åˆ—è¡¨"""
        deps = self.stats.get('top_dependencies', [])
        lines = []
        for module, count in deps:
            lines.append(f"- **{module}**: {count} æ¬¡å¼•ç”¨")
        return '\n'.join(lines)
    
    def generate_ai_engine_readme(self) -> str:
        """ç”Ÿæˆ AI å¼•æ“å°ˆé–€ README"""
        
        ai_files = [f for f in self.analysis_data if 'ai_' in f['file'] or 'bio_neuron' in f['file'] or 'nlg' in f['file']]
        
        template = f"""# AIVA Core - AI å¼•æ“æ¶æ§‹è©³è§£ ğŸ¤–

> **å®šä½**: AIVA å¹³å°çš„ AI æ ¸å¿ƒå¼•æ“  
> **è¦æ¨¡**: {len(ai_files)} å€‹ AI çµ„ä»¶  
> **ä¸»åŠ›æŠ€è¡“**: ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ã€åå¹»è¦ºç³»çµ±ã€è‡ªç„¶èªè¨€ç”Ÿæˆ

---

## ğŸ¯ **AI å¼•æ“ç¸½è¦½**

### **ğŸ”¥ AI å¼•æ“æ¶æ§‹**

```
ğŸ¤– AI å¼•æ“å±¤
â”œâ”€â”€ ğŸ§  ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ (bio_neuron_core.py)
â”‚   â”œâ”€â”€ ç”Ÿç‰©è„ˆè¡å±¤ (BiologicalSpikingLayer)
â”‚   â”œâ”€â”€ åå¹»è¦ºæ¨¡çµ„ (AntiHallucinationModule)
â”‚   â””â”€â”€ å¯æ“´å±•ç”Ÿç‰©ç¶²çµ¡ (ScalableBioNet)
â”œâ”€â”€ ğŸ›ï¸ AI æ§åˆ¶å™¨ (ai_controller.py)
â”‚   â”œâ”€â”€ çµ±ä¸€ AI æ§åˆ¶å™¨ (UnifiedAIController)
â”‚   â””â”€â”€ å¤šèªè¨€å”èª¿æ•´åˆ
â”œâ”€â”€ ğŸ§© AI æŒ‡æ®å®˜ (ai_commander.py)
â”‚   â”œâ”€â”€ AI ä»»å‹™é¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ AI çµ„ä»¶å”èª¿
â”‚   â””â”€â”€ ç‹€æ…‹ç®¡ç†èˆ‡ä¿å­˜
â”œâ”€â”€ ğŸ§  AI æ¨¡å‹ç®¡ç†å™¨ (ai_model_manager.py)
â”‚   â””â”€â”€ æ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†
â”œâ”€â”€ ğŸ’¬ è‡ªç„¶èªè¨€ç”Ÿæˆ (nlg_system.py)
â”‚   â””â”€â”€ æ™ºèƒ½æ–‡æœ¬ç”Ÿæˆç³»çµ±
â””â”€â”€ ğŸ”Œ AI æ‘˜è¦æ’ä»¶ (ai_summary_plugin.py)
    â””â”€â”€ æ™ºèƒ½æ‘˜è¦åŠŸèƒ½
```

### **âš¡ æ ¸å¿ƒèƒ½åŠ›**

| AI æ¨¡çµ„ | ä¸»è¦åŠŸèƒ½ | ä»£ç¢¼è¦æ¨¡ | è¤‡é›œåº¦ |
|---------|----------|----------|--------|
| **bio_neuron_core** | ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ã€åå¹»è¦º | 648 è¡Œ | 97 |
| **ai_controller** | çµ±ä¸€ AI æ§åˆ¶ | 621 è¡Œ | 77 |
| **bio_neuron_master** | ä¸»æ§åˆ¶å™¨ | 488 è¡Œ | 45 |
| **ai_model_manager** | æ¨¡å‹ç®¡ç† | 370 è¡Œ | 38 |
| **nlg_system** | è‡ªç„¶èªè¨€ç”Ÿæˆ | 365 è¡Œ | 43 |

---

## ğŸ§  **ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡æ ¸å¿ƒ**

### **æ ¸å¿ƒæ¶æ§‹**

```python
from typing import Dict, List, Optional
import torch
import torch.nn as nn

class BiologicalSpikingLayer(nn.Module):
    \"\"\"ç”Ÿç‰©è„ˆè¡ç¥ç¶“ç¶²çµ¡å±¤\"\"\"
    
    def __init__(self, input_size: int, output_size: int):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.weights = nn.Parameter(torch.randn(input_size, output_size))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        \"\"\"å‰å‘å‚³æ’­ - ç”Ÿç‰©è„ˆè¡æ©Ÿåˆ¶\"\"\"
        # å¯¦ç¾ç”Ÿç‰©è„ˆè¡é‚è¼¯
        return self._biological_spike(x)
    
    def _biological_spike(self, x: torch.Tensor) -> torch.Tensor:
        \"\"\"ç”Ÿç‰©è„ˆè¡è¨ˆç®—\"\"\"
        # æ¨¡æ“¬ç¥ç¶“å…ƒè„ˆè¡è¡Œç‚º
        pass

class AntiHallucinationModule(nn.Module):
    \"\"\"åå¹»è¦ºæ¨¡çµ„ - ç¢ºä¿ AI è¼¸å‡ºå¯é æ€§\"\"\"
    
    def __init__(self, confidence_threshold: float = 0.7):
        super().__init__()
        self.confidence_threshold = confidence_threshold
        
    def validate_output(self, output: Dict, context: Dict) -> bool:
        \"\"\"é©—è­‰è¼¸å‡ºæ˜¯å¦å¯é \"\"\"
        confidence = self._calculate_confidence(output, context)
        return confidence >= self.confidence_threshold
    
    def _calculate_confidence(self, output: Dict, context: Dict) -> float:
        \"\"\"è¨ˆç®—è¼¸å‡ºä¿¡å¿ƒåº¦\"\"\"
        # å¤šç¶­åº¦ä¿¡å¿ƒåº¦è©•ä¼°
        pass

class ScalableBioNet(nn.Module):
    \"\"\"å¯æ“´å±•ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡\"\"\"
    
    def __init__(self, config: Dict):
        super().__init__()
        self.layers = nn.ModuleList([
            BiologicalSpikingLayer(config['input_size'], config['hidden_size']),
            BiologicalSpikingLayer(config['hidden_size'], config['output_size'])
        ])
        self.anti_hallucination = AntiHallucinationModule()
    
    async def forward_with_validation(self, x: torch.Tensor, context: Dict) -> Dict:
        \"\"\"å¸¶é©—è­‰çš„å‰å‘å‚³æ’­\"\"\"
        output = self.forward(x)
        
        # åå¹»è¦ºé©—è­‰
        is_valid = self.anti_hallucination.validate_output(output, context)
        
        return {{
            'output': output,
            'valid': is_valid,
            'confidence': self.anti_hallucination._calculate_confidence(output, context)
        }}
```

---

## ğŸ›ï¸ **çµ±ä¸€ AI æ§åˆ¶å™¨**

### **æ§åˆ¶å™¨æ¶æ§‹**

```python
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class AIControllerConfig:
    \"\"\"AI æ§åˆ¶å™¨é…ç½®\"\"\"
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 2000
    use_anti_hallucination: bool = True
    enable_learning: bool = True

class UnifiedAIController:
    \"\"\"çµ±ä¸€ AI æ§åˆ¶å™¨ - å”èª¿æ‰€æœ‰ AI çµ„ä»¶\"\"\"
    
    def __init__(self, config: AIControllerConfig):
        self.config = config
        self.bio_net = ScalableBioNet(self._get_bio_net_config())
        self.nlg_system = AIVANaturalLanguageGenerator()
        self.model_manager = AIModelManager()
        
    async def process_request(self, request: Dict) -> Dict:
        \"\"\"è™•ç† AI è«‹æ±‚ - ä¸»è¦å…¥å£é»\"\"\"
        
        # 1. é è™•ç†
        processed_input = await self._preprocess_request(request)
        
        # 2. AI æ¨ç†
        raw_output = await self._run_inference(processed_input)
        
        # 3. åå¹»è¦ºé©—è­‰
        if self.config.use_anti_hallucination:
            validated_output = await self._validate_output(raw_output, request)
        else:
            validated_output = raw_output
        
        # 4. è‡ªç„¶èªè¨€ç”Ÿæˆ
        final_response = await self._generate_response(validated_output)
        
        # 5. å­¸ç¿’èˆ‡æ›´æ–°
        if self.config.enable_learning:
            await self._update_learning(request, final_response)
        
        return final_response
    
    async def _run_inference(self, input_data: Dict) -> Dict:
        \"\"\"åŸ·è¡Œ AI æ¨ç†\"\"\"
        # ä½¿ç”¨ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡é€²è¡Œæ¨ç†
        tensor_input = self._convert_to_tensor(input_data)
        result = await self.bio_net.forward_with_validation(
            tensor_input, 
            context=input_data.get('context', {{}})
        )
        return result
    
    async def _validate_output(self, output: Dict, original_request: Dict) -> Dict:
        \"\"\"é©—è­‰è¼¸å‡º - é˜²æ­¢å¹»è¦º\"\"\"
        if not output.get('valid', False):
            # è¼¸å‡ºä¸å¯é ï¼Œé‡æ–°ç”Ÿæˆæˆ–ä½¿ç”¨å‚™ç”¨ç­–ç•¥
            return await self._fallback_generation(original_request)
        return output
```

---

## ğŸ’¬ **è‡ªç„¶èªè¨€ç”Ÿæˆç³»çµ±**

### **NLG æ¶æ§‹**

```python
class AIVANaturalLanguageGenerator:
    \"\"\"AIVA è‡ªç„¶èªè¨€ç”Ÿæˆå™¨\"\"\"
    
    def __init__(self):
        self.templates = self._load_templates()
        self.context_manager = ContextManager()
        
    async def generate(self, data: Dict, style: str = "professional") -> str:
        \"\"\"ç”Ÿæˆè‡ªç„¶èªè¨€è¼¸å‡º\"\"\"
        
        # 1. é¸æ“‡æ¨¡æ¿
        template = self._select_template(data['type'], style)
        
        # 2. å¡«å……ä¸Šä¸‹æ–‡
        context = await self.context_manager.build_context(data)
        
        # 3. ç”Ÿæˆæ–‡æœ¬
        generated_text = self._fill_template(template, context)
        
        # 4. å¾Œè™•ç†
        polished_text = self._polish_text(generated_text)
        
        return polished_text
    
    def _select_template(self, data_type: str, style: str) -> str:
        \"\"\"é¸æ“‡åˆé©çš„æ¨¡æ¿\"\"\"
        key = f"{{data_type}}_{{style}}"
        return self.templates.get(key, self.templates['default'])
    
    def _polish_text(self, text: str) -> str:
        \"\"\"æ–‡æœ¬æ½¤è‰²\"\"\"
        # èªæ³•æª¢æŸ¥ã€æ ¼å¼åŒ–ã€å„ªåŒ–å¯è®€æ€§
        pass
```

---

## ğŸ§ª **æ¸¬è©¦èˆ‡é©—è­‰**

### **AI å¼•æ“æ¸¬è©¦**

```python
import pytest
import asyncio

class TestBioNeuronCore:
    
    async def test_biological_spike_layer(self):
        \"\"\"æ¸¬è©¦ç”Ÿç‰©è„ˆè¡å±¤\"\"\"
        layer = BiologicalSpikingLayer(input_size=10, output_size=5)
        input_tensor = torch.randn(1, 10)
        
        output = layer(input_tensor)
        
        assert output.shape == (1, 5)
        assert torch.all(torch.isfinite(output))
    
    async def test_anti_hallucination(self):
        \"\"\"æ¸¬è©¦åå¹»è¦ºæ¨¡çµ„\"\"\"
        module = AntiHallucinationModule(confidence_threshold=0.7)
        
        # é«˜ä¿¡å¿ƒåº¦è¼¸å‡º
        valid_output = {{'data': 'test', 'confidence': 0.85}}
        assert module.validate_output(valid_output, {{}}) == True
        
        # ä½ä¿¡å¿ƒåº¦è¼¸å‡º
        invalid_output = {{'data': 'test', 'confidence': 0.5}}
        assert module.validate_output(invalid_output, {{}}) == False

@pytest.mark.asyncio
class TestUnifiedAIController:
    
    async def test_process_request(self):
        \"\"\"æ¸¬è©¦ AI è«‹æ±‚è™•ç†\"\"\"
        config = AIControllerConfig(model_name="bio-gpt")
        controller = UnifiedAIController(config)
        
        request = {{
            'type': 'scan_analysis',
            'data': {{'target': 'example.com'}},
            'context': {{'user': 'test_user'}}
        }}
        
        response = await controller.process_request(request)
        
        assert 'output' in response
        assert response.get('valid', False) == True
```

---

**ğŸ“ ç‰ˆæœ¬**: v1.0 - AI Engine Deep Dive  
**ğŸ”„ æœ€å¾Œæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d')}  
**ğŸ¤– AI æŠ€è¡“æ£§**: PyTorch + ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ + åå¹»è¦ºç³»çµ±  
**ğŸ‘¥ ç¶­è­·åœ˜éšŠ**: AIVA AI Engine Team

*æœ¬æ–‡ä»¶è©³ç´°ä»‹ç´¹ AIVA Core æ¨¡çµ„çš„ AI å¼•æ“æ¶æ§‹ï¼ŒåŒ…å«ç”Ÿç‰©ç¥ç¶“ç¶²çµ¡ã€AI æ§åˆ¶å™¨å’Œè‡ªç„¶èªè¨€ç”Ÿæˆç³»çµ±ã€‚*
"""
        return template
    
    def run_generation(self):
        """åŸ·è¡Œ README ç”Ÿæˆ"""
        print("ğŸš€ é–‹å§‹ç”Ÿæˆ Core æ¨¡çµ„å¤šå±¤æ¬¡ README æ¶æ§‹...")
        
        readmes = {
            "README.md": self.generate_main_readme(),
            "docs/README_AI_ENGINE.md": self.generate_ai_engine_readme(),
            # TODO: å…¶ä»– README æ–‡ä»¶å¯ä»¥å¾ŒçºŒæ·»åŠ 
        }
        
        for file_path, content in readmes.items():
            full_path = self.base_dir / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… ç”Ÿæˆ README: {full_path}")
        
        print(f"ğŸ‰ å®Œæˆï¼ç”Ÿæˆäº† {len(readmes)} å€‹ README æ–‡ä»¶")
        print(f"\nğŸ“ ç”Ÿæˆä½ç½®: {self.base_dir.absolute()}")

if __name__ == "__main__":
    generator = CoreMultiLayerReadmeGenerator()
    generator.run_generation()
