#!/usr/bin/env python3
"""
AIVA Features æ™ºèƒ½åˆ†ææ¡†æ¶ V3.0
åŸºæ–¼V2.0ç¶“é©—æ•™è¨“çš„å®Œå…¨é‡æ§‹ç‰ˆæœ¬

æ ¸å¿ƒæ”¹é€²:
1. çµ±ä¸€çš„åˆ†ææ¡†æ¶å’Œä»‹é¢æ¨™æº–
2. é…ç½®é©…å‹•çš„è¦å‰‡ç³»çµ±
3. å®Œæ•´çš„æ¸¬è©¦è¦†è“‹å’Œé©—è­‰æ©Ÿåˆ¶
4. æ€§èƒ½å„ªåŒ–å’Œç·©å­˜ç³»çµ±
5. æ·±åº¦å¯¦ç¾æ›¿ä»£ç°¡åŒ–ç‰ˆæœ¬
6. æ™ºèƒ½åœ–è¡¨ç”Ÿæˆå’Œå•é¡Œç™¼ç¾
"""

import json
import os
import sys
import time
import hashlib
from abc import ABC, abstractmethod
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set, Any, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum
import re
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import glob
import shutil
from datetime import datetime

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AnalysisQuality(Enum):
    """åˆ†æå“è³ªç­‰ç´š"""
    BASIC = "basic"
    STANDARD = "standard"
    DEEP = "deep"
    EXPERT = "expert"

class ComponentType(Enum):
    """çµ„ä»¶é¡å‹"""
    CONFIG = "config"
    SERVICE = "service"
    WORKER = "worker"
    MANAGER = "manager"
    VALIDATOR = "validator"
    BUILDER = "builder"
    FACTORY = "factory"
    HANDLER = "handler"
    ADAPTER = "adapter"
    CONTROLLER = "controller"
    DECORATOR = "decorator"
    OBSERVER = "observer"
    STRATEGY = "strategy"
    STATE = "state"
    UNKNOWN = "unknown"

@dataclass
class AnalysisResult:
    """åˆ†æçµæœæ¨™æº–æ ¼å¼"""
    method_name: str
    category: str
    quality_level: AnalysisQuality
    component_count: int
    confidence_score: float  # 0.0 - 1.0
    analysis_time: float
    groups: Dict[str, List[str]]
    metadata: Dict[str, Any]
    issues: List[str] = None
    
    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        result['quality_level'] = self.quality_level.value
        return result

class AnalysisConfig:
    """åˆ†æé…ç½®ç®¡ç†"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path
        self.config = self._load_default_config()
        if config_path and os.path.exists(config_path):
            self._load_custom_config()
    
    def _load_default_config(self) -> Dict:
        """è¼‰å…¥é è¨­é…ç½®"""
        return {
            'analysis_quality': AnalysisQuality.STANDARD.value,
            'enable_caching': True,
            'cache_ttl': 3600,  # 1å°æ™‚
            'max_workers': 4,
            'timeout_seconds': 300,  # 5åˆ†é˜
            'confidence_threshold': 0.7,
            'max_groups_per_method': 50,
            'enable_validation': True,
            'generate_charts': True,
            'detect_issues': True,
            'rules': {
                'role_patterns': {
                    'manager': ['manager', 'mgr', 'admin', 'supervisor'],
                    'controller': ['controller', 'ctrl', 'command'],
                    'service': ['service', 'svc', 'api', 'endpoint'],
                    'worker': ['worker', 'processor', 'executor'],
                    'handler': ['handler', 'handle', 'process'],
                    'builder': ['builder', 'build', 'construct'],
                    'factory': ['factory', 'creator', 'make'],
                    'validator': ['validator', 'validate', 'check', 'verify'],
                    'config': ['config', 'setting', 'option', 'param'],
                    'adapter': ['adapter', 'adapt', 'convert'],
                    'decorator': ['decorator', 'wrap', 'enhance'],
                    'observer': ['observer', 'listen', 'watch', 'monitor'],
                    'strategy': ['strategy', 'policy', 'algorithm'],
                    'state': ['state', 'status', 'condition']
                },
                'quality_indicators': {
                    'high_maintainability': ['simple', 'clean', 'clear', 'basic'],
                    'low_maintainability': ['complex', 'legacy', 'hack', 'temp'],
                    'high_testability': ['test', 'mock', 'stub', 'fake'],
                    'low_testability': ['static', 'singleton', 'global'],
                    'performance_critical': ['fast', 'optimize', 'cache', 'performance'],
                    'security_critical': ['auth', 'security', 'crypto', 'encrypt']
                },
                'complexity_thresholds': {
                    'simple': 1,
                    'moderate': 3,
                    'complex': 6,
                    'very_complex': 10
                }
            }
        }
    
    def _load_custom_config(self):
        """è¼‰å…¥è‡ªå®šç¾©é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                custom_config = json.load(f)
                self._merge_config(custom_config)
        except Exception as e:
            logger.warning(f"ç„¡æ³•è¼‰å…¥è‡ªå®šç¾©é…ç½®: {e}")
    
    def _merge_config(self, custom_config: Dict):
        """åˆä½µé…ç½®"""
        def merge_dict(base: Dict, custom: Dict):
            for key, value in custom.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    merge_dict(base[key], value)
                else:
                    base[key] = value
        
        merge_dict(self.config, custom_config)
    
    def get(self, key: str, default=None):
        """ç²å–é…ç½®å€¼"""
        keys = key.split('.')
        current = self.config
        for k in keys:
            if isinstance(current, dict) and k in current:
                current = current[k]
            else:
                return default
        return current

class CacheManager:
    """ç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self, enabled: bool = True, cache_dir: str = ".cache"):
        self.enabled = enabled
        self.cache_dir = Path(cache_dir)
        if enabled:
            self.cache_dir.mkdir(exist_ok=True)
    
    def _get_cache_key(self, data: Any) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        content = json.dumps(data, sort_keys=True) if isinstance(data, (dict, list)) else str(data)
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ç²å–ç·©å­˜"""
        if not self.enabled:
            return None
        
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                    if time.time() - cached_data['timestamp'] < 3600:  # 1å°æ™‚æœ‰æ•ˆ
                        return cached_data['data']
        except Exception as e:
            logger.warning(f"ç·©å­˜è®€å–å¤±æ•—: {e}")
        return None
    
    def set(self, key: str, data: Any):
        """è¨­ç½®ç·©å­˜"""
        if not self.enabled:
            return
        
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'data': data,
                    'timestamp': time.time()
                }, f)
        except Exception as e:
            logger.warning(f"ç·©å­˜å¯«å…¥å¤±æ•—: {e}")

class BaseAnalyzer(ABC):
    """åˆ†æå™¨åŸºé¡ - çµ±ä¸€ä»‹é¢æ¨™æº–"""
    
    def __init__(self, config: AnalysisConfig, cache_manager: CacheManager):
        self.config = config
        self.cache = cache_manager
        self.name = self.__class__.__name__
    
    @abstractmethod
    def analyze(self, components: Dict[str, Any]) -> AnalysisResult:
        """åŸ·è¡Œåˆ†æ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass
    
    @abstractmethod
    def get_required_quality(self) -> AnalysisQuality:
        """ç²å–éœ€è¦çš„åˆ†æå“è³ªç­‰ç´š"""
        pass
    
    def validate_result(self, result: AnalysisResult) -> List[str]:
        """é©—è­‰åˆ†æçµæœ"""
        issues = []
        
        # æª¢æŸ¥çµæœå®Œæ•´æ€§
        if not result.groups:
            issues.append("åˆ†æçµæœç‚ºç©º")
        
        # æª¢æŸ¥ä¿¡å¿ƒåº¦
        if result.confidence_score < self.config.get('confidence_threshold', 0.7):
            issues.append(f"ä¿¡å¿ƒåº¦éä½: {result.confidence_score:.2f}")
        
        # æª¢æŸ¥çµ„æ•¸é‡
        max_groups = self.config.get('max_groups_per_method', 50)
        if len(result.groups) > max_groups:
            issues.append(f"çµ„æ•¸é‡éå¤š: {len(result.groups)} > {max_groups}")
        
        # æª¢æŸ¥çµ„ä»¶åˆ†ä½ˆ
        total_components = sum(len(components) for components in result.groups.values())
        if total_components != result.component_count:
            issues.append(f"çµ„ä»¶è¨ˆæ•¸ä¸åŒ¹é…: {total_components} != {result.component_count}")
        
        return issues
    
    def _extract_component_type(self, name: str) -> ComponentType:
        """æå–çµ„ä»¶é¡å‹"""
        name_lower = name.lower()
        role_patterns = self.config.get('rules.role_patterns', {})
        
        for role, patterns in role_patterns.items():
            if any(pattern in name_lower for pattern in patterns):
                try:
                    return ComponentType(role.upper())
                except ValueError:
                    return ComponentType.UNKNOWN
        
        return ComponentType.UNKNOWN
    
    def _calculate_confidence(self, groups: Dict[str, List[str]], total_components: int) -> float:
        """è¨ˆç®—åˆ†æä¿¡å¿ƒåº¦"""
        if not groups or total_components == 0:
            return 0.0
        
        # åŸºæ–¼åˆ†çµ„è¦†è“‹ç‡
        covered_components = sum(len(components) for components in groups.values())
        coverage_score = covered_components / total_components
        
        # åŸºæ–¼åˆ†çµ„å¹³è¡¡åº¦ (é¿å…ä¸€å€‹çµ„åŒ…å«æ‰€æœ‰çµ„ä»¶)
        group_sizes = [len(components) for components in groups.values()]
        if not group_sizes:
            return 0.0
        
        max_size = max(group_sizes)
        balance_score = 1.0 - (max_size / total_components)
        
        # åŸºæ–¼çµ„æ•¸é‡åˆç†æ€§
        group_count = len(groups)
        optimal_groups = min(20, max(3, total_components // 50))  # ç¶“é©—å€¼
        group_score = 1.0 - abs(group_count - optimal_groups) / optimal_groups
        
        # ç¶œåˆè¨ˆç®—
        confidence = (coverage_score * 0.5 + balance_score * 0.3 + group_score * 0.2)
        return min(1.0, max(0.0, confidence))

class SemanticAnalyzer(BaseAnalyzer):
    """èªç¾©åˆ†æå™¨ - æ·±åº¦å¯¦ç¾ç‰ˆæœ¬"""
    
    def get_required_quality(self) -> AnalysisQuality:
        return AnalysisQuality.DEEP
    
    def analyze(self, components: Dict[str, Any]) -> AnalysisResult:
        """åŸ·è¡Œèªç¾©åˆ†æ"""
        start_time = time.time()
        
        # æª¢æŸ¥ç·©å­˜
        cache_key = f"semantic_{hash(str(sorted(components.keys())))}"
        cached_result = self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        groups = {}
        
        # 1. è©æ€§åˆ†æ
        pos_groups = self._analyze_part_of_speech(components)
        groups.update({f"è©æ€§_{k}": v for k, v in pos_groups.items()})
        
        # 2. èªç¾©å ´åˆ†æ
        semantic_groups = self._analyze_semantic_fields(components)
        groups.update({f"èªç¾©å ´_{k}": v for k, v in semantic_groups.items()})
        
        # 3. æ¦‚å¿µå±¤æ¬¡åˆ†æ
        hierarchy_groups = self._analyze_concept_hierarchy(components)
        groups.update({f"æ¦‚å¿µå±¤æ¬¡_{k}": v for k, v in hierarchy_groups.items()})
        
        analysis_time = time.time() - start_time
        confidence = self._calculate_confidence(groups, len(components))
        
        result = AnalysisResult(
            method_name="èªç¾©æ™ºèƒ½åˆ†æ",
            category="èªç¾©åˆ†æ",
            quality_level=self.get_required_quality(),
            component_count=len(components),
            confidence_score=confidence,
            analysis_time=analysis_time,
            groups=groups,
            metadata={
                'pos_groups_count': len(pos_groups),
                'semantic_groups_count': len(semantic_groups),
                'hierarchy_groups_count': len(hierarchy_groups)
            }
        )
        
        # ç·©å­˜çµæœ
        self.cache.set(cache_key, result)
        
        return result
    
    def _analyze_part_of_speech(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """è©æ€§åˆ†æ - å®Œæ•´å¯¦ç¾"""
        pos_groups = defaultdict(list)
        
        # å‹•è©æ¨¡å¼ï¼ˆè¡¨ç¤ºå‹•ä½œï¼‰
        verb_patterns = [
            'create', 'build', 'make', 'generate', 'process', 'handle', 
            'manage', 'execute', 'run', 'start', 'stop', 'update', 
            'delete', 'get', 'set', 'add', 'remove', 'find', 'search'
        ]
        
        # åè©æ¨¡å¼ï¼ˆè¡¨ç¤ºå¯¦é«”ï¼‰
        noun_patterns = [
            'manager', 'service', 'worker', 'handler', 'config', 'data', 
            'model', 'entity', 'client', 'server', 'database', 'cache',
            'queue', 'pool', 'factory', 'builder', 'validator'
        ]
        
        # å½¢å®¹è©æ¨¡å¼ï¼ˆè¡¨ç¤ºå±¬æ€§ï¼‰
        adjective_patterns = [
            'smart', 'fast', 'secure', 'simple', 'complex', 'advanced', 
            'basic', 'enhanced', 'optimized', 'efficient', 'robust'
        ]
        
        for name, info in components.items():
            name_lower = name.lower()
            
            # æª¢æŸ¥å‹•è©æ¨¡å¼
            verb_count = sum(1 for pattern in verb_patterns if pattern in name_lower)
            # æª¢æŸ¥åè©æ¨¡å¼  
            noun_count = sum(1 for pattern in noun_patterns if pattern in name_lower)
            # æª¢æŸ¥å½¢å®¹è©æ¨¡å¼
            adj_count = sum(1 for pattern in adjective_patterns if pattern in name_lower)
            
            # åŸºæ–¼ä¸»è¦ç‰¹å¾µåˆ†é¡
            if verb_count > noun_count and verb_count > adj_count:
                pos_groups['å‹•è©å°å‘_å‹•ä½œå‹'].append(name)
            elif noun_count > verb_count and noun_count > adj_count:
                pos_groups['åè©å°å‘_å¯¦é«”å‹'].append(name)
            elif adj_count > 0:
                pos_groups['å½¢å®¹è©å°å‘_å±¬æ€§å‹'].append(name)
            else:
                pos_groups['æ··åˆå‹_è¤‡åˆæ¦‚å¿µ'].append(name)
        
        return dict(pos_groups)
    
    def _analyze_semantic_fields(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """èªç¾©å ´åˆ†æ - å®Œæ•´å¯¦ç¾"""
        semantic_fields = defaultdict(list)
        
        # å®šç¾©èªç¾©å ´
        fields_config = {
            'èªçŸ¥è¨ˆç®—é ˜åŸŸ': ['think', 'analyze', 'understand', 'learn', 'intelligence', 'smart', 'ai', 'ml'],
            'æ•¸æ“šè™•ç†é ˜åŸŸ': ['data', 'process', 'transform', 'parse', 'encode', 'decode', 'serialize'],
            'ç¶²çµ¡é€šä¿¡é ˜åŸŸ': ['http', 'api', 'client', 'server', 'request', 'response', 'network'],
            'å­˜å„²ç®¡ç†é ˜åŸŸ': ['store', 'save', 'load', 'cache', 'database', 'memory', 'persist'],
            'å®‰å…¨é˜²è­·é ˜åŸŸ': ['auth', 'security', 'encrypt', 'decrypt', 'validate', 'verify', 'protect'],
            'ç³»çµ±æ§åˆ¶é ˜åŸŸ': ['control', 'manage', 'monitor', 'supervise', 'coordinate', 'orchestrate'],
            'ç”¨æˆ¶äº¤äº’é ˜åŸŸ': ['ui', 'user', 'interface', 'display', 'render', 'view', 'presentation'],
            'æ¥­å‹™é‚è¼¯é ˜åŸŸ': ['business', 'logic', 'rule', 'workflow', 'process', 'operation'],
            'æ¸¬è©¦é©—è­‰é ˜åŸŸ': ['test', 'mock', 'stub', 'verify', 'assert', 'check', 'validate'],
            'é…ç½®ç®¡ç†é ˜åŸŸ': ['config', 'setting', 'option', 'parameter', 'property', 'preference']
        }
        
        for name, info in components.items():
            name_lower = name.lower()
            matched_fields = []
            
            # æª¢æŸ¥æ¯å€‹èªç¾©å ´
            for field_name, keywords in fields_config.items():
                match_count = sum(1 for keyword in keywords if keyword in name_lower)
                if match_count > 0:
                    matched_fields.append((field_name, match_count))
            
            # é¸æ“‡æœ€åŒ¹é…çš„èªç¾©å ´
            if matched_fields:
                best_field = max(matched_fields, key=lambda x: x[1])[0]
                semantic_fields[best_field].append(name)
            else:
                semantic_fields['é€šç”¨å·¥å…·é ˜åŸŸ'].append(name)
        
        return dict(semantic_fields)
    
    def _analyze_concept_hierarchy(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """æ¦‚å¿µéšå±¤åˆ†æ - å®Œæ•´å¯¦ç¾"""
        hierarchy_groups = defaultdict(list)
        
        for name, info in components.items():
            # è¨ˆç®—æ¦‚å¿µè¤‡é›œåº¦æŒ‡æ¨™
            word_count = len(re.findall(r'[A-Z][a-z]*|[a-z]+', name))
            separator_count = len(re.findall(r'[._-]', name))
            
            # æª¢æŸ¥æŠ½è±¡åº¦æŒ‡æ¨™
            abstract_indicators = ['base', 'abstract', 'generic', 'common', 'core']
            concrete_indicators = ['impl', 'implementation', 'specific', 'custom', 'detail']
            
            abstract_score = sum(1 for indicator in abstract_indicators if indicator.lower() in name.lower())
            concrete_score = sum(1 for indicator in concrete_indicators if indicator.lower() in name.lower())
            
            # è¨ˆç®—ç¸½è¤‡é›œåº¦
            complexity = word_count + separator_count * 0.5
            
            # åˆ†é¡
            if abstract_score > concrete_score:
                if complexity <= 2:
                    hierarchy_groups['é«˜åº¦æŠ½è±¡_æ ¸å¿ƒæ¦‚å¿µ'].append(name)
                else:
                    hierarchy_groups['æŠ½è±¡å±¤_è¨­è¨ˆæ¨¡å¼'].append(name)
            elif concrete_score > abstract_score:
                if complexity >= 4:
                    hierarchy_groups['å…·é«”å¯¦ç¾_æ¥­å‹™é‚è¼¯'].append(name)
                else:
                    hierarchy_groups['å¯¦ç¾å±¤_åŠŸèƒ½çµ„ä»¶'].append(name)
            else:
                if complexity <= 1:
                    hierarchy_groups['åŸå­æ¦‚å¿µ_åŸºç¤å–®å…ƒ'].append(name)
                elif complexity <= 3:
                    hierarchy_groups['ä¸­é–“å±¤_æœå‹™çµ„ä»¶'].append(name)
                else:
                    hierarchy_groups['è¤‡åˆæ¦‚å¿µ_é›†æˆæ¨¡å¡Š'].append(name)
        
        return dict(hierarchy_groups)

class ArchitecturalAnalyzer(BaseAnalyzer):
    """æ¶æ§‹åˆ†æå™¨"""
    
    def get_required_quality(self) -> AnalysisQuality:
        return AnalysisQuality.EXPERT
    
    def analyze(self, components: Dict[str, Any]) -> AnalysisResult:
        """åŸ·è¡Œæ¶æ§‹åˆ†æ"""
        start_time = time.time()
        
        groups = {}
        
        # 1. å…­é‚Šå½¢æ¶æ§‹åˆ†æ
        hex_groups = self._analyze_hexagonal_architecture(components)
        groups.update({f"å…­é‚Šå½¢_{k}": v for k, v in hex_groups.items()})
        
        # 2. åˆ†å±¤æ¶æ§‹åˆ†æ
        layer_groups = self._analyze_layered_architecture(components)
        groups.update({f"åˆ†å±¤_{k}": v for k, v in layer_groups.items()})
        
        # 3. å¾®æœå‹™æ¨¡å¼åˆ†æ
        microservice_groups = self._analyze_microservice_patterns(components)
        groups.update({f"å¾®æœå‹™_{k}": v for k, v in microservice_groups.items()})
        
        analysis_time = time.time() - start_time
        confidence = self._calculate_confidence(groups, len(components))
        
        return AnalysisResult(
            method_name="æ¶æ§‹æ™ºèƒ½åˆ†æ",
            category="æ¶æ§‹åˆ†æ", 
            quality_level=self.get_required_quality(),
            component_count=len(components),
            confidence_score=confidence,
            analysis_time=analysis_time,
            groups=groups,
            metadata={
                'hexagonal_groups': len(hex_groups),
                'layer_groups': len(layer_groups),
                'microservice_groups': len(microservice_groups)
            }
        )
    
    def _analyze_hexagonal_architecture(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å…­é‚Šå½¢æ¶æ§‹åˆ†æ"""
        hex_groups = defaultdict(list)
        
        patterns = {
            'é ˜åŸŸæ ¸å¿ƒ': ['domain', 'business', 'core', 'logic', 'entity', 'aggregate'],
            'æ‡‰ç”¨æœå‹™': ['application', 'service', 'use_case', 'command', 'query'],
            'è¼¸å…¥ç«¯å£': ['port', 'api', 'controller', 'handler', 'endpoint'],
            'è¼¸å‡ºç«¯å£': ['port', 'repository', 'gateway', 'adapter'],
            'è¼¸å…¥é©é…å™¨': ['adapter', 'web', 'rest', 'graphql', 'cli', 'ui'],
            'è¼¸å‡ºé©é…å™¨': ['adapter', 'database', 'file', 'http', 'message', 'email'],
            'åŸºç¤è¨­æ–½': ['infrastructure', 'config', 'logging', 'monitoring', 'security']
        }
        
        for name, info in components.items():
            name_lower = name.lower()
            best_match = None
            best_score = 0
            
            for arch_type, keywords in patterns.items():
                score = sum(1 for keyword in keywords if keyword in name_lower)
                if score > best_score:
                    best_score = score
                    best_match = arch_type
            
            if best_match:
                hex_groups[best_match].append(name)
            else:
                hex_groups['æœªåˆ†é¡çµ„ä»¶'].append(name)
        
        return dict(hex_groups)
    
    def _analyze_layered_architecture(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """åˆ†å±¤æ¶æ§‹åˆ†æ"""
        layer_groups = defaultdict(list)
        
        layer_patterns = {
            'è¡¨ç¾å±¤': ['ui', 'view', 'controller', 'presentation', 'web', 'api'],
            'æ‡‰ç”¨å±¤': ['application', 'app', 'service', 'facade', 'workflow'],
            'é ˜åŸŸå±¤': ['domain', 'business', 'model', 'entity', 'aggregate'],
            'åŸºç¤è¨­æ–½å±¤': ['infrastructure', 'repository', 'dao', 'database', 'external'],
            'è·¨å±¤é—œæ³¨é»': ['logging', 'security', 'monitoring', 'caching', 'validation']
        }
        
        for name, info in components.items():
            name_lower = name.lower()
            assigned = False
            
            for layer, keywords in layer_patterns.items():
                if any(keyword in name_lower for keyword in keywords):
                    layer_groups[layer].append(name)
                    assigned = True
                    break
            
            if not assigned:
                layer_groups['æ ¸å¿ƒé‚è¼¯å±¤'].append(name)
        
        return dict(layer_groups)
    
    def _analyze_microservice_patterns(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å¾®æœå‹™æ¨¡å¼åˆ†æ"""
        ms_groups = defaultdict(list)
        
        ms_patterns = {
            'APIç¶²é—œ': ['gateway', 'proxy', 'router', 'load_balancer'],
            'æœå‹™ç™¼ç¾': ['discovery', 'registry', 'consul', 'eureka'],
            'æ–·è·¯å™¨': ['circuit_breaker', 'hystrix', 'resilience', 'fault_tolerance'],
            'é…ç½®ä¸­å¿ƒ': ['config', 'configuration', 'settings', 'properties'],
            'æ¶ˆæ¯ç¸½ç·š': ['message', 'event', 'queue', 'broker', 'pubsub'],
            'ç›£æ§å‘Šè­¦': ['monitor', 'metrics', 'health', 'alert', 'trace'],
            'æœå‹™ç¶²æ ¼': ['mesh', 'sidecar', 'proxy', 'istio', 'envoy'],
            'æ•¸æ“šç®¡ç†': ['database', 'storage', 'cache', 'persistence']
        }
        
        for name, info in components.items():
            name_lower = name.lower()
            matched_patterns = []
            
            for pattern_name, keywords in ms_patterns.items():
                match_score = sum(1 for keyword in keywords if keyword in name_lower)
                if match_score > 0:
                    matched_patterns.append((pattern_name, match_score))
            
            if matched_patterns:
                best_pattern = max(matched_patterns, key=lambda x: x[1])[0]
                ms_groups[best_pattern].append(name)
            else:
                ms_groups['æ¥­å‹™æœå‹™'].append(name)
        
        return dict(ms_groups)

class QualityAnalyzer(BaseAnalyzer):
    """å“è³ªåˆ†æå™¨"""
    
    def get_required_quality(self) -> AnalysisQuality:
        return AnalysisQuality.STANDARD
    
    def analyze(self, components: Dict[str, Any]) -> AnalysisResult:
        """åŸ·è¡Œå“è³ªåˆ†æ"""
        start_time = time.time()
        
        groups = {}
        
        # åˆ†æå„ç¨®å“è³ªå±¬æ€§
        quality_aspects = {
            'å¯ç¶­è­·æ€§': self._analyze_maintainability(components),
            'å¯æ¸¬è©¦æ€§': self._analyze_testability(components),
            'æ€§èƒ½é—œæ³¨': self._analyze_performance(components),
            'å®‰å…¨æ€§': self._analyze_security(components),
            'å¯é æ€§': self._analyze_reliability(components)
        }
        
        for aspect, aspect_groups in quality_aspects.items():
            groups.update({f"{aspect}_{k}": v for k, v in aspect_groups.items()})
        
        analysis_time = time.time() - start_time
        confidence = self._calculate_confidence(groups, len(components))
        
        return AnalysisResult(
            method_name="å“è³ªæ™ºèƒ½åˆ†æ",
            category="å“è³ªåˆ†æ",
            quality_level=self.get_required_quality(),
            component_count=len(components),
            confidence_score=confidence,
            analysis_time=analysis_time,
            groups=groups,
            metadata={
                'quality_aspects': list(quality_aspects.keys()),
                'total_groups': len(groups)
            }
        )
    
    def _analyze_maintainability(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å¯ç¶­è­·æ€§åˆ†æ"""
        maintainability_groups = defaultdict(list)
        
        high_maintainability = ['simple', 'clean', 'clear', 'basic', 'util', 'helper']
        medium_maintainability = ['standard', 'common', 'regular']
        low_maintainability = ['complex', 'legacy', 'hack', 'temp', 'workaround', 'fix']
        
        for name, info in components.items():
            name_lower = name.lower()
            
            # è¨ˆç®—å¯ç¶­è­·æ€§åˆ†æ•¸
            high_score = sum(1 for pattern in high_maintainability if pattern in name_lower)
            low_score = sum(1 for pattern in low_maintainability if pattern in name_lower)
            
            # åŸºæ–¼åç¨±è¤‡é›œåº¦
            complexity_score = len(re.findall(r'[A-Z][a-z]*', name)) + len(re.findall(r'[._-]', name))
            
            if high_score > 0 or complexity_score <= 2:
                maintainability_groups['é«˜å¯ç¶­è­·æ€§'].append(name)
            elif low_score > 0 or complexity_score >= 6:
                maintainability_groups['ä½å¯ç¶­è­·æ€§'].append(name)
            else:
                maintainability_groups['ä¸­ç­‰å¯ç¶­è­·æ€§'].append(name)
        
        return dict(maintainability_groups)
    
    def _analyze_testability(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å¯æ¸¬è©¦æ€§åˆ†æ"""
        testability_groups = defaultdict(list)
        
        high_testability = ['test', 'mock', 'stub', 'fake', 'interface', 'injectable']
        low_testability = ['static', 'singleton', 'global', 'hardcoded', 'final']
        
        for name, info in components.items():
            name_lower = name.lower()
            
            high_score = sum(1 for pattern in high_testability if pattern in name_lower)
            low_score = sum(1 for pattern in low_testability if pattern in name_lower)
            
            if high_score > 0:
                testability_groups['é«˜å¯æ¸¬è©¦æ€§'].append(name)
            elif low_score > 0:
                testability_groups['ä½å¯æ¸¬è©¦æ€§'].append(name)
            else:
                testability_groups['ä¸­ç­‰å¯æ¸¬è©¦æ€§'].append(name)
        
        return dict(testability_groups)
    
    def _analyze_performance(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """æ€§èƒ½åˆ†æ"""
        performance_groups = defaultdict(list)
        
        performance_critical = ['performance', 'optimize', 'fast', 'cache', 'speed', 'efficient']
        performance_neutral = ['standard', 'regular', 'normal']
        
        for name, info in components.items():
            name_lower = name.lower()
            
            if any(pattern in name_lower for pattern in performance_critical):
                performance_groups['æ€§èƒ½é—œéµ'].append(name)
            else:
                performance_groups['æ€§èƒ½ä¸€èˆ¬'].append(name)
        
        return dict(performance_groups)
    
    def _analyze_security(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å®‰å…¨æ€§åˆ†æ"""
        security_groups = defaultdict(list)
        
        security_critical = ['auth', 'security', 'crypto', 'encrypt', 'password', 'token', 'key']
        security_sensitive = ['validate', 'verify', 'check', 'filter', 'sanitize']
        
        for name, info in components.items():
            name_lower = name.lower()
            
            if any(pattern in name_lower for pattern in security_critical):
                security_groups['å®‰å…¨é—œéµ'].append(name)
            elif any(pattern in name_lower for pattern in security_sensitive):
                security_groups['å®‰å…¨æ•æ„Ÿ'].append(name)
            else:
                security_groups['å®‰å…¨ä¸€èˆ¬'].append(name)
        
        return dict(security_groups)
    
    def _analyze_reliability(self, components: Dict[str, Any]) -> Dict[str, List[str]]:
        """å¯é æ€§åˆ†æ"""
        reliability_groups = defaultdict(list)
        
        high_reliability = ['robust', 'stable', 'reliable', 'resilient', 'fault_tolerant']
        low_reliability = ['experimental', 'beta', 'alpha', 'prototype', 'temp']
        
        for name, info in components.items():
            name_lower = name.lower()
            
            if any(pattern in name_lower for pattern in high_reliability):
                reliability_groups['é«˜å¯é æ€§'].append(name)
            elif any(pattern in name_lower for pattern in low_reliability):
                reliability_groups['ä½å¯é æ€§'].append(name)
            else:
                reliability_groups['æ¨™æº–å¯é æ€§'].append(name)
        
        return dict(reliability_groups)

class AnalysisOrchestrator:
    """åˆ†æç·¨æ’å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰åˆ†æå™¨"""
    
    def __init__(self, config_path: str = None):
        self.config = AnalysisConfig(config_path)
        self.cache = CacheManager(
            enabled=self.config.get('enable_caching', True),
            cache_dir=self.config.get('cache_dir', '.cache')
        )
        
        # è¨»å†Šåˆ†æå™¨
        self.analyzers = {
            'semantic': SemanticAnalyzer(self.config, self.cache),
            'architectural': ArchitecturalAnalyzer(self.config, self.cache),
            'quality': QualityAnalyzer(self.config, self.cache)
        }
        
        self.results = {}
        self.issues = []
    
    def run_analysis(self, components: Dict[str, Any], selected_analyzers: List[str] = None) -> Dict[str, AnalysisResult]:
        """é‹è¡Œåˆ†æ"""
        if selected_analyzers is None:
            selected_analyzers = list(self.analyzers.keys())
        
        logger.info(f"é–‹å§‹åˆ†æ {len(components)} å€‹çµ„ä»¶ï¼Œä½¿ç”¨åˆ†æå™¨: {selected_analyzers}")
        
        # ä¸¦è¡ŒåŸ·è¡Œåˆ†æ
        with ThreadPoolExecutor(max_workers=self.config.get('max_workers', 4)) as executor:
            future_to_analyzer = {}
            
            for analyzer_name in selected_analyzers:
                if analyzer_name in self.analyzers:
                    analyzer = self.analyzers[analyzer_name]
                    future = executor.submit(analyzer.analyze, components)
                    future_to_analyzer[future] = analyzer_name
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_analyzer):
                analyzer_name = future_to_analyzer[future]
                try:
                    result = future.result(timeout=self.config.get('timeout_seconds', 300))
                    self.results[analyzer_name] = result
                    
                    # é©—è­‰çµæœ
                    if self.config.get('enable_validation', True):
                        analyzer = self.analyzers[analyzer_name]
                        issues = analyzer.validate_result(result)
                        if issues:
                            self.issues.extend([f"{analyzer_name}: {issue}" for issue in issues])
                    
                    logger.info(f"å®Œæˆ {analyzer_name} åˆ†æï¼Œä¿¡å¿ƒåº¦: {result.confidence_score:.2f}")
                    
                except Exception as e:
                    logger.error(f"{analyzer_name} åˆ†æå¤±æ•—: {e}")
                    self.issues.append(f"{analyzer_name}: åˆ†æç•°å¸¸ - {str(e)}")
        
        return self.results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆå ±å‘Š"""
        if not self.results:
            return {'error': 'æ²’æœ‰åˆ†æçµæœ'}
        
        # çµ±è¨ˆä¿¡æ¯
        total_methods = sum(len(result.groups) for result in self.results.values())
        total_components = max(result.component_count for result in self.results.values()) if self.results else 0
        avg_confidence = sum(result.confidence_score for result in self.results.values()) / len(self.results)
        total_analysis_time = sum(result.analysis_time for result in self.results.values())
        
        # å“è³ªè©•ä¼°
        quality_distribution = defaultdict(int)
        for result in self.results.values():
            quality_distribution[result.quality_level.value] += 1
        
        # å•é¡Œçµ±è¨ˆ
        issue_categories = defaultdict(int)
        for issue in self.issues:
            category = issue.split(':')[0]
            issue_categories[category] += 1
        
        return {
            'summary': {
                'total_analyzers': len(self.results),
                'total_methods': total_methods,
                'total_components': total_components,
                'average_confidence': avg_confidence,
                'total_analysis_time': total_analysis_time,
                'issues_found': len(self.issues)
            },
            'quality_distribution': dict(quality_distribution),
            'analyzer_results': {name: result.to_dict() for name, result in self.results.items()},
            'issues': self.issues,
            'issue_categories': dict(issue_categories),
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ä¿¡å¿ƒåº¦çš„å»ºè­°
        low_confidence_analyzers = [
            name for name, result in self.results.items() 
            if result.confidence_score < 0.7
        ]
        
        if low_confidence_analyzers:
            recommendations.append(
                f"ä»¥ä¸‹åˆ†æå™¨ä¿¡å¿ƒåº¦è¼ƒä½ï¼Œå»ºè­°æª¢æŸ¥é…ç½®æˆ–å¢åŠ è¦å‰‡: {', '.join(low_confidence_analyzers)}"
            )
        
        # åŸºæ–¼å•é¡Œæ•¸é‡çš„å»ºè­°
        if len(self.issues) > 5:
            recommendations.append("ç™¼ç¾è¼ƒå¤šå•é¡Œï¼Œå»ºè­°å„ªåŒ–åˆ†æé…ç½®å’Œçµ„ä»¶å‘½åè¦ç¯„")
        
        # åŸºæ–¼æ€§èƒ½çš„å»ºè­°
        slow_analyzers = [
            name for name, result in self.results.items()
            if result.analysis_time > 10.0
        ]
        
        if slow_analyzers:
            recommendations.append(
                f"ä»¥ä¸‹åˆ†æå™¨é‹è¡Œè¼ƒæ…¢ï¼Œå»ºè­°å•Ÿç”¨ç·©å­˜æˆ–å„ªåŒ–ç®—æ³•: {', '.join(slow_analyzers)}"
            )
        
        return recommendations


class DiagramManager:
    """åœ–è¡¨ç®¡ç†å™¨ - è™•ç†åœ–è¡¨æ¸…ç†å’Œç‰ˆæœ¬æ§åˆ¶"""
    
    def __init__(self, workspace_root: Path):
        self.workspace_root = Path(workspace_root)
        self.diagram_patterns = {
            'keep': [
                '**/INTEGRATED_ARCHITECTURE_FINAL*.mmd',
                '**/ULTIMATE_ORGANIZATION_DISCOVERY*FINAL*.md',
                '**/INTELLIGENT_ANALYSIS*FINAL*.md',
                '**/architecture_diagrams/**/*FINAL*.mmd',
                '**/reports/**/*SUMMARY*.md',
            ],
            'cleanup': [
                '**/*AUTO_INTEGRATED*.mmd',
                '**/*_temp_*.mmd',
                '**/*_debug_*.json',
                '**/*_intermediate_*.mmd',
                '**/scripts/*_v[12]_*.py',  # æ¸…ç†èˆŠç‰ˆæœ¬è…³æœ¬
            ],
            'archive': [
                '**/*_v1_*.py',
                '**/*_v2_*.py',
                '**/backup/**/*',
            ]
        }
        
    def auto_cleanup(self) -> Dict[str, List[str]]:
        """è‡ªå‹•æ¸…ç†åœ–è¡¨å’Œè‡¨æ™‚æ–‡ä»¶"""
        results = {
            'cleaned': [],
            'archived': [],
            'kept': [],
            'errors': []
        }
        
        logger.info("ğŸ—‚ï¸ é–‹å§‹è‡ªå‹•åœ–è¡¨æ¸…ç†...")
        
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        for pattern in self.diagram_patterns['cleanup']:
            for file_path in self.workspace_root.glob(pattern):
                try:
                    if self._should_cleanup(file_path):
                        # æª¢æŸ¥æ˜¯å¦éœ€è¦å‚™ä»½
                        if self._is_important_for_backup(file_path):
                            self._backup_file(file_path)
                        
                        file_path.unlink()
                        results['cleaned'].append(str(file_path))
                        logger.info(f"ğŸ—‘ï¸ æ¸…ç†: {file_path.name}")
                        
                except Exception as e:
                    results['errors'].append(f"æ¸…ç†å¤±æ•— {file_path}: {e}")
                    logger.error(f"æ¸…ç†å¤±æ•—: {file_path} - {e}")
        
        # æ­¸æª”èˆŠç‰ˆæœ¬
        for pattern in self.diagram_patterns['archive']:
            for file_path in self.workspace_root.glob(pattern):
                try:
                    if self._should_archive(file_path):
                        archive_path = self._get_archive_path(file_path)
                        archive_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.move(str(file_path), str(archive_path))
                        results['archived'].append(str(archive_path))
                        logger.info(f"ğŸ“¦ æ­¸æª”: {file_path.name}")
                        
                except Exception as e:
                    results['errors'].append(f"æ­¸æª”å¤±æ•— {file_path}: {e}")
        
        # è¨˜éŒ„ä¿ç•™æ–‡ä»¶
        for pattern in self.diagram_patterns['keep']:
            for file_path in self.workspace_root.glob(pattern):
                results['kept'].append(str(file_path))
        
        logger.info(f"âœ… æ¸…ç†å®Œæˆ: æ¸…ç†{len(results['cleaned'])}å€‹, æ­¸æª”{len(results['archived'])}å€‹")
        return results
    
    def _should_cleanup(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ–‡ä»¶æ˜¯å¦æ‡‰è©²æ¸…ç†"""
        # æª¢æŸ¥æ˜¯å¦åœ¨ä¿ç•™åˆ—è¡¨ä¸­
        for keep_pattern in self.diagram_patterns['keep']:
            if file_path.match(keep_pattern.split('/')[-1]):
                return False
        
        # æª¢æŸ¥æ–‡ä»¶å¹´é½¡ (è¶…é1å¤©çš„è‡¨æ™‚æ–‡ä»¶)
        if file_path.stat().st_mtime < (time.time() - 86400):
            return True
            
        # æª¢æŸ¥æ˜¯å¦ç‚ºè‡ªå‹•ç”Ÿæˆçš„è‡¨æ™‚æ–‡ä»¶
        temp_indicators = ['_temp_', 'debug', 'intermediate', 'AUTO_INTEGRATED']
        return any(indicator in file_path.name for indicator in temp_indicators)
    
    def _should_archive(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ–‡ä»¶æ˜¯å¦æ‡‰è©²æ­¸æª”"""
        # V1.0 å’Œ V2.0 ç‰ˆæœ¬æ–‡ä»¶
        version_patterns = ['_v1_', '_v2_', 'v1.', 'v2.']
        return any(pattern in file_path.name for pattern in version_patterns)
    
    def _is_important_for_backup(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦å‚™ä»½"""
        important_keywords = ['FINAL', 'SUMMARY', 'REPORT', 'ANALYSIS']
        return any(keyword in file_path.name.upper() for keyword in important_keywords)
    
    def _backup_file(self, file_path: Path):
        """å‚™ä»½é‡è¦æ–‡ä»¶"""
        backup_dir = self.workspace_root / "_cleanup_backup" / datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backup_path = backup_dir / file_path.name
        shutil.copy2(str(file_path), str(backup_path))
        logger.info(f"ğŸ’¾ å‚™ä»½: {file_path.name} -> {backup_path}")
    
    def _get_archive_path(self, file_path: Path) -> Path:
        """ç²å–æ­¸æª”è·¯å¾‘"""
        archive_dir = self.workspace_root / "_archive" / "historical_versions"
        return archive_dir / file_path.name


class VariabilityManager:
    """è®Šç•°æ€§ç®¡ç†å™¨ - è™•ç†åˆ†æçµæœçš„è®Šç•°æ€§å’Œç©©å®šæ€§"""
    
    def __init__(self, workspace_root: Path):
        self.workspace_root = Path(workspace_root)
        self.history_file = workspace_root / "_out" / "analysis_history.json"
        
    def record_analysis(self, results: Dict[str, Any]) -> None:
        """è¨˜éŒ„åˆ†æçµæœæ­·å²"""
        history_entry = {
            'timestamp': datetime.now().isoformat(),
            'version': 'V3.0',
            'summary': results.get('summary', {}),
            'method_count': results.get('summary', {}).get('total_methods', 0),
            'component_count': results.get('summary', {}).get('total_components', 0),
            'quality_metrics': {
                'confidence': results.get('summary', {}).get('average_confidence', 0),
                'issues_count': results.get('summary', {}).get('issues_found', 0),
                'analysis_time': results.get('summary', {}).get('total_analysis_time', 0),
            }
        }
        
        # è¼‰å…¥æ­·å²è¨˜éŒ„
        history = self._load_history()
        history.append(history_entry)
        
        # ä¿æŒæœ€è¿‘10æ¬¡è¨˜éŒ„
        if len(history) > 10:
            history = history[-10:]
        
        # ä¿å­˜æ­·å²
        self._save_history(history)
        logger.info(f"ğŸ“ è¨˜éŒ„åˆ†ææ­·å²: {len(history)} æ¢è¨˜éŒ„")
    
    def analyze_variability(self) -> Dict[str, Any]:
        """åˆ†æçµæœè®Šç•°æ€§"""
        history = self._load_history()
        
        if len(history) < 2:
            return {'message': 'æ­·å²è¨˜éŒ„ä¸è¶³ï¼Œç„¡æ³•åˆ†æè®Šç•°æ€§'}
        
        # è¨ˆç®—è®Šç•°æ€§æŒ‡æ¨™
        method_counts = [entry['method_count'] for entry in history]
        component_counts = [entry['component_count'] for entry in history]
        confidence_scores = [entry['quality_metrics']['confidence'] for entry in history]
        
        variability_report = {
            'stability_metrics': {
                'method_count': {
                    'mean': sum(method_counts) / len(method_counts),
                    'variance': self._calculate_variance(method_counts),
                    'stability_score': 1 - (self._calculate_variance(method_counts) / max(method_counts, default=1))
                },
                'component_count': {
                    'mean': sum(component_counts) / len(component_counts),
                    'variance': self._calculate_variance(component_counts),
                    'stability_score': 1 - (self._calculate_variance(component_counts) / max(component_counts, default=1))
                },
                'confidence': {
                    'mean': sum(confidence_scores) / len(confidence_scores),
                    'variance': self._calculate_variance(confidence_scores),
                    'trend': 'improving' if confidence_scores[-1] > confidence_scores[0] else 'declining'
                }
            },
            'recommendations': self._generate_stability_recommendations(history),
            'last_comparison': self._compare_recent_analyses(history[-2:]) if len(history) >= 2 else None
        }
        
        return variability_report
    
    def _calculate_variance(self, values: List[float]) -> float:
        """è¨ˆç®—æ–¹å·®"""
        if not values:
            return 0
        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)
    
    def _generate_stability_recommendations(self, history: List[Dict]) -> List[str]:
        """ç”Ÿæˆç©©å®šæ€§å»ºè­°"""
        recommendations = []
        
        if len(history) < 3:
            return ["éœ€è¦æ›´å¤šæ­·å²æ•¸æ“šä¾†åˆ†æç©©å®šæ€§"]
        
        # æª¢æŸ¥æ–¹æ³•æ•¸é‡ç©©å®šæ€§
        method_counts = [entry['method_count'] for entry in history[-3:]]
        if self._calculate_variance(method_counts) > 100:
            recommendations.append("çµ„ç¹”æ–¹å¼æ•¸é‡æ³¢å‹•è¼ƒå¤§ï¼Œå»ºè­°æª¢æŸ¥åˆ†æé…ç½®çš„ä¸€è‡´æ€§")
        
        # æª¢æŸ¥ä¿¡å¿ƒåº¦è¶¨å‹¢
        confidence_scores = [entry['quality_metrics']['confidence'] for entry in history[-3:]]
        if confidence_scores[-1] < confidence_scores[0] - 0.1:
            recommendations.append("åˆ†æä¿¡å¿ƒåº¦ä¸‹é™ï¼Œå»ºè­°æª¢æŸ¥çµ„ä»¶å“è³ªæˆ–æ›´æ–°åˆ†æè¦å‰‡")
        
        # æª¢æŸ¥å•é¡Œæ•¸é‡è¶¨å‹¢
        issues_counts = [entry['quality_metrics']['issues_count'] for entry in history[-3:]]
        if issues_counts[-1] > issues_counts[0] + 2:
            recommendations.append("ç™¼ç¾å•é¡Œæ•¸é‡å¢åŠ ï¼Œå»ºè­°å„ªåŒ–åˆ†æç®—æ³•æˆ–çµ„ä»¶è¦ç¯„")
        
        return recommendations
    
    def _compare_recent_analyses(self, recent_entries: List[Dict]) -> Dict[str, Any]:
        """æ¯”è¼ƒæœ€è¿‘å…©æ¬¡åˆ†æ"""
        if len(recent_entries) < 2:
            return {}
        
        prev, curr = recent_entries
        
        return {
            'method_count_change': curr['method_count'] - prev['method_count'],
            'confidence_change': curr['quality_metrics']['confidence'] - prev['quality_metrics']['confidence'],
            'issues_change': curr['quality_metrics']['issues_count'] - prev['quality_metrics']['issues_count'],
            'performance_change': curr['quality_metrics']['analysis_time'] - prev['quality_metrics']['analysis_time'],
        }
    
    def _load_history(self) -> List[Dict]:
        """è¼‰å…¥åˆ†ææ­·å²"""
        if not self.history_file.exists():
            return []
        
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
            return []
    
    def _save_history(self, history: List[Dict]) -> None:
        """ä¿å­˜åˆ†ææ­·å²"""
        try:
            self.history_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    
    # è¼‰å…¥çµ„ä»¶æ•¸æ“š
    features_classification_path = Path(__file__).parent.parent / "_out" / "architecture_diagrams" / "features_diagram_classification.json"
    
    if not features_classification_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°ç‰¹å¾µåˆ†é¡æ–‡ä»¶: {features_classification_path}")
        return
    
    try:
        with open(features_classification_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        components = data.get('classifications', data) if isinstance(data, dict) else data
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(components)} å€‹çµ„ä»¶")
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ•¸æ“šå¤±æ•—: {e}")
        return
    
    # å‰µå»ºç®¡ç†å™¨
    workspace_root = Path(__file__).parent.parent
    diagram_manager = DiagramManager(workspace_root)
    variability_manager = VariabilityManager(workspace_root)
    
    # è‡ªå‹•æ¸…ç†åœ–è¡¨
    cleanup_results = diagram_manager.auto_cleanup()
    
    # å‰µå»ºåˆ†æç·¨æ’å™¨
    orchestrator = AnalysisOrchestrator()
    
    # é‹è¡Œåˆ†æ
    results = orchestrator.run_analysis(components)
    
    # ç”Ÿæˆå ±å‘Š
    report = orchestrator.generate_comprehensive_report()
    
    # è¨˜éŒ„åˆ†ææ­·å²
    variability_manager.record_analysis(report)
    
    # åˆ†æè®Šç•°æ€§
    variability_report = variability_manager.analyze_variability()
    
    # è¼¸å‡ºçµæœ
    print("\n" + "="*60)
    print("ğŸ¯ AIVA Features æ™ºèƒ½åˆ†æ V3.0 å®Œæˆ")
    print("="*60)
    
    summary = report['summary']
    print(f"ğŸ“Š åˆ†æçµ±è¨ˆ:")
    print(f"   - åˆ†æå™¨æ•¸é‡: {summary['total_analyzers']}")
    print(f"   - çµ„ç¹”æ–¹å¼æ•¸: {summary['total_methods']}")
    print(f"   - çµ„ä»¶ç¸½æ•¸: {summary['total_components']}")
    print(f"   - å¹³å‡ä¿¡å¿ƒåº¦: {summary['average_confidence']:.2f}")
    print(f"   - åˆ†æè€—æ™‚: {summary['total_analysis_time']:.1f}ç§’")
    print(f"   - ç™¼ç¾å•é¡Œ: {summary['issues_found']}å€‹")
    
    if report['recommendations']:
        print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"   {i}. {rec}")
    
    # é¡¯ç¤ºæ¸…ç†çµæœ
    if cleanup_results['cleaned'] or cleanup_results['archived']:
        print(f"\nğŸ—‚ï¸ åœ–è¡¨ç®¡ç†:")
        print(f"   - æ¸…ç†æ–‡ä»¶: {len(cleanup_results['cleaned'])}å€‹")
        print(f"   - æ­¸æª”æ–‡ä»¶: {len(cleanup_results['archived'])}å€‹")
        print(f"   - ä¿ç•™æ–‡ä»¶: {len(cleanup_results['kept'])}å€‹")
        if cleanup_results['errors']:
            print(f"   - è™•ç†éŒ¯èª¤: {len(cleanup_results['errors'])}å€‹")
    
    # é¡¯ç¤ºè®Šç•°æ€§åˆ†æ
    if 'stability_metrics' in variability_report:
        print(f"\nğŸ“Š ç©©å®šæ€§åˆ†æ:")
        stability = variability_report['stability_metrics']
        print(f"   - æ–¹æ³•æ•¸ç©©å®šåº¦: {stability['method_count']['stability_score']:.3f}")
        print(f"   - çµ„ä»¶æ•¸ç©©å®šåº¦: {stability['component_count']['stability_score']:.3f}")
        print(f"   - ä¿¡å¿ƒåº¦è¶¨å‹¢: {stability['confidence']['trend']}")
        
        if variability_report.get('recommendations'):
            print(f"\nğŸ”„ ç©©å®šæ€§å»ºè­°:")
            for i, rec in enumerate(variability_report['recommendations'], 1):
                print(f"   {i}. {rec}")
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    output_path = Path(__file__).parent / "intelligent_analysis_v3_report.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜è®Šç•°æ€§å ±å‘Š
    variability_path = Path(__file__).parent / "variability_analysis_report.json"
    with open(variability_path, 'w', encoding='utf-8') as f:
        json.dump(variability_report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“ è©³ç´°å ±å‘Šå·²ä¿å­˜: {output_path}")
    print(f"ğŸ“ˆ è®Šç•°æ€§å ±å‘Šå·²ä¿å­˜: {variability_path}")
    print("ğŸš€ V3.0 æ™ºèƒ½åˆ†ææ¡†æ¶å·²å°±ç·’ï¼")

if __name__ == "__main__":
    main()