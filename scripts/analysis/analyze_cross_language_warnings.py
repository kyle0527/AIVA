#!/usr/bin/env python3
"""
AIVA è·¨èªè¨€è­¦å‘Šåˆ†æå·¥å…·
ç”¨æ–¼è©³ç´°åˆ†æå’Œåˆ†é¡è·¨èªè¨€é©—è­‰ä¸­çš„è­¦å‘Š
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Any

class CrossLanguageWarningAnalyzer:
    """è·¨èªè¨€è­¦å‘Šåˆ†æå™¨"""
    
    def __init__(self, report_file: str = "cross_language_validation_report.json"):
        self.report_file = Path(report_file)
        self.warnings = []
        self.analysis = {}
        
    def load_validation_report(self) -> bool:
        """è¼‰å…¥é©—è­‰å ±å‘Š"""
        try:
            if not self.report_file.exists():
                print(f"âŒ æ‰¾ä¸åˆ°é©—è­‰å ±å‘Šæ–‡ä»¶: {self.report_file}")
                return False
                
            with open(self.report_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            self.warnings = [i for i in data.get('issues', []) if i.get('severity') == 'warning']
            print(f"âœ… è¼‰å…¥ {len(self.warnings)} å€‹è­¦å‘Š")
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥é©—è­‰å ±å‘Šå¤±æ•—: {e}")
            return False
    
    def analyze_warnings(self) -> Dict[str, Any]:
        """è©³ç´°åˆ†æè­¦å‘Š"""
        analysis = {
            'analysis_date': datetime.now().isoformat(),
            'total_warnings': len(self.warnings),
            'by_category': defaultdict(list),
            'by_schema': defaultdict(list), 
            'by_language': defaultdict(list),
            'by_message_pattern': defaultdict(list),
            'specific_issues': defaultdict(list)
        }
        
        # åˆ†é¡æ¯å€‹è­¦å‘Š
        for warning in self.warnings:
            category = warning.get('category', 'unknown')
            schema = warning.get('schema_name', 'unknown')
            languages = warning.get('languages', [])
            message = warning.get('message', '')
            field_name = warning.get('field_name', '')
            
            # åŸºæœ¬åˆ†é¡
            analysis['by_category'][category].append(warning)
            analysis['by_schema'][schema].append(warning)
            
            for lang in languages:
                analysis['by_language'][lang].append(warning)
            
            # æŒ‰æ¶ˆæ¯æ¨¡å¼è©³ç´°åˆ†é¡
            if 'é¡å‹' in message and 'æ²’æœ‰æ˜ å°„' in message:
                pattern = 'é¡å‹æ˜ å°„ç¼ºå¤±'
                # æå–å…·é«”ç¼ºå¤±çš„é¡å‹
                if "é¡å‹ '" in message and "' åœ¨" in message:
                    type_start = message.find("é¡å‹ '") + 3
                    type_end = message.find("' åœ¨", type_start)
                    missing_type = message[type_start:type_end]
                    analysis['specific_issues'][f'ç¼ºå¤±é¡å‹_{missing_type}'].append({
                        'schema': schema,
                        'field': field_name,
                        'languages': languages,
                        'type': missing_type
                    })
                    
            elif 'å¯é¸å­—æ®µ' in message and 'é¡å‹æ¨™è¨˜ä¸æ­£ç¢º' in message:
                pattern = 'å¯é¸å­—æ®µæ¨™è¨˜'
                # è¨˜éŒ„å…·é«”çš„å¯é¸å­—æ®µå•é¡Œ
                analysis['specific_issues'][f'å¯é¸å­—æ®µ_{schema}_{field_name}'].append({
                    'schema': schema,
                    'field': field_name,
                    'languages': languages,
                    'issue': 'é¡å‹æ¨™è¨˜ä¸æ­£ç¢º'
                })
                
            else:
                pattern = 'å…¶ä»–ä¸åŒ¹é…'
                
            analysis['by_message_pattern'][pattern].append(warning) 
        
        self.analysis = analysis
        return analysis
    
    def generate_detailed_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        if not self.analysis:
            self.analyze_warnings()
            
        # çµ±è¨ˆæ‘˜è¦
        summary = {
            'total_warnings': self.analysis['total_warnings'],
            'by_category': {k: len(v) for k, v in self.analysis['by_category'].items()},
            'by_language': {k: len(v) for k, v in self.analysis['by_language'].items()},
            'by_message_pattern': {k: len(v) for k, v in self.analysis['by_message_pattern'].items()}
        }
        
        # å•é¡Œ Schema æ’å
        schema_counts = Counter({k: len(v) for k, v in self.analysis['by_schema'].items()})
        top_problematic_schemas = dict(schema_counts.most_common(10))
        
        # å…·é«”å•é¡Œè©³æƒ…
        detailed_issues = {}
        for pattern, warnings_list in self.analysis['by_message_pattern'].items():
            if not warnings_list:
                continue
                
            pattern_detail = {
                'count': len(warnings_list),
                'affected_schemas': sorted(list(set(w.get('schema_name', '') for w in warnings_list))),
                'affected_languages': sorted(list(set(lang for w in warnings_list for lang in w.get('languages', [])))),
                'sample_messages': [w.get('message', '') for w in warnings_list[:3]],
                'improvement_suggestions': self._get_improvement_suggestions(pattern)
            }
            
            # æ·»åŠ å…·é«”æ¡ˆä¾‹
            if pattern == 'é¡å‹æ˜ å°„ç¼ºå¤±':
                missing_types = defaultdict(int)
                for w in warnings_list:
                    msg = w.get('message', '')
                    if "é¡å‹ '" in msg and "' åœ¨" in msg:
                        type_start = msg.find("é¡å‹ '") + 3
                        type_end = msg.find("' åœ¨", type_start)
                        missing_type = msg[type_start:type_end]
                        missing_types[missing_type] += 1
                        
                pattern_detail['missing_types'] = dict(Counter(missing_types).most_common(10))
                
            elif pattern == 'å¯é¸å­—æ®µæ¨™è¨˜':
                field_issues = defaultdict(int)
                for w in warnings_list:
                    schema = w.get('schema_name', '')
                    field = w.get('field_name', '')
                    if schema and field:
                        field_issues[f"{schema}.{field}"] += 1
                        
                pattern_detail['problematic_fields'] = dict(Counter(field_issues).most_common(10))
            
            detailed_issues[pattern] = pattern_detail
        
        report = {
            'analysis_metadata': {
                'analysis_date': self.analysis['analysis_date'],
                'report_file': str(self.report_file),
                'total_warnings': summary['total_warnings']
            },
            'summary': summary,
            'top_problematic_schemas': top_problematic_schemas,
            'detailed_issues': detailed_issues,
            'improvement_roadmap': self._generate_improvement_roadmap()
        }
        
        return report
    
    def _get_improvement_suggestions(self, pattern: str) -> List[str]:
        """ç²å–ç‰¹å®šæ¨¡å¼çš„æ”¹é€²å»ºè­°"""
        suggestions = {
            'é¡å‹æ˜ å°„ç¼ºå¤±': [
                "åœ¨ cross_language_interface.py ä¸­è£œå……å®Œæ•´çš„é¡å‹æ˜ å°„é…ç½®",
                "ç‚ºè¤‡åˆé¡å‹ (å¦‚ Optional[T], List[T], Dict[K,V]) æ·»åŠ æ˜ å°„è¦å‰‡",
                "å¯¦ç¾å‹•æ…‹é¡å‹æ˜ å°„ç”Ÿæˆæ©Ÿåˆ¶",
                "æ·»åŠ é¡å‹æ˜ å°„å®Œæ•´æ€§é©—è­‰"
            ],
            'å¯é¸å­—æ®µæ¨™è¨˜': [
                "çµ±ä¸€ YAML Schema ä¸­å¯é¸å­—æ®µçš„å®šç¾©æ ¼å¼",
                "æ”¹é€²ä»£ç¢¼ç”Ÿæˆå™¨å°å¯é¸é¡å‹çš„è™•ç†é‚è¼¯",
                "å»ºç«‹è·¨èªè¨€å¯é¸é¡å‹è½‰æ›è¦å‰‡",
                "æ·»åŠ å¯é¸å­—æ®µæ¨™è¨˜é©—è­‰æ©Ÿåˆ¶"
            ],
            'å…¶ä»–ä¸åŒ¹é…': [
                "çµ±ä¸€å‘½åç´„å®šè™•ç† (snake_case vs camelCase)",
                "æ”¹é€²è¨»è§£å’Œæ–‡æª”ç”Ÿæˆæ ¼å¼",
                "å„ªåŒ–é è¨­å€¼è™•ç†æ©Ÿåˆ¶",
                "å»ºç«‹ä»£ç¢¼é¢¨æ ¼ä¸€è‡´æ€§æª¢æŸ¥"
            ]
        }
        return suggestions.get(pattern, ["éœ€è¦é€²ä¸€æ­¥åˆ†æå…·é«”å•é¡Œ"])
    
    def _generate_improvement_roadmap(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ”¹é€²è·¯ç·šåœ–"""
        return {
            "Phase 1: é¡å‹æ˜ å°„å®Œå–„ (é«˜å„ªå…ˆç´š)": {
                "description": "è£œå……ç¼ºå¤±çš„é¡å‹æ˜ å°„ï¼Œè§£æ±º 337 å€‹é¡å‹æ˜ å°„è­¦å‘Š",
                "tasks": [
                    "è£œå…… Python èªè¨€çš„å®Œæ•´é¡å‹æ˜ å°„",
                    "æ·»åŠ è¤‡åˆé¡å‹æ˜ å°„è¦å‰‡ (Dict[str, Any], List[T] ç­‰)",
                    "å¯¦ç¾å‹•æ…‹é¡å‹æ˜ å°„ç”Ÿæˆ",
                    "å»ºç«‹é¡å‹æ˜ å°„é©—è­‰æ©Ÿåˆ¶"
                ],
                "expected_impact": "æ¸›å°‘ ~300 å€‹è­¦å‘Š"
            },
            "Phase 2: å¯é¸å­—æ®µæ¨™æº–åŒ– (ä¸­å„ªå…ˆç´š)": {
                "description": "çµ±ä¸€å¯é¸å­—æ®µè™•ç†ï¼Œè§£æ±º 352 å€‹å¯é¸å­—æ®µè­¦å‘Š",
                "tasks": [
                    "å»ºç«‹çµ±ä¸€çš„å¯é¸å­—æ®µ YAML å®šç¾©æ¨™æº–",
                    "æ”¹é€²ä»£ç¢¼ç”Ÿæˆå™¨çš„å¯é¸é¡å‹è™•ç†é‚è¼¯",
                    "æ·»åŠ è·¨èªè¨€å¯é¸é¡å‹é©—è­‰",
                    "å»ºç«‹å¯é¸å­—æ®µæœ€ä½³å¯¦è¸æ–‡æª”"
                ],
                "expected_impact": "æ¸›å°‘ ~300 å€‹è­¦å‘Š"
            },
            "Phase 3: ä»£ç¢¼å“è³ªæå‡ (ä½å„ªå…ˆç´š)": {
                "description": "æå‡æ•´é«”ä»£ç¢¼å“è³ªå’Œä¸€è‡´æ€§",
                "tasks": [
                    "çµ±ä¸€å‘½åç´„å®šè™•ç†",
                    "æ”¹é€²è¨»è§£å’Œæ–‡æª”ç”Ÿæˆ",
                    "å„ªåŒ–é è¨­å€¼è™•ç†æ©Ÿåˆ¶",
                    "å»ºç«‹æŒçºŒç›£æ§æ©Ÿåˆ¶"
                ],
                "expected_impact": "æ¸›å°‘å‰©é¤˜è­¦å‘Šï¼Œæå‡ç³»çµ±å“è³ª"
            }
        }
    
    def save_report(self, output_file: str = "detailed_warning_analysis.json") -> bool:
        """ä¿å­˜è©³ç´°å ±å‘Š"""
        try:
            report = self.generate_detailed_report()
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            print(f"âœ… è©³ç´°è­¦å‘Šåˆ†æå ±å‘Šå·²ä¿å­˜: {output_file}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å ±å‘Šå¤±æ•—: {e}")
            return False
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        if not self.analysis:
            self.analyze_warnings()
            
        print("\n" + "="*60)
        print("ğŸ” AIVA è·¨èªè¨€è­¦å‘Šè©³ç´°åˆ†ææ‘˜è¦")
        print("="*60)
        
        print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   ç¸½è­¦å‘Šæ•¸é‡: {self.analysis['total_warnings']} å€‹")
        print(f"   åˆ†ææ™‚é–“: {self.analysis['analysis_date']}")
        
        print(f"\nğŸ“ æŒ‰é¡åˆ¥åˆ†å¸ƒ:")
        for pattern, warnings_list in self.analysis['by_message_pattern'].items():
            print(f"   {pattern}: {len(warnings_list)} å€‹")
            
        print(f"\nğŸŒ æŒ‰èªè¨€åˆ†å¸ƒ:")
        for lang, warnings_list in self.analysis['by_language'].items():
            print(f"   {lang}: {len(warnings_list)} å€‹")
            
        print(f"\nâš ï¸  æœ€å¤šè­¦å‘Šçš„ Schema (å‰5å):")
        schema_counts = Counter({k: len(v) for k, v in self.analysis['by_schema'].items()})
        for schema, count in schema_counts.most_common(5):
            print(f"   {schema}: {count} å€‹è­¦å‘Š")
            
        print(f"\nâœ¨ æ”¹é€²æ½›åŠ›:")
        print(f"   é€šéé¡å‹æ˜ å°„å®Œå–„å¯æ¸›å°‘ ~337 å€‹è­¦å‘Š")
        print(f"   é€šéå¯é¸å­—æ®µæ¨™æº–åŒ–å¯æ¸›å°‘ ~352 å€‹è­¦å‘Š")
        print(f"   é æœŸå¯å°‡è­¦å‘Šæ•¸é‡é™è‡³ <100 å€‹")

def main():
    """ä¸»å‡½æ•¸"""
    analyzer = CrossLanguageWarningAnalyzer()
    
    if not analyzer.load_validation_report():
        sys.exit(1)
        
    analyzer.print_summary()
    analyzer.save_report()
    
    print(f"\nğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•:")
    print(f"   1. æŸ¥çœ‹ detailed_warning_analysis.json äº†è§£è©³ç´°å•é¡Œ")
    print(f"   2. åƒè€ƒ CROSS_LANGUAGE_WARNING_ANALYSIS.md åˆ¶å®šæ”¹é€²è¨ˆåŠƒ")
    print(f"   3. å„ªå…ˆè™•ç†é¡å‹æ˜ å°„ç¼ºå¤±å•é¡Œ")

if __name__ == "__main__":
    main()