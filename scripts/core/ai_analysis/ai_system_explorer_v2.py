"""
AIVA AI ç³»çµ±æ¢ç´¢å™¨ v2.0 - å¤šèªè¨€å¢é‡åˆ†æç‰ˆæœ¬

ä¸»è¦æ”¹é€²:
1. å¤šèªè¨€æ”¯æ´ (Python, Go, Rust, TypeScript, JavaScript)
2. å¢é‡æ¢ç´¢èˆ‡é€²åº¦æŒä¹…åŒ–
3. è·¨èªè¨€ä¾è³´é—œä¿‚åˆ†æ
4. æ™ºæ…§å»ºè­°ç³»çµ±
5. èªç¾©åˆ†æèƒ½åŠ›

ä½œè€…: AIVA Development Team
ç‰ˆæœ¬: 2.0.0
æ—¥æœŸ: 2025-10-28
"""

import asyncio
import json
import sqlite3
import hashlib
import subprocess
import logging
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import argparse

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(name)s - %(message)s',
    datefmt='%Y-%m-%dT%H:%M:%S%z'
)
logger = logging.getLogger(__name__)

@dataclass
class FileAnalysis:
    path: str
    language: str
    size_bytes: int
    line_count: int
    last_modified: datetime
    file_hash: str
    functions: List[str] = None
    classes: List[str] = None
    imports: List[str] = None
    exports: List[str] = None
    
@dataclass
class ModuleAnalysis:
    module_id: str
    name: str
    path: str
    language: str
    files: List[FileAnalysis]
    dependencies: List[str]
    cross_language_calls: List[str]
    health_score: float
    last_analysis: datetime
    issues: List[str]
    warnings: List[str]

@dataclass
class ExplorationSnapshot:
    report_id: str
    timestamp: datetime
    modules: Dict[str, ModuleAnalysis]
    overall_health: float
    total_files: int
    total_lines: int
    language_distribution: Dict[str, int]

class ExplorationDatabase:
    """æ¢ç´¢é€²åº¦æŒä¹…åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "reports/ai_diagnostics/exploration.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–è³‡æ–™åº«çµæ§‹"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS explorations (
                    id TEXT PRIMARY KEY,
                    timestamp TEXT,
                    overall_health REAL,
                    total_files INTEGER,
                    total_lines INTEGER,
                    raw_data TEXT
                );
                
                CREATE TABLE IF NOT EXISTS file_checksums (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT,
                    last_modified TEXT,
                    file_size INTEGER
                );
                
                CREATE TABLE IF NOT EXISTS module_analysis (
                    module_id TEXT,
                    exploration_id TEXT,
                    analysis_data TEXT,
                    health_score REAL,
                    timestamp TEXT,
                    PRIMARY KEY (module_id, exploration_id)
                );
                
                CREATE INDEX IF NOT EXISTS idx_exploration_timestamp 
                ON explorations(timestamp);
                
                CREATE INDEX IF NOT EXISTS idx_file_hash 
                ON file_checksums(file_hash);
            """)
    
    def save_exploration(self, snapshot: ExplorationSnapshot):
        """ä¿å­˜æ¢ç´¢å¿«ç…§"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO explorations 
                (id, timestamp, overall_health, total_files, total_lines, raw_data)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                snapshot.report_id,
                snapshot.timestamp.isoformat(),
                snapshot.overall_health,
                snapshot.total_files,
                snapshot.total_lines,
                json.dumps(asdict(snapshot), default=str)
            ))
            
            # ä¿å­˜æ¨¡çµ„åˆ†æ
            for module_id, analysis in snapshot.modules.items():
                conn.execute("""
                    INSERT OR REPLACE INTO module_analysis
                    (module_id, exploration_id, analysis_data, health_score, timestamp)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    module_id,
                    snapshot.report_id,
                    json.dumps(asdict(analysis), default=str),
                    analysis.health_score,
                    analysis.last_analysis.isoformat()
                ))
    
    def get_last_exploration(self) -> Optional[ExplorationSnapshot]:
        """ç²å–æœ€è¿‘ä¸€æ¬¡æ¢ç´¢çµæœ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT raw_data FROM explorations 
                ORDER BY timestamp DESC LIMIT 1
            """)
            row = cursor.fetchone()
            if row:
                data = json.loads(row[0])
                # é‡å»º ExplorationSnapshot ç‰©ä»¶
                return self._reconstruct_snapshot(data)
        return None
    
    def check_file_changed(self, file_path: str) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦æœ‰è®Šæ›´"""
        try:
            current_stat = os.stat(file_path)
            current_hash = self._calculate_file_hash(file_path)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT file_hash, file_size FROM file_checksums 
                    WHERE file_path = ?
                """, (file_path,))
                row = cursor.fetchone()
                
                if not row:
                    # æ–°æª”æ¡ˆï¼Œè¨˜éŒ„ä¸¦è¿”å› True
                    self._update_file_checksum(file_path, current_hash, current_stat)
                    return True
                
                stored_hash, stored_size = row
                if stored_hash != current_hash or stored_size != current_stat.st_size:
                    # æª”æ¡ˆæœ‰è®Šæ›´ï¼Œæ›´æ–°è¨˜éŒ„
                    self._update_file_checksum(file_path, current_hash, current_stat)
                    return True
                    
            return False
            
        except (OSError, IOError):
            return True  # æª”æ¡ˆä¸å¯è®€ï¼Œç•¶ä½œæœ‰è®Šæ›´
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è¨ˆç®—æª”æ¡ˆé›œæ¹Šå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except (OSError, IOError):
            return ""
    
    def _update_file_checksum(self, file_path: str, file_hash: str, file_stat):
        """æ›´æ–°æª”æ¡ˆé›œæ¹Šè¨˜éŒ„"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO file_checksums
                (file_path, file_hash, last_modified, file_size)
                VALUES (?, ?, ?, ?)
            """, (
                file_path,
                file_hash,
                datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                file_stat.st_size
            ))
    
    def _reconstruct_snapshot(self, data: dict) -> ExplorationSnapshot:
        """é‡å»ºæ¢ç´¢å¿«ç…§ç‰©ä»¶"""
        # é€™è£¡éœ€è¦æ›´è¤‡é›œçš„é‡å»ºé‚è¼¯
        # æš«æ™‚è¿”å›åŸºæœ¬çµæ§‹
        return ExplorationSnapshot(
            report_id=data.get('report_id', ''),
            timestamp=datetime.fromisoformat(data.get('timestamp', datetime.now().isoformat())),
            modules={},
            overall_health=data.get('overall_health', 0.0),
            total_files=data.get('total_files', 0),
            total_lines=data.get('total_lines', 0),
            language_distribution=data.get('language_distribution', {})
        )

class LanguageAnalyzer:
    """å¤šèªè¨€åˆ†æå™¨åŸºé¡"""
    
    def __init__(self, language: str):
        self.language = language
    
    async def analyze_file(self, file_path: str) -> FileAnalysis:
        """åˆ†æå–®å€‹æª”æ¡ˆ"""
        try:
            stat = os.stat(file_path)
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            return FileAnalysis(
                path=file_path,
                language=self.language,
                size_bytes=stat.st_size,
                line_count=len(content.splitlines()),
                last_modified=datetime.fromtimestamp(stat.st_mtime),
                file_hash=hashlib.md5(content.encode()).hexdigest(),
                functions=await self._extract_functions(content),
                classes=await self._extract_classes(content),
                imports=await self._extract_imports(content),
                exports=await self._extract_exports(content)
            )
        except Exception as e:
            logger.warning(f"åˆ†ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return None
    
    async def _extract_functions(self, content: str) -> List[str]:
        """æå–å‡½æ•¸åç¨± - å­é¡å¯¦ç¾"""
        return []
    
    async def _extract_classes(self, content: str) -> List[str]:
        """æå–é¡åˆ¥åç¨± - å­é¡å¯¦ç¾"""
        return []
    
    async def _extract_imports(self, content: str) -> List[str]:
        """æå–å°å…¥ä¾è³´ - å­é¡å¯¦ç¾"""
        return []
    
    async def _extract_exports(self, content: str) -> List[str]:
        """æå–å°å‡ºé …ç›® - å­é¡å¯¦ç¾"""
        return []

class PythonAnalyzer(LanguageAnalyzer):
    """Python ä»£ç¢¼åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("Python")
    
    async def _extract_functions(self, content: str) -> List[str]:
        import re
        pattern = r'^def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        return re.findall(pattern, content, re.MULTILINE)
    
    async def _extract_classes(self, content: str) -> List[str]:
        import re
        pattern = r'^class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[\(:]'
        return re.findall(pattern, content, re.MULTILINE)
    
    async def _extract_imports(self, content: str) -> List[str]:
        import re
        imports = []
        # from x import y
        pattern1 = r'^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import'
        imports.extend(re.findall(pattern1, content, re.MULTILINE))
        # import x
        pattern2 = r'^import\s+([a-zA-Z_][a-zA-Z0-9_.]*)'
        imports.extend(re.findall(pattern2, content, re.MULTILINE))
        return list(set(imports))

class GoAnalyzer(LanguageAnalyzer):
    """Go ä»£ç¢¼åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("Go")
    
    async def _extract_functions(self, content: str) -> List[str]:
        import re
        pattern = r'^func\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        return re.findall(pattern, content, re.MULTILINE)
    
    async def _extract_imports(self, content: str) -> List[str]:
        import re
        imports = []
        # Single import
        pattern1 = r'^import\s+"([^"]+)"'
        imports.extend(re.findall(pattern1, content, re.MULTILINE))
        # Multi import block
        pattern2 = r'import\s*\(\s*([^)]+)\s*\)'
        blocks = re.findall(pattern2, content, re.DOTALL)
        for block in blocks:
            import_lines = re.findall(r'"([^"]+)"', block)
            imports.extend(import_lines)
        return list(set(imports))

class RustAnalyzer(LanguageAnalyzer):
    """Rust ä»£ç¢¼åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("Rust")
    
    async def _extract_functions(self, content: str) -> List[str]:
        import re
        pattern = r'^(?:pub\s+)?fn\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[\(<]'
        return re.findall(pattern, content, re.MULTILINE)
    
    async def _extract_imports(self, content: str) -> List[str]:
        import re
        imports = []
        # use statements
        pattern = r'^use\s+([a-zA-Z_][a-zA-Z0-9_:]*)'
        imports.extend(re.findall(pattern, content, re.MULTILINE))
        return list(set(imports))

class TypeScriptAnalyzer(LanguageAnalyzer):
    """TypeScript ä»£ç¢¼åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("TypeScript")
    
    async def _extract_functions(self, content: str) -> List[str]:
        import re
        functions = []
        # function declarations
        pattern1 = r'^(?:export\s+)?function\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\('
        functions.extend(re.findall(pattern1, content, re.MULTILINE))
        # arrow functions
        pattern2 = r'(?:const|let|var)\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*\([^)]*\)\s*=>'
        functions.extend(re.findall(pattern2, content, re.MULTILINE))
        return functions
    
    async def _extract_imports(self, content: str) -> List[str]:
        import re
        imports = []
        # ES6 imports
        pattern1 = r'^import\s+.*from\s+[\'"]([^\'"]+)[\'"]'
        imports.extend(re.findall(pattern1, content, re.MULTILINE))
        # require statements
        pattern2 = r'require\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)'
        imports.extend(re.findall(pattern2, content))
        return list(set(imports))

class IncrementalSystemExplorer:
    """å¢é‡ç³»çµ±æ¢ç´¢å™¨"""
    
    def __init__(self, workspace_root: str = None):
        self.workspace_root = Path(workspace_root or os.getcwd())
        self.db = ExplorationDatabase()
        self.analyzers = {
            '.py': PythonAnalyzer(),
            '.go': GoAnalyzer(),
            '.rs': RustAnalyzer(),
            '.ts': TypeScriptAnalyzer(),
            '.js': TypeScriptAnalyzer(),  # ä½¿ç”¨ TS åˆ†æå™¨è™•ç† JS
        }
        
        # æ¨¡çµ„é…ç½®
        self.module_configs = {
            "ai_core": {
                "name": "AI æ ¸å¿ƒå¼•æ“",
                "path": "services/core/aiva_core",
                "primary_language": "Python"
            },
            "attack_engine": {
                "name": "æ”»æ“ŠåŸ·è¡Œå¼•æ“", 
                "path": "services/core/aiva_core/attack",
                "primary_language": "Python"
            },
            "scan_engine": {
                "name": "æƒæå¼•æ“",
                "path": "services/scan",
                "primary_language": "Mixed"
            },
            "integration_service": {
                "name": "æ•´åˆæœå‹™",
                "path": "services/integration",
                "primary_language": "Python"
            },
            "feature_detection": {
                "name": "åŠŸèƒ½æª¢æ¸¬",
                "path": "services/features",
                "primary_language": "Mixed"
            }
        }
    
    async def incremental_explore(self, force_full: bool = False) -> ExplorationSnapshot:
        """åŸ·è¡Œå¢é‡æ¢ç´¢"""
        logger.info("ğŸ” é–‹å§‹å¢é‡ç³»çµ±æ¢ç´¢...")
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å®Œå…¨é‡æ–°æƒæ
        last_exploration = self.db.get_last_exploration()
        if force_full or not last_exploration:
            logger.info("ğŸ“Š åŸ·è¡Œå®Œæ•´ç³»çµ±æƒæ...")
            return await self._full_exploration()
        
        # æª¢æŸ¥è®Šæ›´çš„æª”æ¡ˆ
        changed_modules = await self._detect_changed_modules()
        
        if not changed_modules:
            logger.info("âœ… æ²’æœ‰æª¢æ¸¬åˆ°æ¨¡çµ„è®Šæ›´ï¼Œä½¿ç”¨ç·©å­˜çµæœ")
            return last_exploration
        
        logger.info(f"ğŸ”„ æª¢æ¸¬åˆ° {len(changed_modules)} å€‹æ¨¡çµ„æœ‰è®Šæ›´: {', '.join(changed_modules)}")
        return await self._incremental_exploration(last_exploration, changed_modules)
    
    async def _detect_changed_modules(self) -> List[str]:
        """æª¢æ¸¬æœ‰è®Šæ›´çš„æ¨¡çµ„"""
        changed_modules = []
        
        for module_id, config in self.module_configs.items():
            module_path = self.workspace_root / config["path"]
            if not module_path.exists():
                continue
            
            # æª¢æŸ¥æ¨¡çµ„ç›®éŒ„ä¸‹çš„æª”æ¡ˆ
            has_changes = False
            for file_path in module_path.rglob("*"):
                if file_path.is_file() and self._is_code_file(file_path):
                    if self.db.check_file_changed(str(file_path)):
                        has_changes = True
                        break
            
            if has_changes:
                changed_modules.append(module_id)
        
        return changed_modules
    
    async def _incremental_exploration(self, 
                                     last_exploration: ExplorationSnapshot,
                                     changed_modules: List[str]) -> ExplorationSnapshot:
        """åŸ·è¡Œå¢é‡æ¢ç´¢"""
        logger.info(f"ğŸ”„ å¢é‡åˆ†æ {len(changed_modules)} å€‹è®Šæ›´çš„æ¨¡çµ„...")
        
        # è¤‡è£½ä¸Šæ¬¡çš„çµæœ
        new_modules = last_exploration.modules.copy()
        
        # é‡æ–°åˆ†æè®Šæ›´çš„æ¨¡çµ„
        for module_id in changed_modules:
            if module_id in self.module_configs:
                logger.info(f"ğŸ” é‡æ–°åˆ†ææ¨¡çµ„: {self.module_configs[module_id]['name']}")
                analysis = await self._analyze_module(module_id, self.module_configs[module_id])
                if analysis:
                    new_modules[module_id] = analysis
        
        # é‡æ–°è¨ˆç®—æ•´é«”å¥åº·ç‹€æ…‹
        overall_health = self._calculate_overall_health(new_modules)
        total_files = sum(len(analysis.files) for analysis in new_modules.values())
        total_lines = sum(sum(f.line_count for f in analysis.files) 
                         for analysis in new_modules.values())
        
        # è¨ˆç®—èªè¨€åˆ†å¸ƒ
        language_distribution = self._calculate_language_distribution(new_modules)
        
        # å‰µå»ºæ–°çš„æ¢ç´¢å¿«ç…§
        snapshot = ExplorationSnapshot(
            report_id=f"incremental-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            timestamp=datetime.now(),
            modules=new_modules,
            overall_health=overall_health,
            total_files=total_files,
            total_lines=total_lines,
            language_distribution=language_distribution
        )
        
        # ä¿å­˜çµæœ
        self.db.save_exploration(snapshot)
        
        logger.info(f"âœ… å¢é‡æ¢ç´¢å®Œæˆ (å¥åº·åˆ†æ•¸: {overall_health:.2f})")
        return snapshot
    
    async def _full_exploration(self) -> ExplorationSnapshot:
        """åŸ·è¡Œå®Œæ•´æ¢ç´¢"""
        logger.info("ğŸ“Š åŸ·è¡Œå®Œæ•´ç³»çµ±æ¢ç´¢...")
        
        modules = {}
        for module_id, config in self.module_configs.items():
            logger.info(f"ğŸ” åˆ†ææ¨¡çµ„: {config['name']}")
            analysis = await self._analyze_module(module_id, config)
            if analysis:
                modules[module_id] = analysis
        
        # è¨ˆç®—æ•´é«”æŒ‡æ¨™
        overall_health = self._calculate_overall_health(modules)
        total_files = sum(len(analysis.files) for analysis in modules.values())
        total_lines = sum(sum(f.line_count for f in analysis.files) 
                         for analysis in modules.values())
        language_distribution = self._calculate_language_distribution(modules)
        
        # å‰µå»ºæ¢ç´¢å¿«ç…§
        snapshot = ExplorationSnapshot(
            report_id=f"full-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            timestamp=datetime.now(),
            modules=modules,
            overall_health=overall_health,
            total_files=total_files,
            total_lines=total_lines,
            language_distribution=language_distribution
        )
        
        # ä¿å­˜çµæœ
        self.db.save_exploration(snapshot)
        
        logger.info(f"âœ… å®Œæ•´æ¢ç´¢å®Œæˆ (å¥åº·åˆ†æ•¸: {overall_health:.2f})")
        return snapshot
    
    async def _analyze_module(self, module_id: str, config: dict) -> Optional[ModuleAnalysis]:
        """åˆ†æå–®å€‹æ¨¡çµ„"""
        module_path = self.workspace_root / config["path"]
        if not module_path.exists():
            logger.warning(f"æ¨¡çµ„è·¯å¾‘ä¸å­˜åœ¨: {module_path}")
            return None
        
        files = []
        dependencies = set()
        cross_language_calls = []
        issues = []
        warnings = []
        
        # æƒææ‰€æœ‰ç¨‹å¼ç¢¼æª”æ¡ˆ
        for file_path in module_path.rglob("*"):
            if file_path.is_file() and self._is_code_file(file_path):
                analysis = await self._analyze_file(file_path)
                if analysis:
                    files.append(analysis)
                    if analysis.imports:
                        dependencies.update(analysis.imports)
        
        if not files:
            warnings.append("æ¨¡çµ„ä¸­æ²’æœ‰æ‰¾åˆ°ç¨‹å¼ç¢¼æª”æ¡ˆ")
        
        # æª¢æ¸¬è·¨èªè¨€èª¿ç”¨
        cross_language_calls = await self._detect_cross_language_calls(files)
        
        # è¨ˆç®—å¥åº·åˆ†æ•¸
        health_score = self._calculate_module_health(files, issues, warnings)
        
        return ModuleAnalysis(
            module_id=module_id,
            name=config["name"],
            path=config["path"],
            language=config["primary_language"],
            files=files,
            dependencies=list(dependencies),
            cross_language_calls=cross_language_calls,
            health_score=health_score,
            last_analysis=datetime.now(),
            issues=issues,
            warnings=warnings
        )
    
    async def _analyze_file(self, file_path: Path) -> Optional[FileAnalysis]:
        """åˆ†æå–®å€‹æª”æ¡ˆ"""
        extension = file_path.suffix.lower()
        analyzer = self.analyzers.get(extension)
        
        if analyzer:
            return await analyzer.analyze_file(str(file_path))
        else:
            # åŸºæœ¬æª”æ¡ˆåˆ†æ - è·³éå¤§å‹éæ–‡æœ¬æª”æ¡ˆ
            try:
                stat = file_path.stat()
                
                # è·³ééå¤§çš„æª”æ¡ˆ (>10MB)
                if stat.st_size > 10 * 1024 * 1024:
                    logger.debug(f"è·³éå¤§å‹æª”æ¡ˆ: {file_path} ({stat.st_size} bytes)")
                    return None
                
                # è·³éæ˜é¡¯çš„äºŒé€²ä½æª”æ¡ˆ
                if self._is_binary_file(file_path):
                    return None
                
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                return FileAnalysis(
                    path=str(file_path),
                    language="Unknown",
                    size_bytes=stat.st_size,
                    line_count=len(content.splitlines()),
                    last_modified=datetime.fromtimestamp(stat.st_mtime),
                    file_hash=hashlib.md5(content.encode()).hexdigest()
                )
            except Exception as e:
                logger.warning(f"åŸºæœ¬æª”æ¡ˆåˆ†æå¤±æ•— {file_path}: {e}")
                return None
    
    async def _detect_cross_language_calls(self, files: List[FileAnalysis]) -> List[str]:
        """æª¢æ¸¬è·¨èªè¨€èª¿ç”¨"""
        calls = []
        
        for file_analysis in files:
            if not file_analysis.imports:
                continue
                
            for import_item in file_analysis.imports:
                # æª¢æ¸¬ Python -> Go FFI
                if 'ctypes' in import_item or 'cffi' in import_item:
                    calls.append(f"Python->C/Go FFI: {import_item}")
                
                # æª¢æ¸¬ Python -> Rust PyO3
                if any(rust_lib in import_item.lower() 
                      for rust_lib in ['pyo3', 'maturin']):
                    calls.append(f"Python->Rust: {import_item}")
                
                # æª¢æ¸¬ Node.js æ•´åˆ
                if 'subprocess' in import_item or 'child_process' in import_item:
                    calls.append(f"Process integration: {import_item}")
        
        return calls
    
    def _is_code_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºç¨‹å¼ç¢¼æª”æ¡ˆ"""
        code_extensions = {'.py', '.go', '.rs', '.ts', '.js', '.html', '.css', '.sql'}
        return file_path.suffix.lower() in code_extensions
    
    def _is_binary_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºäºŒé€²ä½æª”æ¡ˆ"""
        binary_extensions = {
            '.exe', '.dll', '.so', '.dylib', '.a', '.lib', '.obj', '.o',
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.svg',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.zip', '.tar', '.gz', '.rar', '.7z',
            '.mp3', '.mp4', '.avi', '.mov', '.wav',
            '.db', '.sqlite', '.bin', '.dat'
        }
        
        extension = file_path.suffix.lower()
        if extension in binary_extensions:
            return True
        
        # é¡å¤–æª¢æŸ¥ï¼šå˜—è©¦è®€å–å‰å¹¾å€‹ä½å…ƒçµ„
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(512)
                # å¦‚æœåŒ…å«å¤ªå¤š null å­—ç¯€ï¼Œå¯èƒ½æ˜¯äºŒé€²ä½æª”æ¡ˆ
                if chunk.count(b'\x00') > len(chunk) * 0.1:
                    return True
        except:
            pass
        
        return False
    
    def _calculate_module_health(self, files: List[FileAnalysis], 
                               issues: List[str], warnings: List[str]) -> float:
        """è¨ˆç®—æ¨¡çµ„å¥åº·åˆ†æ•¸"""
        if not files:
            return 0.0
        
        base_score = 1.0
        
        # æ‰£é™¤å•é¡Œåˆ†æ•¸
        base_score -= len(issues) * 0.2
        base_score -= len(warnings) * 0.1
        
        # æ ¹æ“šæª”æ¡ˆå¤§å°èª¿æ•´ (é¿å…å–®å€‹å·¨å¤§æª”æ¡ˆ)
        avg_file_size = sum(f.size_bytes for f in files) / len(files)
        if avg_file_size > 50000:  # 50KB
            base_score -= 0.1
        
        return max(0.0, min(1.0, base_score))
    
    def _calculate_overall_health(self, modules: Dict[str, ModuleAnalysis]) -> float:
        """è¨ˆç®—æ•´é«”å¥åº·åˆ†æ•¸"""
        if not modules:
            return 0.0
        
        scores = [module.health_score for module in modules.values()]
        return sum(scores) / len(scores)
    
    def _calculate_language_distribution(self, modules: Dict[str, ModuleAnalysis]) -> Dict[str, int]:
        """è¨ˆç®—èªè¨€åˆ†å¸ƒ"""
        distribution = defaultdict(int)
        
        for module in modules.values():
            for file_analysis in module.files:
                distribution[file_analysis.language] += file_analysis.line_count
        
        return dict(distribution)

async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="AIVA AI ç³»çµ±æ¢ç´¢å™¨ v2.0")
    parser.add_argument("--workspace", "-w", type=str, help="å·¥ä½œç›®éŒ„è·¯å¾‘")
    parser.add_argument("--force-full", "-f", action="store_true", help="å¼·åˆ¶å®Œæ•´æƒæ")
    parser.add_argument("--output", "-o", type=str, help="è¼¸å‡ºæ ¼å¼ (json, text, both)", default="both")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¢ç´¢å™¨
    workspace = args.workspace or os.getcwd()
    explorer = IncrementalSystemExplorer(workspace)
    
    print("ğŸ” AIVA å¢é‡ç³»çµ±æ¢ç´¢å™¨ v2.0")
    print(f"ğŸ“ å·¥ä½œç›®éŒ„: {workspace}")
    
    try:
        # åŸ·è¡Œæ¢ç´¢
        start_time = datetime.now()
        snapshot = await explorer.incremental_explore(force_full=args.force_full)
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # ç”Ÿæˆå ±å‘Š
        if args.output in ["json", "both"]:
            json_report_path = f"reports/ai_diagnostics/incremental_report_{snapshot.report_id}.json"
            Path(json_report_path).parent.mkdir(parents=True, exist_ok=True)
            with open(json_report_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(snapshot), f, indent=2, default=str, ensure_ascii=False)
            print(f"ğŸ“„ JSON å ±å‘Šå·²ä¿å­˜: {json_report_path}")
        
        if args.output in ["text", "both"]:
            text_report_path = f"reports/ai_diagnostics/incremental_summary_{snapshot.report_id}.txt"
            with open(text_report_path, 'w', encoding='utf-8') as f:
                f.write(f"AIVA å¢é‡ç³»çµ±æ¢ç´¢å ±å‘Š\n")
                f.write(f"{'='*50}\n")
                f.write(f"å ±å‘Š ID: {snapshot.report_id}\n")
                f.write(f"ç”Ÿæˆæ™‚é–“: {snapshot.timestamp}\n")
                f.write(f"æ•´é«”å¥åº·ç‹€æ…‹: {snapshot.overall_health:.2f}\n")
                f.write(f"ç¸½æª”æ¡ˆæ•¸: {snapshot.total_files}\n")
                f.write(f"ç¸½ç¨‹å¼ç¢¼è¡Œæ•¸: {snapshot.total_lines}\n")
                f.write(f"æƒæè€—æ™‚: {duration:.2f}ç§’\n\n")
                
                f.write("èªè¨€åˆ†å¸ƒ:\n")
                f.write("-" * 30 + "\n")
                for lang, lines in snapshot.language_distribution.items():
                    percentage = (lines / snapshot.total_lines) * 100 if snapshot.total_lines > 0 else 0
                    f.write(f"  {lang}: {lines} è¡Œ ({percentage:.1f}%)\n")
                
                f.write("\næ¨¡çµ„ç‹€æ…‹:\n")
                f.write("-" * 30 + "\n")
                for module_id, analysis in snapshot.modules.items():
                    status = "âœ… å¥åº·" if analysis.health_score > 0.8 else "âš ï¸ è­¦å‘Š" if analysis.health_score > 0.6 else "âŒ å•é¡Œ"
                    f.write(f"  {status} {analysis.name}: {analysis.health_score:.2f} ({len(analysis.files)} æª”æ¡ˆ)\n")
                    
                    if analysis.cross_language_calls:
                        f.write(f"    è·¨èªè¨€èª¿ç”¨: {len(analysis.cross_language_calls)} å€‹\n")
                    
                    if analysis.issues:
                        f.write(f"    å•é¡Œ: {', '.join(analysis.issues)}\n")
                    
                    if analysis.warnings:
                        f.write(f"    è­¦å‘Š: {', '.join(analysis.warnings)}\n")
            
            print(f"ğŸ“‹ æ–‡å­—å ±å‘Šå·²ä¿å­˜: {text_report_path}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print(f"\nğŸ“Š æ¢ç´¢å®Œæˆ!")
        print(f"æ•´é«”å¥åº·ç‹€æ…‹: {snapshot.overall_health:.2f}")
        print(f"æ¢ç´¢æ¨¡çµ„æ•¸é‡: {len(snapshot.modules)}")
        print(f"ç¸½æª”æ¡ˆæ•¸: {snapshot.total_files}")
        print(f"ç¸½ç¨‹å¼ç¢¼è¡Œæ•¸: {snapshot.total_lines}")
        print(f"æƒæè€—æ™‚: {duration:.2f}ç§’")
        
        # èªè¨€åˆ†å¸ƒ
        print(f"\nğŸŒ èªè¨€åˆ†å¸ƒ:")
        for lang, lines in sorted(snapshot.language_distribution.items(), 
                                key=lambda x: x[1], reverse=True):
            percentage = (lines / snapshot.total_lines) * 100 if snapshot.total_lines > 0 else 0
            print(f"  {lang}: {lines} è¡Œ ({percentage:.1f}%)")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ¢ç´¢")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ¢ç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())