#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIVA Docker åŸºç¤è¨­æ–½åˆ†æå·¥å…·
åˆ†æç•¶å‰ Docker åŸºç¤è¨­æ–½ä¸¦æä¾›çµ„ç¹”å»ºè­°
"""

import os
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Any

class DockerInfrastructureAnalyzer:
    def __init__(self, repo_path: str = "."):
        self.repo_path = Path(repo_path).resolve()
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "repo_path": str(self.repo_path),
            "docker_files": {},
            "architecture_analysis": {},
            "growth_prediction": {},
            "organization_recommendations": []
        }
    
    def analyze_docker_files(self) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰ Docker ç›¸é—œæ–‡ä»¶"""
        docker_files = {}
        
        # æƒææ ¹ç›®éŒ„çš„ Docker æ–‡ä»¶
        for pattern in ["Dockerfile*", "docker-compose*", "*.dockerfile"]:
            for file_path in self.repo_path.glob(pattern):
                if file_path.is_file():
                    file_info = self._get_file_info(file_path)
                    docker_files[str(file_path.relative_to(self.repo_path))] = file_info
        
        # æƒæ docker/ å­ç›®éŒ„
        docker_dir = self.repo_path / "docker"
        if docker_dir.exists():
            for file_path in docker_dir.rglob("*"):
                if file_path.is_file():
                    file_info = self._get_file_info(file_path)
                    docker_files[str(file_path.relative_to(self.repo_path))] = file_info
        
        # æƒæ k8s/ ç›®éŒ„
        k8s_dir = self.repo_path / "k8s"
        if k8s_dir.exists():
            for file_path in k8s_dir.rglob("*.yaml"):
                if file_path.is_file():
                    file_info = self._get_file_info(file_path)
                    docker_files[str(file_path.relative_to(self.repo_path))] = file_info
        
        # æƒæ helm/ ç›®éŒ„
        helm_dir = self.repo_path / "helm"
        if helm_dir.exists():
            for file_path in helm_dir.rglob("*"):
                if file_path.is_file() and not file_path.name.startswith('.'):
                    file_info = self._get_file_info(file_path)
                    docker_files[str(file_path.relative_to(self.repo_path))] = file_info
        
        self.analysis_results["docker_files"] = docker_files
        return docker_files
    
    def _get_file_info(self, file_path: Path) -> Dict[str, Any]:
        """ç²å–æ–‡ä»¶è©³ç´°ä¿¡æ¯"""
        stat = file_path.stat()
        
        # è¨ˆç®—æ–‡ä»¶å¹´é½¡
        file_age_days = (datetime.now() - datetime.fromtimestamp(stat.st_mtime)).days
        
        file_info = {
            "size": stat.st_size,
            "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
            "last_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "age_days": file_age_days,
            "directory": str(file_path.parent.relative_to(self.repo_path)),
            "extension": file_path.suffix,
            "type": self._classify_docker_file(file_path)
        }
        
        # åˆ†ææ–‡ä»¶å…§å®¹ï¼ˆå¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ä¸”ä¸å¤ªå¤§ï¼‰
        if stat.st_size < 100000:  # 100KB ä»¥ä¸‹
            try:
                content = file_path.read_text(encoding='utf-8', errors='ignore')
                file_info["content_analysis"] = self._analyze_file_content(content, file_path)
            except Exception as e:
                file_info["content_analysis"] = {"error": str(e)}
        
        return file_info
    
    def _classify_docker_file(self, file_path: Path) -> str:
        """åˆ†é¡ Docker æ–‡ä»¶é¡å‹"""
        name = file_path.name.lower()
        parent = file_path.parent.name.lower()
        
        if name.startswith("dockerfile"):
            if "core" in name:
                return "dockerfile_core"
            elif "component" in name:
                return "dockerfile_component"
            elif "patch" in name:
                return "dockerfile_patch"
            elif "minimal" in name:
                return "dockerfile_minimal"
            else:
                return "dockerfile_generic"
        elif name.startswith("docker-compose"):
            return "docker_compose"
        elif name.endswith(".yaml") or name.endswith(".yml"):
            if parent == "k8s":
                return "kubernetes_manifest"
            elif parent == "helm" or "helm" in str(file_path):
                return "helm_chart"
            else:
                return "yaml_config"
        else:
            return "docker_related"
    
    def _analyze_file_content(self, content: str, file_path: Path) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å…§å®¹"""
        analysis = {
            "lines": len(content.splitlines()),
            "size_chars": len(content)
        }
        
        # Dockerfile ç‰¹å®šåˆ†æ
        if file_path.name.startswith("Dockerfile"):
            analysis.update(self._analyze_dockerfile_content(content))
        
        # docker-compose ç‰¹å®šåˆ†æ
        elif file_path.name.startswith("docker-compose"):
            analysis.update(self._analyze_compose_content(content))
        
        # Kubernetes ç‰¹å®šåˆ†æ
        elif file_path.suffix in [".yaml", ".yml"] and "k8s" in str(file_path):
            analysis.update(self._analyze_k8s_content(content))
        
        return analysis
    
    def _analyze_dockerfile_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æ Dockerfile å…§å®¹"""
        lines = content.splitlines()
        analysis = {
            "from_image": None,
            "exposed_ports": [],
            "copy_commands": 0,
            "run_commands": 0,
            "env_vars": 0,
            "workdir": None,
            "maintainer": None,
            "labels": {}
        }
        
        for line in lines:
            line = line.strip()
            upper_line = line.upper()
            
            if upper_line.startswith("FROM "):
                analysis["from_image"] = line[5:].strip()
            elif upper_line.startswith("EXPOSE "):
                ports = line[7:].strip().split()
                analysis["exposed_ports"].extend(ports)
            elif upper_line.startswith("COPY ") or upper_line.startswith("ADD "):
                analysis["copy_commands"] += 1
            elif upper_line.startswith("RUN "):
                analysis["run_commands"] += 1
            elif upper_line.startswith("ENV "):
                analysis["env_vars"] += 1
            elif upper_line.startswith("WORKDIR "):
                analysis["workdir"] = line[8:].strip()
            elif upper_line.startswith("MAINTAINER "):
                analysis["maintainer"] = line[11:].strip()
            elif upper_line.startswith("LABEL "):
                # ç°¡å–®çš„ label è§£æ
                label_part = line[6:].strip()
                if "=" in label_part:
                    key, value = label_part.split("=", 1)
                    analysis["labels"][key.strip()] = value.strip().strip('"')
        
        return analysis
    
    def _analyze_compose_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æ docker-compose å…§å®¹"""
        analysis = {
            "version": None,
            "services_count": 0,
            "networks_count": 0,
            "volumes_count": 0,
            "services": []
        }
        
        # ç°¡å–®çš„ YAML è§£æ - è¨ˆç®—æœå‹™æ•¸é‡
        lines = content.splitlines()
        in_services = False
        service_indent = None
        
        for line in lines:
            stripped = line.strip()
            
            if stripped.startswith("version:"):
                analysis["version"] = stripped.split(":", 1)[1].strip().strip('"\'')
            elif stripped == "services:":
                in_services = True
                continue
            elif stripped in ["networks:", "volumes:", "configs:", "secrets:"]:
                in_services = False
                if stripped == "networks:":
                    analysis["networks_count"] = self._count_yaml_items(lines, lines.index(line))
                elif stripped == "volumes:":
                    analysis["volumes_count"] = self._count_yaml_items(lines, lines.index(line))
                continue
            
            if in_services and line and not line.startswith(' ') and not line.startswith('#'):
                # é€™æ˜¯ä¸€å€‹æœå‹™å®šç¾©
                if ':' in line:
                    service_name = line.split(':')[0].strip()
                    analysis["services"].append(service_name)
                    analysis["services_count"] += 1
        
        return analysis
    
    def _analyze_k8s_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æ Kubernetes manifest å…§å®¹"""
        analysis = {
            "api_version": None,
            "kind": None,
            "namespace": None,
            "name": None
        }
        
        lines = content.splitlines()
        for line in lines:
            stripped = line.strip()
            
            if stripped.startswith("apiVersion:"):
                analysis["api_version"] = stripped.split(":", 1)[1].strip()
            elif stripped.startswith("kind:"):
                analysis["kind"] = stripped.split(":", 1)[1].strip()
            elif stripped.startswith("namespace:"):
                analysis["namespace"] = stripped.split(":", 1)[1].strip()
            elif stripped.startswith("name:") and analysis["name"] is None:
                analysis["name"] = stripped.split(":", 1)[1].strip()
        
        return analysis
    
    def _count_yaml_items(self, lines: List[str], start_index: int) -> int:
        """è¨ˆç®— YAML ç¯€é»ä¸‹çš„é …ç›®æ•¸é‡"""
        count = 0
        base_indent = len(lines[start_index]) - len(lines[start_index].lstrip())
        
        for i in range(start_index + 1, len(lines)):
            line = lines[i]
            if not line.strip() or line.strip().startswith('#'):
                continue
            
            current_indent = len(line) - len(line.lstrip())
            
            if current_indent <= base_indent and line.strip():
                break
            
            if current_indent == base_indent + 2 and ':' in line:
                count += 1
        
        return count
    
    def analyze_architecture(self) -> Dict[str, Any]:
        """åˆ†æ AIVA æ¶æ§‹å’Œ Docker ä½¿ç”¨æ¨¡å¼"""
        arch_analysis = {
            "service_architecture": self._analyze_service_architecture(),
            "containerization_strategy": self._analyze_containerization_strategy(),
            "deployment_patterns": self._analyze_deployment_patterns(),
            "file_distribution": self._analyze_file_distribution()
        }
        
        self.analysis_results["architecture_analysis"] = arch_analysis
        return arch_analysis
    
    def _analyze_service_architecture(self) -> Dict[str, Any]:
        """åˆ†ææœå‹™æ¶æ§‹"""
        services_dir = self.repo_path / "services"
        
        analysis = {
            "core_services": [],
            "feature_components": [],
            "total_services": 0
        }
        
        if services_dir.exists():
            for service_dir in services_dir.iterdir():
                if service_dir.is_dir() and not service_dir.name.startswith('.'):
                    service_name = service_dir.name
                    analysis["total_services"] += 1
                    
                    if service_name in ["core", "aiva_common"]:
                        analysis["core_services"].append(service_name)
                    else:
                        analysis["feature_components"].append(service_name)
        
        return analysis
    
    def _analyze_containerization_strategy(self) -> Dict[str, Any]:
        """åˆ†æå®¹å™¨åŒ–ç­–ç•¥"""
        dockerfiles = [f for f in self.analysis_results["docker_files"].keys() 
                      if f.startswith("Dockerfile")]
        
        strategy = {
            "dockerfile_count": len(dockerfiles),
            "dockerfile_types": {},
            "multi_stage_builds": 0,
            "base_images": set(),
            "specialized_containers": []
        }
        
        for dockerfile_path in dockerfiles:
            file_info = self.analysis_results["docker_files"][dockerfile_path]
            file_type = file_info["type"]
            
            if file_type not in strategy["dockerfile_types"]:
                strategy["dockerfile_types"][file_type] = 0
            strategy["dockerfile_types"][file_type] += 1
            
            # åˆ†æå…§å®¹
            content_analysis = file_info.get("content_analysis", {})
            if "from_image" in content_analysis:
                strategy["base_images"].add(content_analysis["from_image"])
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯ç‰¹åŒ–å®¹å™¨
            if any(keyword in dockerfile_path.lower() for keyword in 
                   ["core", "component", "patch", "minimal"]):
                strategy["specialized_containers"].append(dockerfile_path)
        
        strategy["base_images"] = list(strategy["base_images"])
        return strategy
    
    def _analyze_deployment_patterns(self) -> Dict[str, Any]:
        """åˆ†æéƒ¨ç½²æ¨¡å¼"""
        patterns = {
            "has_docker_compose": False,
            "has_kubernetes": False,
            "has_helm": False,
            "deployment_environments": []
        }
        
        # æª¢æŸ¥ docker-compose
        if any(f.startswith("docker-compose") for f in self.analysis_results["docker_files"].keys()):
            patterns["has_docker_compose"] = True
            patterns["deployment_environments"].append("docker-compose")
        
        # æª¢æŸ¥ Kubernetes
        if any("k8s/" in f for f in self.analysis_results["docker_files"].keys()):
            patterns["has_kubernetes"] = True
            patterns["deployment_environments"].append("kubernetes")
        
        # æª¢æŸ¥ Helm
        if any("helm/" in f for f in self.analysis_results["docker_files"].keys()):
            patterns["has_helm"] = True
            patterns["deployment_environments"].append("helm")
        
        return patterns
    
    def _analyze_file_distribution(self) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶åˆ†ä½ˆ"""
        distribution = {
            "root_level_files": 0,
            "docker_subdir_files": 0,
            "k8s_files": 0,
            "helm_files": 0,
            "other_locations": 0
        }
        
        for file_path in self.analysis_results["docker_files"].keys():
            if "/" not in file_path:
                distribution["root_level_files"] += 1
            elif file_path.startswith("docker/"):
                distribution["docker_subdir_files"] += 1
            elif file_path.startswith("k8s/"):
                distribution["k8s_files"] += 1
            elif file_path.startswith("helm/"):
                distribution["helm_files"] += 1
            else:
                distribution["other_locations"] += 1
        
        return distribution
    
    def predict_growth(self) -> Dict[str, Any]:
        """é æ¸¬ Docker åŸºç¤è¨­æ–½å¢é•·"""
        prediction = {
            "growth_factors": [],
            "expected_dockerfile_growth": "moderate",
            "recommended_organization_threshold": 8,
            "current_complexity_score": 0,
            "projected_complexity_score": 0
        }
        
        # åˆ†æå¢é•·å› å­
        arch_analysis = self.analysis_results["architecture_analysis"]
        
        # æœå‹™æ•¸é‡å› å­
        total_services = arch_analysis["service_architecture"]["total_services"]
        if total_services > 3:
            prediction["growth_factors"].append(f"å¤šæœå‹™æ¶æ§‹ ({total_services} å€‹æœå‹™)")
        
        # ç‰¹åŒ–å®¹å™¨å› å­
        specialized_count = len(arch_analysis["containerization_strategy"]["specialized_containers"])
        if specialized_count > 2:
            prediction["growth_factors"].append(f"å¤šç¨®ç‰¹åŒ–å®¹å™¨ ({specialized_count} å€‹)")
        
        # éƒ¨ç½²ç’°å¢ƒå› å­
        env_count = len(arch_analysis["deployment_patterns"]["deployment_environments"])
        if env_count > 1:
            prediction["growth_factors"].append(f"å¤šéƒ¨ç½²ç’°å¢ƒ ({env_count} å€‹)")
        
        # è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸
        current_complexity = (
            len(self.analysis_results["docker_files"]) * 1 +
            specialized_count * 2 +
            env_count * 3 +
            total_services * 1
        )
        
        prediction["current_complexity_score"] = current_complexity
        prediction["projected_complexity_score"] = current_complexity * 1.5  # é æœŸå¢é•·50%
        
        # é æ¸¬å¢é•·ç¨‹åº¦
        if current_complexity < 10:
            prediction["expected_dockerfile_growth"] = "ä½"
        elif current_complexity < 20:
            prediction["expected_dockerfile_growth"] = "ä¸­ç­‰"
        else:
            prediction["expected_dockerfile_growth"] = "é«˜"
        
        self.analysis_results["growth_prediction"] = prediction
        return prediction
    
    def generate_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆçµ„ç¹”å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ç•¶å‰è¤‡é›œåº¦çš„å»ºè­°
        complexity_score = self.analysis_results["growth_prediction"]["current_complexity_score"]
        file_distribution = self.analysis_results["architecture_analysis"]["file_distribution"]
        
        # å»ºè­°1: æ–‡ä»¶çµ„ç¹”
        if file_distribution["root_level_files"] > 4:
            recommendations.append({
                "priority": "é«˜",
                "category": "æ–‡ä»¶çµ„ç¹”",
                "title": "æ•´ç†æ ¹ç›®éŒ„ Docker æ–‡ä»¶",
                "description": f"æ ¹ç›®éŒ„æœ‰ {file_distribution['root_level_files']} å€‹ Docker æ–‡ä»¶ï¼Œå»ºè­°ç§»è‡³å°ˆé–€ç›®éŒ„",
                "action": "å‰µå»º docker/ å­ç›®éŒ„ä¸¦ç§»å‹•æ–‡ä»¶",
                "estimated_effort": "1-2 å°æ™‚"
            })
        
        # å»ºè­°2: Dockerfile æ¨™æº–åŒ–
        dockerfile_types = self.analysis_results["architecture_analysis"]["containerization_strategy"]["dockerfile_types"]
        if len(dockerfile_types) > 3:
            recommendations.append({
                "priority": "ä¸­",
                "category": "æ¨™æº–åŒ–",
                "title": "å»ºç«‹ Dockerfile å‘½åè¦ç¯„",
                "description": f"ç™¼ç¾ {len(dockerfile_types)} ç¨®ä¸åŒé¡å‹çš„ Dockerfileï¼Œéœ€è¦çµ±ä¸€å‘½åè¦ç¯„",
                "action": "åˆ¶å®šä¸¦æ‡‰ç”¨ä¸€è‡´çš„å‘½åæ¨¡å¼",
                "estimated_effort": "2-3 å°æ™‚"
            })
        
        # å»ºè­°3: éƒ¨ç½²é…ç½®æ•´ç†
        if self.analysis_results["architecture_analysis"]["deployment_patterns"]["has_kubernetes"]:
            recommendations.append({
                "priority": "ä¸­",
                "category": "éƒ¨ç½²é…ç½®",
                "title": "Kubernetes é…ç½®ç‰ˆæœ¬ç®¡ç†",
                "description": "å»ºç«‹ K8s é…ç½®çš„ç‰ˆæœ¬ç®¡ç†å’Œç’°å¢ƒå€åˆ†",
                "action": "æŒ‰ç’°å¢ƒçµ„ç¹” K8s manifests (dev/staging/prod)",
                "estimated_effort": "3-4 å°æ™‚"
            })
        
        # å»ºè­°4: å¢é•·é å‚™
        if complexity_score > 15:
            recommendations.append({
                "priority": "ä¸­",
                "category": "æ¶æ§‹æº–å‚™",
                "title": "æº–å‚™å®¹å™¨åŒ–æ¶æ§‹æ“´å±•",
                "description": "ç³»çµ±è¤‡é›œåº¦è¼ƒé«˜ï¼Œå»ºè­°å»ºç«‹å®¹å™¨åŒ–æ²»ç†æ¡†æ¶",
                "action": "å»ºç«‹ Docker æ–‡ä»¶æ¨¡æ¿å’Œæœ€ä½³å¯¦è¸æ–‡æª”",
                "estimated_effort": "4-6 å°æ™‚"
            })
        
        # å»ºè­°5: è‡ªå‹•åŒ–
        if len(self.analysis_results["docker_files"]) > 10:
            recommendations.append({
                "priority": "ä½",
                "category": "è‡ªå‹•åŒ–",
                "title": "å»ºç«‹å®¹å™¨æ§‹å»ºè‡ªå‹•åŒ–",
                "description": "Docker æ–‡ä»¶æ•¸é‡è¼ƒå¤šï¼Œå»ºè­°è‡ªå‹•åŒ–æ§‹å»ºæµç¨‹",
                "action": "è¨­ç½® CI/CD ç®¡é“è‡ªå‹•æ§‹å»ºå’Œæ¨é€å®¹å™¨",
                "estimated_effort": "6-8 å°æ™‚"
            })
        
        # å»ºè­°6: æ–‡æª”åŒ–
        recommendations.append({
            "priority": "ä¸­",
            "category": "æ–‡æª”",
            "title": "æ›´æ–°å®¹å™¨åŒ–æ–‡æª”",
            "description": "ç¢ºä¿æ‰€æœ‰ Docker æ–‡ä»¶éƒ½æœ‰é©ç•¶çš„æ–‡æª”èªªæ˜",
            "action": "ç‚ºæ¯å€‹ Dockerfile æ·»åŠ è©³ç´°è¨»é‡‹å’Œä½¿ç”¨èªªæ˜",
            "estimated_effort": "2-3 å°æ™‚"
        })
        
        self.analysis_results["organization_recommendations"] = recommendations
        return recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Š"""
        report_lines = [
            "# AIVA Docker åŸºç¤è¨­æ–½åˆ†æå ±å‘Š",
            "",
            f"**åˆ†ææ™‚é–“**: {self.analysis_results['timestamp']}",
            f"**åˆ†æè·¯å¾‘**: {self.analysis_results['repo_path']}",
            "",
            "## ğŸ“Š ç¸½é«”æ¦‚æ³",
            "",
            f"- **Docker ç›¸é—œæ–‡ä»¶ç¸½æ•¸**: {len(self.analysis_results['docker_files'])}",
            f"- **è¤‡é›œåº¦è©•åˆ†**: {self.analysis_results['growth_prediction']['current_complexity_score']}/100",
            f"- **é æœŸå¢é•·**: {self.analysis_results['growth_prediction']['expected_dockerfile_growth']}",
            "",
            "## ğŸ“ æ–‡ä»¶åˆ†ä½ˆåˆ†æ",
            ""
        ]
        
        # æ–‡ä»¶åˆ†ä½ˆè¡¨æ ¼
        distribution = self.analysis_results["architecture_analysis"]["file_distribution"]
        report_lines.extend([
            "| ä½ç½® | æ–‡ä»¶æ•¸é‡ | èªªæ˜ |",
            "|------|----------|------|",
            f"| æ ¹ç›®éŒ„ | {distribution['root_level_files']} | ç›´æ¥åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ |",
            f"| docker/ | {distribution['docker_subdir_files']} | Docker å°ˆç”¨ç›®éŒ„ |",
            f"| k8s/ | {distribution['k8s_files']} | Kubernetes é…ç½® |",
            f"| helm/ | {distribution['helm_files']} | Helm Charts |",
            f"| å…¶ä»–ä½ç½® | {distribution['other_locations']} | å…¶ä»–å­ç›®éŒ„ |",
            "",
            "## ğŸ³ Docker æ–‡ä»¶è©³ç´°åˆ†æ",
            ""
        ])
        
        # Docker æ–‡ä»¶åˆ—è¡¨
        for file_path, file_info in self.analysis_results["docker_files"].items():
            age_status = "ğŸŸ¢ æœ€æ–°" if file_info["age_days"] < 7 else "ğŸŸ¡ ä¸€é€±å‰" if file_info["age_days"] < 14 else "ğŸ”´ å…©é€±å‰"
            report_lines.append(f"### {file_path}")
            report_lines.extend([
                f"- **é¡å‹**: {file_info['type']}",
                f"- **å¤§å°**: {file_info['size']} bytes",
                f"- **æœ€å¾Œä¿®æ”¹**: {file_info['last_modified']} ({age_status})",
                f"- **æ–‡ä»¶å¹´é½¡**: {file_info['age_days']} å¤©",
                ""
            ])
            
            # å…§å®¹åˆ†æ
            if "content_analysis" in file_info and "error" not in file_info["content_analysis"]:
                content = file_info["content_analysis"]
                if file_info["type"].startswith("dockerfile"):
                    report_lines.extend([
                        f"  - **åŸºç¤æ˜ åƒ**: {content.get('from_image', 'N/A')}",
                        f"  - **æš´éœ²ç«¯å£**: {', '.join(content.get('exposed_ports', [])) or 'None'}",
                        f"  - **è¤‡è£½å‘½ä»¤**: {content.get('copy_commands', 0)}",
                        f"  - **åŸ·è¡Œå‘½ä»¤**: {content.get('run_commands', 0)}",
                        ""
                    ])
                elif file_info["type"] == "docker_compose":
                    report_lines.extend([
                        f"  - **ç‰ˆæœ¬**: {content.get('version', 'N/A')}",
                        f"  - **æœå‹™æ•¸é‡**: {content.get('services_count', 0)}",
                        f"  - **ç¶²çµ¡æ•¸é‡**: {content.get('networks_count', 0)}",
                        f"  - **æ•¸æ“šå·æ•¸é‡**: {content.get('volumes_count', 0)}",
                        ""
                    ])
        
        # æ¶æ§‹åˆ†æ
        arch = self.analysis_results["architecture_analysis"]
        report_lines.extend([
            "## ğŸ—ï¸ æ¶æ§‹åˆ†æ",
            "",
            f"- **ç¸½æœå‹™æ•¸**: {arch['service_architecture']['total_services']}",
            f"- **æ ¸å¿ƒæœå‹™**: {', '.join(arch['service_architecture']['core_services'])}",
            f"- **åŠŸèƒ½çµ„ä»¶**: {', '.join(arch['service_architecture']['feature_components'])}",
            "",
            f"- **Dockerfile æ•¸é‡**: {arch['containerization_strategy']['dockerfile_count']}",
            f"- **åŸºç¤æ˜ åƒ**: {', '.join(arch['containerization_strategy']['base_images'])}",
            f"- **ç‰¹åŒ–å®¹å™¨**: {len(arch['containerization_strategy']['specialized_containers'])}",
            "",
            f"- **éƒ¨ç½²ç’°å¢ƒ**: {', '.join(arch['deployment_patterns']['deployment_environments'])}",
            ""
        ])
        
        # å¢é•·é æ¸¬
        growth = self.analysis_results["growth_prediction"]
        report_lines.extend([
            "## ğŸ“ˆ å¢é•·é æ¸¬",
            "",
            f"- **ç•¶å‰è¤‡é›œåº¦**: {growth['current_complexity_score']}/100",
            f"- **é æœŸè¤‡é›œåº¦**: {growth['projected_complexity_score']}/100",
            f"- **é æœŸå¢é•·ç¨‹åº¦**: {growth['expected_dockerfile_growth']}",
            "",
            "**å¢é•·å› å­**:",
        ])
        
        for factor in growth["growth_factors"]:
            report_lines.append(f"- {factor}")
        
        # çµ„ç¹”å»ºè­°
        report_lines.extend([
            "",
            "## ğŸ’¡ çµ„ç¹”å»ºè­°",
            ""
        ])
        
        for i, rec in enumerate(self.analysis_results["organization_recommendations"], 1):
            priority_emoji = {"é«˜": "ğŸ”´", "ä¸­": "ğŸŸ¡", "ä½": "ğŸŸ¢"}
            report_lines.extend([
                f"### {i}. {rec['title']} {priority_emoji.get(rec['priority'], 'âšª')}",
                "",
                f"**å„ªå…ˆç´š**: {rec['priority']}",
                f"**é¡åˆ¥**: {rec['category']}",
                f"**æè¿°**: {rec['description']}",
                f"**å»ºè­°è¡Œå‹•**: {rec['action']}",
                f"**é ä¼°å·¥ä½œé‡**: {rec['estimated_effort']}",
                ""
            ])
        
        # çµè«–å’Œä¸‹ä¸€æ­¥
        report_lines.extend([
            "## ğŸ¯ çµè«–å’Œä¸‹ä¸€æ­¥",
            "",
            "åŸºæ–¼åˆ†æçµæœï¼Œå»ºè­°æŒ‰ä»¥ä¸‹é †åºé€²è¡Œçµ„ç¹”æ”¹é€²ï¼š",
            "",
            "1. **ç«‹å³åŸ·è¡Œ** (é«˜å„ªå…ˆç´šå»ºè­°)",
            "2. **çŸ­æœŸè¦åŠƒ** (ä¸­å„ªå…ˆç´šå»ºè­°)",
            "3. **é•·æœŸç¶­è­·** (ä½å„ªå…ˆç´šå»ºè­°)",
            "",
            "é€™æ¨£çš„çµ„ç¹”æ–¹å¼å°‡æœ‰åŠ©æ–¼ï¼š",
            "- æé«˜ Docker æ–‡ä»¶çš„å¯ç¶­è­·æ€§",
            "- æ¸›å°‘éƒ¨ç½²é…ç½®çš„è¤‡é›œåº¦",
            "- ç‚ºæœªä¾†çš„æ¶æ§‹æ“´å±•åšå¥½æº–å‚™",
            "- å»ºç«‹æ¨™æº–åŒ–çš„å®¹å™¨åŒ–æµç¨‹",
            "",
            f"---",
            f"*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}*"
        ])
        
        return "\n".join(report_lines)
    
    def save_report(self, output_file: str = None) -> str:
        """ä¿å­˜åˆ†æå ±å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"docker_infrastructure_analysis_{timestamp}.md"
        
        output_path = self.repo_path / output_file
        
        # ç”Ÿæˆå ±å‘Šå…§å®¹
        report_content = self.generate_report()
        
        # æ·»åŠ æ™‚é–“æˆ³æ¨™é ­
        timestamped_content = f"""---
Created: {datetime.now().strftime("%Y-%m-%d")}
Last Modified: {datetime.now().strftime("%Y-%m-%d")}
Document Type: Analysis Report
Analysis Target: Docker Infrastructure
---

{report_content}
"""
        
        # ä¿å­˜å ±å‘Š
        output_path.write_text(timestamped_content, encoding='utf-8')
        
        # ä¿å­˜ JSON æ•¸æ“š
        json_file = output_path.with_suffix('.json')
        json_file.write_text(json.dumps(self.analysis_results, indent=2, ensure_ascii=False), 
                           encoding='utf-8')
        
        return str(output_path)
    
    def run_full_analysis(self) -> str:
        """åŸ·è¡Œå®Œæ•´åˆ†æä¸¦ç”Ÿæˆå ±å‘Š"""
        print("ğŸ” é–‹å§‹ Docker åŸºç¤è¨­æ–½åˆ†æ...")
        
        # 1. åˆ†æ Docker æ–‡ä»¶
        print("ğŸ“ åˆ†æ Docker ç›¸é—œæ–‡ä»¶...")
        self.analyze_docker_files()
        print(f"   ç™¼ç¾ {len(self.analysis_results['docker_files'])} å€‹æ–‡ä»¶")
        
        # 2. åˆ†ææ¶æ§‹
        print("ğŸ—ï¸ åˆ†ææœå‹™æ¶æ§‹...")
        self.analyze_architecture()
        
        # 3. é æ¸¬å¢é•·
        print("ğŸ“ˆ åˆ†æå¢é•·è¶¨å‹¢...")
        self.predict_growth()
        
        # 4. ç”Ÿæˆå»ºè­°
        print("ğŸ’¡ ç”Ÿæˆçµ„ç¹”å»ºè­°...")
        self.generate_recommendations()
        print(f"   ç”Ÿæˆ {len(self.analysis_results['organization_recommendations'])} æ¢å»ºè­°")
        
        # 5. ä¿å­˜å ±å‘Š
        print("ğŸ“„ ç”Ÿæˆåˆ†æå ±å‘Š...")
        report_path = self.save_report()
        
        print(f"âœ… åˆ†æå®Œæˆï¼å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return report_path

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AIVA Docker åŸºç¤è¨­æ–½åˆ†æå·¥å…·")
    parser.add_argument("--repo-path", default=".", help="å°ˆæ¡ˆè·¯å¾‘ (é è¨­: ç•¶å‰ç›®éŒ„)")
    parser.add_argument("--output", help="è¼¸å‡ºå ±å‘Šæ–‡ä»¶å")
    parser.add_argument("--json-only", action="store_true", help="åªè¼¸å‡º JSON æ ¼å¼")
    
    args = parser.parse_args()
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = DockerInfrastructureAnalyzer(args.repo_path)
    
    if args.json_only:
        # åªåŸ·è¡Œåˆ†æï¼Œè¼¸å‡º JSON
        analyzer.analyze_docker_files()
        analyzer.analyze_architecture()
        analyzer.predict_growth()
        analyzer.generate_recommendations()
        
        print(json.dumps(analyzer.analysis_results, indent=2, ensure_ascii=False))
    else:
        # åŸ·è¡Œå®Œæ•´åˆ†æä¸¦ç”Ÿæˆå ±å‘Š
        report_path = analyzer.run_full_analysis()
        print(f"\nğŸ“Š åˆ†ææ‘˜è¦:")
        print(f"   - Docker æ–‡ä»¶æ•¸é‡: {len(analyzer.analysis_results['docker_files'])}")
        print(f"   - è¤‡é›œåº¦è©•åˆ†: {analyzer.analysis_results['growth_prediction']['current_complexity_score']}")
        print(f"   - å»ºè­°æ•¸é‡: {len(analyzer.analysis_results['organization_recommendations'])}")
        print(f"   - å ±å‘Šä½ç½®: {report_path}")

if __name__ == "__main__":
    main()