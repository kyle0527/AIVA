"""
æ•´åˆæ¨¡çµ„å‚™ä»½è…³æœ¬

å®šæœŸå‚™ä»½æ”»æ“Šè·¯å¾‘åœ–å’Œç¶“é©—è³‡æ–™åº«
"""

import argparse
import logging
import shutil
from datetime import datetime
from pathlib import Path

from services.integration.aiva_integration.config import (
    ATTACK_GRAPH_FILE,
    BACKUP_DIR,
    BACKUP_RETENTION_DAYS,
    EXPERIENCE_DB_FILE,
    MODEL_CHECKPOINT_DIR,
    TRAINING_DATASET_DIR,
)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def backup_file(source: Path, backup_dir: Path, prefix: str) -> Path | None:
    """å‚™ä»½å–®å€‹æª”æ¡ˆ

    Args:
        source: ä¾†æºæª”æ¡ˆè·¯å¾‘
        backup_dir: å‚™ä»½ç›®éŒ„
        prefix: å‚™ä»½æª”æ¡ˆå‰ç¶´

    Returns:
        å‚™ä»½æª”æ¡ˆè·¯å¾‘,å¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    if not source.exists():
        logger.warning(f"Source file not found: {source}")
        return None

    # å»ºç«‹å‚™ä»½ç›®éŒ„
    backup_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆå‚™ä»½æª”å (åŒ…å«æ™‚é–“æˆ³)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"{prefix}_{timestamp}{source.suffix}"
    backup_path = backup_dir / backup_filename

    try:
        # è¤‡è£½æª”æ¡ˆ
        shutil.copy2(source, backup_path)
        logger.info(f"âœ… Backed up: {source.name} -> {backup_path.name}")
        return backup_path
    except Exception as e:
        logger.error(f"âŒ Failed to backup {source}: {e}")
        return None


def backup_directory(source: Path, backup_dir: Path, prefix: str) -> Path | None:
    """å‚™ä»½æ•´å€‹ç›®éŒ„

    Args:
        source: ä¾†æºç›®éŒ„è·¯å¾‘
        backup_dir: å‚™ä»½ç›®éŒ„
        prefix: å‚™ä»½ç›®éŒ„å‰ç¶´

    Returns:
        å‚™ä»½ç›®éŒ„è·¯å¾‘,å¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    if not source.exists() or not source.is_dir():
        logger.warning(f"Source directory not found: {source}")
        return None

    # å»ºç«‹å‚™ä»½ç›®éŒ„
    backup_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆå‚™ä»½ç›®éŒ„å (åŒ…å«æ™‚é–“æˆ³)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dirname = f"{prefix}_{timestamp}"
    backup_path = backup_dir / backup_dirname

    try:
        # è¤‡è£½ç›®éŒ„
        shutil.copytree(source, backup_path)
        logger.info(f"âœ… Backed up directory: {source.name} -> {backup_path.name}")
        return backup_path
    except Exception as e:
        logger.error(f"âŒ Failed to backup directory {source}: {e}")
        return None


def cleanup_old_backups(backup_dir: Path, retention_days: int):
    """æ¸…ç†èˆŠå‚™ä»½

    Args:
        backup_dir: å‚™ä»½ç›®éŒ„
        retention_days: ä¿ç•™å¤©æ•¸
    """
    if not backup_dir.exists():
        return

    now = datetime.now()
    deleted_count = 0

    for backup_item in backup_dir.iterdir():
        # è¨ˆç®—æª”æ¡ˆå¹´é½¡
        file_age_days = (now - datetime.fromtimestamp(backup_item.stat().st_mtime)).days

        if file_age_days > retention_days:
            try:
                if backup_item.is_file():
                    backup_item.unlink()
                elif backup_item.is_dir():
                    shutil.rmtree(backup_item)
                logger.info(f"ğŸ—‘ï¸  Deleted old backup: {backup_item.name} (age: {file_age_days} days)")
                deleted_count += 1
            except Exception as e:
                logger.error(f"âŒ Failed to delete {backup_item}: {e}")

    if deleted_count > 0:
        logger.info(f"âœ… Cleaned up {deleted_count} old backup(s)")


def main():
    parser = argparse.ArgumentParser(description="æ•´åˆæ¨¡çµ„å‚™ä»½è…³æœ¬")
    parser.add_argument(
        "--no-cleanup",
        action="store_true",
        help="ä¸æ¸…ç†èˆŠå‚™ä»½",
    )
    parser.add_argument(
        "--attack-graph-only",
        action="store_true",
        help="åƒ…å‚™ä»½æ”»æ“Šè·¯å¾‘åœ–",
    )
    parser.add_argument(
        "--experience-only",
        action="store_true",
        help="åƒ…å‚™ä»½ç¶“é©—è³‡æ–™åº«",
    )
    args = parser.parse_args()

    logger.info("=== é–‹å§‹å‚™ä»½æ•´åˆæ¨¡çµ„è³‡æ–™ ===\n")

    # æ”»æ“Šè·¯å¾‘åœ–å‚™ä»½
    if not args.experience_only:
        attack_graph_backup_dir = BACKUP_DIR / "attack_paths"
        backup_file(ATTACK_GRAPH_FILE, attack_graph_backup_dir, "attack_graph")

        if not args.no_cleanup:
            cleanup_old_backups(
                attack_graph_backup_dir, BACKUP_RETENTION_DAYS["attack_graph"]
            )

    # ç¶“é©—è³‡æ–™åº«å‚™ä»½
    if not args.attack_graph_only:
        experience_backup_dir = BACKUP_DIR / "experiences"
        backup_file(EXPERIENCE_DB_FILE, experience_backup_dir, "experience")

        if not args.no_cleanup:
            cleanup_old_backups(
                experience_backup_dir, BACKUP_RETENTION_DAYS["experience_db"]
            )

    # è¨“ç·´è³‡æ–™é›†å‚™ä»½ (å¯é¸)
    if not args.attack_graph_only and not args.experience_only:
        dataset_backup_dir = BACKUP_DIR / "training_datasets"
        backup_directory(TRAINING_DATASET_DIR, dataset_backup_dir, "datasets")

        if not args.no_cleanup:
            cleanup_old_backups(
                dataset_backup_dir, BACKUP_RETENTION_DAYS["training_dataset"]
            )

    # æ¨¡å‹æª¢æŸ¥é»å‚™ä»½ (å¯é¸)
    if not args.attack_graph_only and not args.experience_only:
        model_backup_dir = BACKUP_DIR / "models"
        backup_directory(MODEL_CHECKPOINT_DIR, model_backup_dir, "models")

        if not args.no_cleanup:
            cleanup_old_backups(model_backup_dir, BACKUP_RETENTION_DAYS["model"])

    logger.info("\n=== å‚™ä»½å®Œæˆ ===")


if __name__ == "__main__":
    main()
