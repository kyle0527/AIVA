"""
æ•´åˆæ¨¡çµ„æ¸…ç†è…³æœ¬

æ¸…ç†èˆŠè³‡æ–™å’Œå‚™ä»½
"""

import argparse
import logging
from datetime import datetime, timedelta
from pathlib import Path

from services.integration.aiva_integration.config import (
    ATTACK_PATHS_EXPORT_DIR,
    BACKUP_DIR,
    EXPERIENCES_EXPORT_DIR,
    LOG_DIR,
)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def cleanup_directory(directory: Path, days: int, pattern: str = "*"):
    """æ¸…ç†ç›®éŒ„ä¸­çš„èˆŠæª”æ¡ˆ

    Args:
        directory: ç›®éŒ„è·¯å¾‘
        days: ä¿ç•™å¤©æ•¸
        pattern: æª”æ¡ˆæ¨¡å¼ (glob)
    """
    if not directory.exists():
        logger.warning(f"Directory not found: {directory}")
        return

    cutoff_date = datetime.now() - timedelta(days=days)
    deleted_count = 0
    total_size = 0

    for file_path in directory.glob(pattern):
        if file_path.is_file():
            file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
            if file_mtime < cutoff_date:
                file_size = file_path.stat().st_size
                try:
                    file_path.unlink()
                    deleted_count += 1
                    total_size += file_size
                    logger.info(
                        f"ğŸ—‘ï¸  Deleted: {file_path.name} (age: {(datetime.now() - file_mtime).days} days)"
                    )
                except Exception as e:
                    logger.error(f"âŒ Failed to delete {file_path}: {e}")

    if deleted_count > 0:
        logger.info(
            f"âœ… Cleaned up {deleted_count} file(s), freed {total_size / 1024 / 1024:.2f} MB"
        )
    else:
        logger.info(f"âœ… No files to clean in {directory.name}")


def main():
    parser = argparse.ArgumentParser(description="æ•´åˆæ¨¡çµ„æ¸…ç†è…³æœ¬")
    parser.add_argument(
        "--days",
        type=int,
        default=30,
        help="æ¸…ç†å¤šå°‘å¤©å‰çš„æª”æ¡ˆ (é è¨­: 30)",
    )
    parser.add_argument(
        "--backup-only",
        action="store_true",
        help="åƒ…æ¸…ç†å‚™ä»½æª”æ¡ˆ",
    )
    parser.add_argument(
        "--logs-only",
        action="store_true",
        help="åƒ…æ¸…ç†æ—¥èªŒæª”æ¡ˆ",
    )
    parser.add_argument(
        "--exports-only",
        action="store_true",
        help="åƒ…æ¸…ç†åŒ¯å‡ºæª”æ¡ˆ",
    )
    args = parser.parse_args()

    logger.info(f"=== é–‹å§‹æ¸…ç†æ•´åˆæ¨¡çµ„è³‡æ–™ (ä¿ç•™ {args.days} å¤©) ===\n")

    # æ¸…ç†å‚™ä»½
    if not args.logs_only and not args.exports_only:
        logger.info("æ¸…ç†å‚™ä»½æª”æ¡ˆ...")
        for backup_subdir in BACKUP_DIR.glob("*"):
            if backup_subdir.is_dir():
                cleanup_directory(backup_subdir, args.days)

    # æ¸…ç†åŒ¯å‡ºæª”æ¡ˆ
    if not args.backup_only and not args.logs_only:
        logger.info("\næ¸…ç†åŒ¯å‡ºæª”æ¡ˆ...")
        cleanup_directory(ATTACK_PATHS_EXPORT_DIR, args.days, "*.html")
        cleanup_directory(ATTACK_PATHS_EXPORT_DIR, args.days, "*.mmd")
        cleanup_directory(EXPERIENCES_EXPORT_DIR, args.days, "*.jsonl")
        cleanup_directory(EXPERIENCES_EXPORT_DIR, args.days, "*.csv")

    # æ¸…ç†æ—¥èªŒæª”æ¡ˆ
    if not args.backup_only and not args.exports_only:
        logger.info("\næ¸…ç†æ—¥èªŒæª”æ¡ˆ...")
        cleanup_directory(LOG_DIR, args.days, "*.log")

    logger.info("\n=== æ¸…ç†å®Œæˆ ===")


if __name__ == "__main__":
    main()
