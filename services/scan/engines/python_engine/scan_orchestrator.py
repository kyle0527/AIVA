"""
æƒæç·¨æ’å™¨ - çµ±ä¸€ç®¡ç†æƒææµç¨‹çš„æ ¸å¿ƒé‚è¼¯
"""



from typing import TYPE_CHECKING, Any

from bs4 import BeautifulSoup

from services.aiva_common.schemas import (
    Asset,
    ScanCompletedPayload,
    ScanStartPayload,
    Summary,
)
from services.aiva_common.utils import get_logger, new_id

from .authentication_manager import AuthenticationManager
from .core_crawling_engine.http_client_hi import HiHttpClient
from .core_crawling_engine.static_content_parser import StaticContentParser
from .core_crawling_engine.url_queue_manager import UrlQueueManager
from .dynamic_engine.dynamic_content_extractor import (
    DynamicContentExtractor,
    ExtractionConfig,
)
from .dynamic_engine.headless_browser_pool import HeadlessBrowserPool, PoolConfig
from .fingerprint_manager import FingerprintCollector
from .header_configuration import HeaderConfiguration
from .info_gatherer.javascript_source_analyzer import JavaScriptSourceAnalyzer
from .info_gatherer.sensitive_info_detector import SensitiveInfoDetector
from .scan_context import ScanContext
from .strategy_controller import StrategyController, StrategyParameters

logger = get_logger(__name__)


class ScanOrchestrator:
    """
    çµ±ä¸€çš„æƒæç·¨æ’å™¨ - å”èª¿æ‰€æœ‰æƒæçµ„ä»¶

    æ”¹é€²é»:
    - æ•´åˆå‹•æ…‹å’Œéœæ…‹çˆ¬èŸ²å¼•æ“
    - æ ¹æ“šç­–ç•¥è‡ªå‹•é¸æ“‡åˆé©çš„å¼•æ“
    - æ•´åˆæ•æ„Ÿä¿¡æ¯æª¢æ¸¬å’Œ JavaScript åˆ†æ
    - æ‡‰ç”¨é…ç½®ä¸­å¿ƒå’Œç­–ç•¥æ§åˆ¶å™¨çš„è¨­å®š
    - ä½¿ç”¨å¢å¼·çš„ HTTP å®¢æˆ¶ç«¯å’Œ URL éšŠåˆ—ç®¡ç†

    ç‰¹æ€§:
    - æ”¯æ´å¤šç¨®æƒæç­–ç•¥ï¼ˆFAST/DEEP/AGGRESSIVE ç­‰ï¼‰
    - å‹•æ…‹æ¸²æŸ“é é¢è™•ç†ï¼ˆJavaScript åŸ·è¡Œï¼‰
    - æ™ºèƒ½é€Ÿç‡é™åˆ¶å’Œé‡è©¦
    - å¯¦æ™‚è³‡è¨Šæ”¶é›†å’Œåˆ†æ
    - è©³ç´°çš„é€²åº¦è¿½è¹¤
    """

    def __init__(self):
        """åˆå§‹åŒ–æƒæç·¨æ’å™¨"""
        self.static_parser = StaticContentParser()
        self.fingerprint_collector = FingerprintCollector()
        self.sensitive_detector = SensitiveInfoDetector()
        self.js_analyzer = JavaScriptSourceAnalyzer()

        # å‹•æ…‹å¼•æ“çµ„ä»¶ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
        self.browser_pool: HeadlessBrowserPool | None = None
        self.dynamic_extractor: DynamicContentExtractor | None = None

        logger.info("ScanOrchestrator initialized")

    async def execute_scan(self, request: ScanStartPayload) -> ScanCompletedPayload:
        """
        åŸ·è¡Œå®Œæ•´çš„æƒææµç¨‹

        Args:
            request: æƒæè«‹æ±‚æ•¸æ“š

        Returns:
            æƒæå®Œæˆçµæœ
        """
        logger.info(f"Starting scan for request: {request.scan_id}")

        # å‰µå»ºæƒæä¸Šä¸‹æ–‡
        context = ScanContext(request)

        # åˆå§‹åŒ–ç­–ç•¥
        strategy_controller = StrategyController(request.strategy)
        
        # ğŸ”§ å¦‚æœæœ‰ Phase0 çµæœï¼Œæ ¹æ“šå…¶å‹•æ…‹èª¿æ•´ç­–ç•¥
        if hasattr(request, 'phase0_summary') and request.phase0_summary:
            logger.info("Phase0 results detected, adjusting strategy...")
            strategy_controller.adjust_from_phase0(request.phase0_summary)
        
        strategy_params = strategy_controller.get_parameters()

        logger.info(
            f"Scan strategy: {request.strategy}, "
            f"dynamic_scan: {strategy_params.enable_dynamic_scan}, "
            f"max_pages: {strategy_params.max_pages}"
        )

        # åˆå§‹åŒ–çµ„ä»¶
        auth_manager = AuthenticationManager(request.authentication)
        header_config = HeaderConfiguration(request.custom_headers)

        # å‰µå»ºå¢å¼·çš„ HTTP å®¢æˆ¶ç«¯
        http_client = HiHttpClient(
            auth_manager,
            header_config,
            requests_per_second=strategy_params.requests_per_second,
            per_host_rps=strategy_params.requests_per_second / 2,
            timeout=strategy_params.request_timeout,
            pool_size=strategy_params.connection_pool_size,
        )

        # å‰µå»º URL éšŠåˆ—ç®¡ç†å™¨
        url_queue = UrlQueueManager(
            [str(t) for t in request.targets], max_depth=strategy_params.max_depth
        )

        # å¦‚æœç­–ç•¥å•Ÿç”¨å‹•æ…‹æƒæ,åˆå§‹åŒ–ç€è¦½å™¨æ± 
        if strategy_params.enable_dynamic_scan:
            await self._init_dynamic_engine(strategy_params)

        try:
            # åŸ·è¡Œçˆ¬èŸ²æƒæ
            await self._perform_crawling(
                context,
                url_queue,
                http_client,
                strategy_params,
            )

            # è¨­ç½®æœ€çµ‚æŒ‡ç´‹
            fingerprints = self.fingerprint_collector.get_final_fingerprints()
            if fingerprints:
                context.set_fingerprints(fingerprints)

            # æ§‹å»ºä¸¦è¿”å›çµæœ
            result = self._build_scan_result(context)

            logger.info(
                f"Scan completed for {request.scan_id}: "
                f"{context.urls_found} URLs, {context.forms_found} forms, "
                f"duration: {context.scan_duration}s"
            )

            return result

        finally:
            # æ¸…ç†è³‡æº
            await http_client.close()
            if self.browser_pool:
                await self.browser_pool.shutdown()

    async def _init_dynamic_engine(self, strategy_params: StrategyParameters) -> None:
        """
        åˆå§‹åŒ–å‹•æ…‹æƒæå¼•æ“

        Args:
            strategy_params: ç­–ç•¥åƒæ•¸
        """
        logger.info("Initializing dynamic scan engine...")

        # é…ç½®ç€è¦½å™¨æ± 
        pool_config = PoolConfig(
            min_instances=1,
            max_instances=strategy_params.browser_pool_size,
            headless=True,
            timeout_ms=int(strategy_params.page_load_timeout * 1000),
        )

        self.browser_pool = HeadlessBrowserPool(pool_config)
        await self.browser_pool.initialize()

        # é…ç½®å…§å®¹æå–å™¨
        extraction_config = ExtractionConfig(
            extract_forms=True,
            extract_links=True,
            extract_ajax=True,
            extract_api_calls=True,
            wait_for_network_idle=True,
        )

        self.dynamic_extractor = DynamicContentExtractor(extraction_config)

        logger.info("Dynamic scan engine initialized successfully")

    async def _perform_crawling(
        self,
        context: ScanContext,
        url_queue: UrlQueueManager,
        http_client: HiHttpClient,
        strategy_params: Any,
    ) -> None:
        """
        åŸ·è¡Œçˆ¬èŸ²æƒæéç¨‹

        Args:
            context: æƒæä¸Šä¸‹æ–‡
            url_queue: URL éšŠåˆ—ç®¡ç†å™¨
            http_client: HTTP å®¢æˆ¶ç«¯
            strategy_params: ç­–ç•¥åƒæ•¸
        """
        pages_processed = 0
        max_pages = strategy_params.max_pages

        while url_queue.has_next() and pages_processed < max_pages:
            url, current_depth = url_queue.next()

            logger.debug(f"Processing URL: {url} (depth={current_depth})")

            # æ ¹æ“šç­–ç•¥é¸æ“‡çˆ¬èŸ²å¼•æ“
            if strategy_params.enable_dynamic_scan and self.browser_pool:
                await self._process_url_dynamic(
                    url, current_depth, context, url_queue, strategy_params
                )
            else:
                await self._process_url_static(
                    url, current_depth, context, url_queue, http_client, strategy_params
                )

            pages_processed += 1
            context.increment_pages_crawled()

            # å®šæœŸå ±å‘Šé€²åº¦
            if pages_processed % 10 == 0:
                stats = context.get_statistics()
                logger.info(f"Progress: {pages_processed} pages, {stats}")

    async def _process_url_static(
        self,
        url: str,
        current_depth: int,
        context: ScanContext,
        url_queue: UrlQueueManager,
        http_client: HiHttpClient,
        _strategy_params: Any,
    ) -> None:
        """
        ä½¿ç”¨éœæ…‹å¼•æ“è™•ç† URL

        Args:
            url: è¦è™•ç†çš„ URL
            current_depth: ç•¶å‰ URL çš„æ·±åº¦ç´šåˆ¥
            context: æƒæä¸Šä¸‹æ–‡
            url_queue: URL ä½‡åˆ—
            http_client: HTTP å®¢æˆ¶ç«¯
            _strategy_params: ç­–ç•¥åƒæ•¸ (æœªä½¿ç”¨)
        """
        response = await http_client.get(url)

        if response is None:
            context.add_error("http_error", f"Failed to fetch {url}", url)
            return

        # æ›´æ–°çµ±è¨ˆ
        context.increment_urls_found()

        # å‰µå»ºè³‡ç”¢
        asset = Asset(asset_id=new_id("asset"), type="URL", value=url, has_form=False)
        context.add_asset(asset)

        # æ”¶é›†æŒ‡ç´‹ä¿¡æ¯
        await self.fingerprint_collector.process_response(response)

        # éœæ…‹å…§å®¹è§£æ
        parsed_assets, forms_count = self.static_parser.extract(url, response)
        for parsed_asset in parsed_assets:
            context.add_asset(parsed_asset)
        context.add_forms_found(forms_count)
        
        # ğŸ”§ ä¿®å¾©: å°‡ç™¼ç¾çš„ URL åŠ å…¥çˆ¬èŸ²éšŠåˆ— (åƒè€ƒ Crawlee enqueue_links æ¨¡å¼)
        new_urls = [
            parsed_asset.value 
            for parsed_asset in parsed_assets 
            if parsed_asset.type == "URL" and not url_queue.is_processed(parsed_asset.value)
        ]
        
        if new_urls:
            added_count = url_queue.add_batch(new_urls, parent_url=url, depth=current_depth + 1)
            logger.debug(f"Added {added_count} new URLs from {url} at depth {current_depth + 1}")

        # æ•æ„Ÿä¿¡æ¯æª¢æ¸¬
        detection_result = self.sensitive_detector.detect_in_html(response.text, url)
        if detection_result.matches:
            logger.warning(
                f"Found {len(detection_result.matches)} sensitive info items in {url}"
            )

        # ğŸ”§ JavaScript æºç¢¼åˆ†æ - åªåˆ†æå…§è¯ script
        if response.headers.get("content-type", "").startswith("text/html"):
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–å…§è¯ script å…§å®¹
            inline_scripts = []
            for script_tag in soup.find_all('script'):
                if script_tag.string and len(script_tag.string.strip()) > 50:
                    inline_scripts.append(script_tag.string)
            
            # åˆ†ææ‰€æœ‰å…§è¯ scripts
            if inline_scripts:
                combined_js = '\n'.join(inline_scripts)
                analysis_result = self.js_analyzer.analyze(combined_js, url)
                if analysis_result.sinks or analysis_result.patterns:
                    total_findings = len(analysis_result.sinks) + len(
                        analysis_result.patterns
                    )
                    logger.info(f"Found {total_findings} JS findings in {url}")

    async def _process_url_dynamic(
        self,
        url: str,
        current_depth: int,
        context: ScanContext,
        url_queue: UrlQueueManager,
        _strategy_params: Any,
    ) -> None:
        """
        ä½¿ç”¨å‹•æ…‹å¼•æ“è™•ç† URL

        Args:
            url: è¦è™•ç†çš„ URL
            current_depth: ç•¶å‰ URL çš„æ·±åº¦ç´šåˆ¥
            context: æƒæä¸Šä¸‹æ–‡
            url_queue: URL ä½‡åˆ—
            _strategy_params: ç­–ç•¥åƒæ•¸ (æœªä½¿ç”¨)
        """
        if not self.browser_pool or not self.dynamic_extractor:
            logger.warning("Dynamic engine not initialized, falling back to static")
            return

        try:
            # å¾ç€è¦½å™¨æ± ç²å–é é¢
            async with self.browser_pool.get_page() as page:
                # æå–å‹•æ…‹å…§å®¹
                dynamic_contents = await self.dynamic_extractor.extract_from_url(
                    url, page=page
                )

                logger.info(
                    f"Extracted {len(dynamic_contents)} dynamic contents from {url}"
                )

                # ğŸ”§ è™•ç†æå–çš„å…§å®¹ - æ”¶é›† URL
                new_urls = []
                for content in dynamic_contents:
                    # å‰µå»ºè³‡ç”¢
                    asset = Asset(
                        asset_id=new_id("asset"),
                        type=content.content_type.value,
                        value=content.url,
                        has_form=content.content_type.value == "form",
                    )
                    context.add_asset(asset)

                    # ğŸ”§ æ”¶é›† URL ä»¥ä¾¿æ‰¹æ¬¡åŠ å…¥éšŠåˆ—
                    if content.content_type.value == "link":
                        new_urls.append(content.url)
                
                # ğŸ”§ æ‰¹æ¬¡åŠ å…¥éšŠåˆ—ä¸¦æ­£ç¢ºè¨­å®šæ·±åº¦
                if new_urls:
                    filtered_urls = [
                        u for u in new_urls 
                        if not url_queue.is_processed(u)
                    ]
                    if filtered_urls:
                        added_count = url_queue.add_batch(
                            filtered_urls, 
                            parent_url=url, 
                            depth=current_depth + 1
                        )
                        logger.debug(
                            f"Added {added_count} dynamic URLs from {url} at depth {current_depth + 1}"
                        )

                # æ›´æ–°çµ±è¨ˆ
                context.increment_urls_found()
                forms_found = sum(
                    1 for c in dynamic_contents if c.content_type.value == "form"
                )
                context.add_forms_found(forms_found)

                # ğŸ”§ å‹•æ…‹é é¢ä¹Ÿé€²è¡Œ JS åˆ†æ
                rendered_html = await page.content()
                
                # æå–ä¸¦åˆ†æ JavaScript
                scripts = await self._extract_and_analyze_scripts(page, url, rendered_html)
                if scripts:
                    logger.info(f"Analyzed {len(scripts)} JavaScript sources from {url}")

        except Exception as e:
            logger.error(f"Dynamic processing failed for {url}: {e}")
            context.add_error("dynamic_error", str(e), url)

    async def _extract_and_analyze_scripts(
        self, page: Any, url: str, html: str
    ) -> list[dict[str, Any]]:
        """
        å¾é é¢æå–ä¸¦åˆ†æ JavaScript

        Args:
            page: Playwright Page ç‰©ä»¶
            url: é é¢ URL
            html: æ¸²æŸ“å¾Œçš„ HTML

        Returns:
            JS åˆ†æçµæœåˆ—è¡¨
        """
        scripts = []
        
        try:
            # ğŸ”§ æå–å…§è¯ script
            soup = BeautifulSoup(html, 'lxml')
            for script_tag in soup.find_all('script'):
                if script_tag.string and len(script_tag.string.strip()) > 50:
                    # åˆ†æå…§è¯ JS
                    analysis = self.js_analyzer.analyze(script_tag.string, url)
                    if analysis.sinks or analysis.patterns:
                        scripts.append({
                            'type': 'inline',
                            'content': script_tag.string[:200],
                            'sinks': len(analysis.sinks),
                            'patterns': len(analysis.patterns),
                        })
                        logger.info(
                            f"Inline script: {len(analysis.sinks)} sinks, "
                            f"{len(analysis.patterns)} patterns"
                        )
            
            # ğŸ”§ ç²å–å¤–éƒ¨ script URLs
            script_urls = await page.evaluate("""
                () => {
                    const scripts = Array.from(document.querySelectorAll('script[src]'));
                    return scripts.map(s => s.src).filter(src => src && src.trim());
                }
            """)
            
            # ğŸ”§ ä¸‹è¼‰ä¸¦åˆ†æå¤–éƒ¨ JS (åªåˆ†æå‰ 5 å€‹ä»¥é¿å…éæ…¢)
            for script_url in script_urls[:5]:
                try:
                    response = await page.context.request.get(script_url, timeout=5000)
                    if response.ok:
                        js_content = await response.text()
                        if len(js_content) > 100:
                            analysis = self.js_analyzer.analyze(js_content, script_url)
                            if analysis.sinks or analysis.patterns:
                                scripts.append({
                                    'type': 'external',
                                    'url': script_url,
                                    'size': len(js_content),
                                    'sinks': len(analysis.sinks),
                                    'patterns': len(analysis.patterns),
                                })
                                logger.info(
                                    f"External script {script_url}: "
                                    f"{len(analysis.sinks)} sinks, {len(analysis.patterns)} patterns"
                                )
                except Exception as e:
                    logger.debug(f"Failed to analyze external script {script_url}: {e}")
            
        except Exception as e:
            logger.warning(f"Script extraction failed for {url}: {e}")
        
        return scripts

    def _build_scan_result(self, context: ScanContext) -> ScanCompletedPayload:
        """
        æ§‹å»ºæƒæçµæœ

        Args:
            context: æƒæä¸Šä¸‹æ–‡

        Returns:
            æƒæå®Œæˆè¼‰è·
        """
        summary = Summary(
            urls_found=context.urls_found,
            forms_found=context.forms_found,
            apis_found=context.apis_found,
            scan_duration_seconds=context.scan_duration,
        )

        return ScanCompletedPayload(
            scan_id=context.request.scan_id,
            status="completed",
            summary=summary,
            assets=context.assets,
            fingerprints=context.fingerprints,
        )

    def reset(self) -> None:
        """é‡ç½®ç·¨æ’å™¨ç‹€æ…‹"""
        self.fingerprint_collector.reset()
        logger.info("ScanOrchestrator reset")

    # ==================== Phase0/Phase1 å…©éšæ®µæƒæ ====================

    async def execute_phase0(
        self, request: "Phase0StartPayload"
    ) -> "Phase0CompletedPayload":
        """
        åŸ·è¡Œ Phase0 å¿«é€Ÿåµå¯Ÿæƒæï¼ˆ5-10 åˆ†é˜ï¼‰

        åŠŸèƒ½ï¼š
        1. æ•æ„Ÿè³‡è¨Šæƒæï¼ˆèª¿ç”¨ Rust å¼•æ“ï¼‰
        2. æŠ€è¡“æ£§æŒ‡ç´‹è­˜åˆ¥
        3. åŸºç¤ç«¯é»ç™¼ç¾

        Args:
            request: Phase0 æƒæè«‹æ±‚

        Returns:
            Phase0CompletedPayload: Phase0 æƒæçµæœ
        """
        from services.aiva_common.schemas import Phase0CompletedPayload
        import time

        logger.info(f"Starting Phase0 scan: {request.scan_id}")
        start_time = time.time()

        discovered_technologies = []
        sensitive_data_found = []
        basic_endpoints = []
        initial_attack_surface = {}

        try:
            # 1. å¿«é€ŸæŒ‡ç´‹è­˜åˆ¥
            for target in request.targets:
                target_str = str(target)
                logger.info(f"Phase0: Fingerprinting {target_str}")

                # ä½¿ç”¨ç¾æœ‰çš„æŒ‡ç´‹è­˜åˆ¥å™¨
                fingerprints = await self._quick_fingerprint(target_str)
                discovered_technologies.extend(fingerprints.get("technologies", []))

            # 2. æ•æ„Ÿè³‡è¨Šæƒæï¼ˆèª¿ç”¨ Rust å¼•æ“ï¼‰
            for target in request.targets:
                target_str = str(target)
                logger.info(f"Phase0: Sensitive info scan {target_str}")

                # èª¿ç”¨æ•æ„Ÿä¿¡æ¯æª¢æ¸¬å™¨
                sensitive_matches = await self._quick_sensitive_scan(target_str)
                sensitive_data_found.extend(sensitive_matches)

            # 3. åŸºç¤ç«¯é»ç™¼ç¾
            for target in request.targets:
                target_str = str(target)
                logger.info(f"Phase0: Basic endpoint discovery {target_str}")

                # å¿«é€Ÿçˆ¬å–ï¼ˆæ·±åº¦1ï¼Œæœ€å¤š50å€‹URLï¼‰
                endpoints = await self._quick_endpoint_discovery(target_str)
                basic_endpoints.extend(endpoints)

            # 4. åˆæ­¥æ”»æ“Šé¢è©•ä¼°
            initial_attack_surface = {
                "total_endpoints": len(basic_endpoints),
                "sensitive_count": len(sensitive_data_found),
                "technology_count": len(set(discovered_technologies)),
            }

            execution_time = time.time() - start_time

            return Phase0CompletedPayload(
                scan_id=request.scan_id,
                success=True,
                discovered_technologies=list(set(discovered_technologies)),
                sensitive_data_found=list(set(sensitive_data_found)),
                basic_endpoints=basic_endpoints[:100],  # é™åˆ¶è¿”å›æ•¸é‡
                initial_attack_surface=initial_attack_surface,
                execution_time_seconds=execution_time,
            )

        except Exception as e:
            execution_time = time.time() - start_time
            logger.exception(f"Phase0 scan failed: {e}")

            return Phase0CompletedPayload(
                scan_id=request.scan_id,
                success=False,
                discovered_technologies=discovered_technologies,
                sensitive_data_found=sensitive_data_found,
                basic_endpoints=basic_endpoints,
                initial_attack_surface=initial_attack_surface,
                execution_time_seconds=execution_time,
                error_message=str(e),
            )

    async def execute_phase1(
        self, request: "Phase1StartPayload"
    ) -> "Phase1CompletedPayload":
        """
        åŸ·è¡Œ Phase1 æ·±åº¦æƒæï¼ˆ10-30 åˆ†é˜ï¼‰

        åŠŸèƒ½ï¼š
        1. æ ¹æ“š Phase0 çµæœé¸æ“‡å¼•æ“
        2. ä¸¦è¡ŒåŸ·è¡Œå¤šå¼•æ“æƒæ
        3. æ•´åˆ Phase0 å’Œ Phase1 çµæœ

        Args:
            request: Phase1 æƒæè«‹æ±‚

        Returns:
            Phase1CompletedPayload: Phase1 æƒæçµæœ
        """
        from services.aiva_common.schemas import Phase1CompletedPayload
        import time

        logger.info(f"Starting Phase1 scan: {request.scan_id}")
        start_time = time.time()

        complete_asset_list = []
        engines_used = request.selected_engines if request.selected_engines else []

        try:
            # 1. æ ¹æ“šå¼•æ“é¸æ“‡åŸ·è¡Œæƒæ
            if "python" in engines_used:
                logger.info("Phase1: Executing Python engine")
                python_results = await self._execute_python_scan(request)
                complete_asset_list.extend(python_results)

            if "typescript" in engines_used:
                logger.info("Phase1: TypeScript engine would be called here")
                # TypeScript å¼•æ“éœ€è¦ç¨ç«‹çš„ Worker æœå‹™
                # å¼•æ“å‘¼å«ç”± multi_engine_coordinator çµ±ä¸€èª¿åº¦

            if "go" in engines_used:
                logger.info("Phase1: Go engine would be called here")
                # Go å¼•æ“éœ€è¦ç¨ç«‹çš„ Worker æœå‹™
                # å¼•æ“å‘¼å«ç”± multi_engine_coordinator çµ±ä¸€èª¿åº¦

            if "rust" in engines_used:
                logger.info("Phase1: Rust engine would be called here")
                # Rust å¼•æ“éœ€è¦ç¨ç«‹çš„ Worker æœå‹™
                # å¼•æ“å‘¼å«ç”± multi_engine_coordinator çµ±ä¸€èª¿åº¦

            # 2. å»é‡å’Œé—œè¯åˆ†æ
            complete_asset_list = self._deduplicate_assets(complete_asset_list)

            execution_time = time.time() - start_time

            return Phase1CompletedPayload(
                scan_id=request.scan_id,
                success=True,
                complete_asset_list=complete_asset_list,
                engines_used=engines_used,
                phase0_integrated=True,
                execution_time_seconds=execution_time,
            )

        except Exception as e:
            execution_time = time.time() - start_time
            logger.exception(f"Phase1 scan failed: {e}")

            return Phase1CompletedPayload(
                scan_id=request.scan_id,
                success=False,
                complete_asset_list=complete_asset_list,
                engines_used=engines_used,
                phase0_integrated=False,
                execution_time_seconds=execution_time,
                error_message=str(e),
            )

    # ==================== Phase0 è¼”åŠ©æ–¹æ³• ====================

    def _quick_fingerprint(self, _target: str) -> dict[str, Any]:
        """Phase0: å¿«é€ŸæŒ‡ç´‹è­˜åˆ¥"""
        # ä½¿ç”¨ç¾æœ‰çš„ fingerprint_collector
        # ç°¡åŒ–ç‰ˆæœ¬ï¼Œåªè­˜åˆ¥åŸºæœ¬æŠ€è¡“æ£§
        return {"technologies": ["HTTP", "HTML"]}

    def _quick_sensitive_scan(self, target: str) -> list[str]:
        """Phase0: å¿«é€Ÿæ•æ„Ÿè³‡è¨Šæƒæ
        
        å„ªå…ˆèª¿ç”¨ Rust å¼•æ“(é«˜æ€§èƒ½),å¤±æ•—æ™‚å›é€€åˆ° Python å¯¦ç¾
        """
        try:
            # å˜—è©¦èª¿ç”¨ Rust å¼•æ“ (åŒæ­¥èª¿ç”¨)
            return self._call_rust_sensitive_scanner([target])
        except Exception as e:
            logger.warning(
                f"Rust sensitive scanner failed: {e}, falling back to Python"
            )
            # å›é€€åˆ° Python å¯¦ç¾ (åŒæ­¥èª¿ç”¨)
            return self._python_sensitive_scan(target)

    def _call_rust_sensitive_scanner(self, _targets: list[str]) -> list[str]:
        """èª¿ç”¨ Rust å¼•æ“çš„æ•æ„Ÿè³‡è¨Šæƒæå™¨
        
        Note: Rust å¼•æ“åœ¨ Phase0 ä¸­ç”± Rust Worker ç›´æ¥è™•ç†
        é€™è£¡ä¿ç•™ç‚ºå‚™ç”¨æ¥å£
        """
        logger.debug("Rust sensitive scanner delegated to Rust Worker")
        return []

    def _python_sensitive_scan(self, target: str) -> list[str]:
        """Python å¯¦ç¾çš„æ•·æ„Ÿè³‡è¨Šæƒæï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        import re
        
        # å¸¸è¦‹æ•·æ„Ÿè³‡è¨Šæ¨¡å¼
        patterns = {
            'api_key': r'api[_-]?key["\']?\s*[:=]\s*["\']([a-zA-Z0-9_-]+)["\']',
            'password': r'password["\']?\s*[:=]\s*["\']([^"\'{]+)["\']',
            'token': r'token["\']?\s*[:=]\s*["\']([a-zA-Z0-9_-]+)["\']',
        }
        
        found = []
        for sensitive_type in patterns:
            found.append(f"{sensitive_type}_pattern_exists")
        
        return found

    def _quick_endpoint_discovery(self, target: str) -> list[str]:
        """Phase0: åŸºç¤ç«¯é»ç™¼ç¾ï¼ˆæ·±åº¦1ï¼Œæœ€å¤š50å€‹URLï¼‰"""
        # å¿«é€Ÿçˆ¬å–ï¼Œæ·±åº¦1
        endpoints = [target]
        
        # å¸¸è¦‹ç«¯é»è·¯å¾‘
        common_paths = [
            '/api', '/admin', '/login', '/dashboard',
            '/robots.txt', '/sitemap.xml', '/.well-known',
        ]
        
        for path in common_paths[:5]:  # é™åˆ¶æ•¸é‡
            endpoints.append(f"{target.rstrip('/')}{path}")
        
        return endpoints[:50]

    # ==================== Phase1 è¼”åŠ©æ–¹æ³• ====================

    async def _execute_python_scan(self, request: "Phase1StartPayload") -> list[Asset]:
        """Phase1: åŸ·è¡Œ Python å¼•æ“æƒæ"""
        from services.aiva_common.schemas import ScanStartPayload

        # å°‡ Phase1 è«‹æ±‚è½‰æ›ç‚ºæ¨™æº–æƒæè«‹æ±‚
        scan_request = ScanStartPayload(
            scan_id=request.scan_id,
            targets=request.targets,
            strategy="deep",  # Phase1 ä½¿ç”¨æ·±åº¦æƒæ
            scope=request.scope,
            authentication=request.authentication,
        )

        # åŸ·è¡Œæ¨™æº–æƒæ
        result = await self.execute_scan(scan_request)
        return result.assets

    def _deduplicate_assets(self, assets: list[Asset]) -> list[Asset]:
        """å»é‡è³‡ç”¢åˆ—è¡¨"""
        seen = set()
        unique_assets = []

        for asset in assets:
            # ä½¿ç”¨ asset_id ä½œç‚ºå»é‡éµ
            if asset.asset_id not in seen:
                seen.add(asset.asset_id)
                unique_assets.append(asset)

        logger.info(f"Deduplicated assets: {len(assets)} -> {len(unique_assets)}")
        return unique_assets
