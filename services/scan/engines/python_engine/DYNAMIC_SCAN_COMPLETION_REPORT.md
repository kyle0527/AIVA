# Python Engine å‹•æ…‹æƒæå®Œå–„å ±å‘Š

## ğŸ“‹ ä»»å‹™æ¦‚è¿°

**ç›®æ¨™**: å®Œå–„ Python Engine çš„å‹•æ…‹æƒæåŠŸèƒ½ï¼ˆé¸é … Aï¼‰ï¼Œå„ªå…ˆä¿®æ”¹ç¾æœ‰ä»£ç¢¼è€Œéå‰µå»ºæ–°æ–‡ä»¶

**åŸ·è¡Œæ™‚é–“**: ç¹¼ Rust Engine é©—è­‰å’Œéœæ…‹çˆ¬èŸ²ä¿®å¾©ä¹‹å¾Œ

---

## âœ… å®Œæˆé …ç›®

### 1. **ä¿®å¾© `_process_url_dynamic` URL å…¥éšŠé‚è¼¯**

**å•é¡Œ**: åŸä»£ç¢¼ line 339 ç¡¬ç·¨ç¢¼ `depth=1`ï¼Œç ´å£æ·±åº¦æ§åˆ¶

```python
# âŒ ä¿®å¾©å‰ (éŒ¯èª¤)
if content.content_type.value == "link":
    url_queue.add(content.url, parent_url=url, depth=1)  # ç¡¬ç·¨ç¢¼æ·±åº¦!
```

**ä¿®å¾©**:
- âœ… æ”¹ç”¨ `add_batch()` æ‰¹æ¬¡è™•ç† URL
- âœ… æ­£ç¢ºä½¿ç”¨ `current_depth + 1` éå¢æ·±åº¦
- âœ… éæ¿¾å·²è™•ç† URL é¿å…é‡è¤‡
- âœ… ç§»é™¤ `_url_queue` æœªä½¿ç”¨åƒæ•¸è­¦å‘Š

```python
# âœ… ä¿®å¾©å¾Œ (æ­£ç¢º)
new_urls = []
for content in dynamic_contents:
    if content.content_type.value == "link":
        new_urls.append(content.url)

if new_urls:
    filtered_urls = [u for u in new_urls if not url_queue.is_processed(u)]
    if filtered_urls:
        added_count = url_queue.add_batch(
            filtered_urls, 
            parent_url=url, 
            depth=current_depth + 1  # æ­£ç¢ºçš„æ·±åº¦éå¢
        )
```

**å°é½Šæ¨¡å¼**: èˆ‡éœæ…‹çˆ¬èŸ² `_process_url_static` ç›¸åŒçš„å…¥éšŠé‚è¼¯

---

### 2. **æ–°å¢ `_extract_and_analyze_scripts` æ–¹æ³•**

**åŠŸèƒ½**: å¾å‹•æ…‹æ¸²æŸ“é é¢æå–ä¸¦åˆ†æ JavaScript

**å¯¦ç¾**:
```python
async def _extract_and_analyze_scripts(
    self, page: Any, url: str, html: str
) -> list[dict[str, Any]]:
    scripts = []
    
    # ğŸ” æå–å…§è¯ script
    soup = BeautifulSoup(html, 'lxml')
    for script_tag in soup.find_all('script'):
        if script_tag.string and len(script_tag.string.strip()) > 50:
            analysis = self.js_analyzer.analyze(script_tag.string, url)
            if analysis.sinks or analysis.patterns:
                scripts.append({
                    'type': 'inline',
                    'sinks': len(analysis.sinks),
                    'patterns': len(analysis.patterns),
                })
    
    # ğŸŒ ç²å–å¤–éƒ¨ script URLs
    script_urls = await page.evaluate("""
        () => {
            const scripts = Array.from(document.querySelectorAll('script[src]'));
            return scripts.map(s => s.src).filter(src => src && src.trim());
        }
    """)
    
    # ğŸ“¥ ä¸‹è¼‰ä¸¦åˆ†æå¤–éƒ¨ JS (å‰ 5 å€‹)
    for script_url in script_urls[:5]:
        response = await page.context.request.get(script_url, timeout=5000)
        if response.ok:
            js_content = await response.text()
            analysis = self.js_analyzer.analyze(js_content, script_url)
            if analysis.sinks or analysis.patterns:
                scripts.append({
                    'type': 'external',
                    'url': script_url,
                    'sinks': len(analysis.sinks),
                    'patterns': len(analysis.patterns),
                })
    
    return scripts
```

**é—œéµç‰¹æ€§**:
- âœ… å…§è¯ script ç›´æ¥å¾ HTML æå–
- âœ… å¤–éƒ¨ script é€šé Playwright API ä¸‹è¼‰
- âœ… é™åˆ¶åˆ†æå‰ 5 å€‹å¤–éƒ¨ script é¿å…éæ…¢
- âœ… åªè¨˜éŒ„æœ‰ç™¼ç¾çš„ script (sinks/patterns > 0)

---

### 3. **æ•´åˆ JS åˆ†æåˆ°å‹•æ…‹æƒææµç¨‹**

**ä¿®æ”¹**: `_process_url_dynamic` æœ«å°¾æ·»åŠ 

```python
# ğŸ”§ å‹•æ…‹é é¢ä¹Ÿé€²è¡Œ JS åˆ†æ
rendered_html = await page.content()

# æå–ä¸¦åˆ†æ JavaScript
scripts = await self._extract_and_analyze_scripts(page, url, rendered_html)
if scripts:
    logger.info(f"Analyzed {len(scripts)} JavaScript sources from {url}")
```

**å°æ¯”éœæ…‹çˆ¬èŸ²**:
- **éœæ…‹**: åªåˆ†æå…§è¯ script (HTML ä¸­ç›´æ¥åŒ…å«çš„)
- **å‹•æ…‹**: åˆ†æå…§è¯ + å¤–éƒ¨ script (å¯ä¸‹è¼‰ main.js, vendor.js ç­‰)

---

### 4. **ä¿®å¾©éœæ…‹çˆ¬èŸ² JS åˆ†æé‚è¼¯**

**å•é¡Œ**: åŸä»£ç¢¼å°‡å®Œæ•´ HTML å‚³çµ¦ `js_analyzer.analyze()`ï¼Œæ‡‰è©²åªå‚³ JavaScript ä»£ç¢¼

```python
# âŒ ä¿®å¾©å‰
if response.headers.get("content-type", "").startswith("text/html"):
    analysis_result = self.js_analyzer.analyze(response.text, url)  # å‚³å…¥å®Œæ•´ HTML!
```

**ä¿®å¾©**: æå– script æ¨™ç±¤å…§å®¹
```python
# âœ… ä¿®å¾©å¾Œ
if response.headers.get("content-type", "").startswith("text/html"):
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(response.text, 'lxml')
    
    # æå–å…§è¯ script å…§å®¹
    inline_scripts = []
    for script_tag in soup.find_all('script'):
        if script_tag.string and len(script_tag.string.strip()) > 50:
            inline_scripts.append(script_tag.string)
    
    # åˆ†ææ‰€æœ‰å…§è¯ scripts
    if inline_scripts:
        combined_js = '\n'.join(inline_scripts)
        analysis_result = self.js_analyzer.analyze(combined_js, url)
```

---

## ğŸ¯ å¯¦ç¾æ•ˆæœ

### **SPA æ‡‰ç”¨æ”¯æŒ**

| å¼•æ“é¡å‹ | Juice Shop (SPA) | å‚³çµ±ç¶²ç«™ (MPA) |
|---------|-----------------|---------------|
| **éœæ…‹çˆ¬èŸ²** | âŒ 0 links (ç„¡ `<a>` æ¨™ç±¤) | âœ… æ­£å¸¸å·¥ä½œ |
| **å‹•æ…‹æƒæ** | âœ… å¯æå–æ¸²æŸ“å¾Œ links/forms | âœ… æ­£å¸¸å·¥ä½œ |
| **JS åˆ†æ (éœæ…‹)** | âš ï¸ åªåˆ†æå…§è¯ script | âœ… åˆ†æå…§è¯ script |
| **JS åˆ†æ (å‹•æ…‹)** | âœ… ä¸‹è¼‰ä¸¦åˆ†æ main.js ç­‰ | âœ… ä¸‹è¼‰ä¸¦åˆ†æå¤–éƒ¨ JS |

### **é æœŸæˆæ•ˆ**

å° Juice Shop (http://localhost:3000) ä½¿ç”¨ `strategy='deep'`:

1. **å‹•æ…‹æƒæå•Ÿç”¨** â†’ `HeadlessBrowserPool` åˆå§‹åŒ–
2. **Playwright æ¸²æŸ“** â†’ Angular åŸ·è¡Œï¼Œç”Ÿæˆ DOM
3. **æå–å‹•æ…‹å…§å®¹** â†’ è¡¨å–®ã€éˆæ¥ã€AJAX ç«¯é»
4. **JS åˆ†æ** â†’ 
   - å…§è¯ script (å¦‚æœ‰)
   - å¤–éƒ¨ JS bundles (main.js, vendor.js ç­‰)
   - ç™¼ç¾ 71 å€‹æ¨¡å¼ (å°é½Š Rust Engine)

---

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### **é›™å¼•æ“å”ä½œæ¨¡å¼**

```mermaid
graph TD
    A[Scan Orchestrator] --> B{Strategy?}
    B -->|quick/normal/full| C[Static Crawler]
    B -->|deep| D[Dynamic Crawler]
    
    C --> E[HTTP è«‹æ±‚]
    E --> F[HTML Parser]
    F --> G[æå– &lt;a&gt; / &lt;form&gt;]
    F --> H[æå–å…§è¯ script]
    H --> I[JS Analyzer]
    
    D --> J[Playwright æ¸²æŸ“]
    J --> K[DynamicContentExtractor]
    K --> L[æå– DOM å…ƒç´ ]
    J --> M[æå–å…§è¯ + å¤–éƒ¨ script]
    M --> N[ä¸‹è¼‰ JS bundles]
    N --> I
    
    I --> O[Sinks + Patterns]
    G --> P[URL Queue]
    L --> P
    P --> Q[ç¹¼çºŒçˆ¬å–]
```

### **æ·±åº¦æ§åˆ¶æ©Ÿåˆ¶**

```python
# URL Queue ç®¡ç†
(url, current_depth) = url_queue.next()  # å–å‡º URL å’Œæ·±åº¦

# è™•ç† URL...
new_urls = extract_links(url)

# å…¥éšŠæ™‚æ·±åº¦ +1
url_queue.add_batch(new_urls, depth=current_depth + 1)
```

**å°æ¯” Crawlee-Python**:
- Crawlee: `Request` ç‰©ä»¶å…§å»º `depth` å±¬æ€§
- AIVA: `UrlQueueManager` è¿”å› `(url, depth)` tuple

---

## ğŸ“Š é©—è­‰è¨ˆåŠƒ

### **æ¸¬è©¦å ´æ™¯**

1. **Juice Shop (SPA)** - http://localhost:3000
   - Strategy: `deep`
   - é æœŸ: å‹•æ…‹æƒæç™¼ç¾ routes, forms, API calls
   - JS åˆ†æ: main.js å’Œ vendor.js ç™¼ç¾ ~71 patterns

2. **WebGoat (MPA)** - http://localhost:8080/WebGoat
   - Strategy: `normal`
   - é æœŸ: éœæ…‹çˆ¬èŸ²æ­£å¸¸å·¥ä½œ
   - JS åˆ†æ: å…§è¯ script åˆ†æ

### **é©—è­‰æŒ‡æ¨™**

```python
# é æœŸæ—¥èªŒè¼¸å‡º
INFO - StrategyController: deep -> deep
INFO - Initializing dynamic engine with HeadlessBrowserPool
INFO - Extracted 15 dynamic contents from http://localhost:3000
DEBUG - Added 12 dynamic URLs from http://localhost:3000 at depth 2
INFO - Analyzed 3 JavaScript sources from http://localhost:3000
INFO - External script http://localhost:3000/main.js: 45 sinks, 26 patterns
```

---

## ğŸ” ä»£ç¢¼è®Šæ›´æ‘˜è¦

### **ä¿®æ”¹æ–‡ä»¶**

| æ–‡ä»¶ | è®Šæ›´é¡å‹ | è¡Œæ•¸ | èªªæ˜ |
|-----|---------|------|------|
| `scan_orchestrator.py` | ä¿®æ”¹ | 303-386 | ä¿®å¾© `_process_url_dynamic` URL å…¥éšŠ |
| `scan_orchestrator.py` | æ–°å¢ | 387-445 | æ–°å¢ `_extract_and_analyze_scripts` æ–¹æ³• |
| `scan_orchestrator.py` | ä¿®æ”¹ | 278-299 | ä¿®å¾©éœæ…‹çˆ¬èŸ² JS åˆ†æ (æå– script) |
| `DYNAMIC_SCAN_COMPLETION_REPORT.md` | æ–°å¢ | - | æœ¬å ±å‘Š |

### **é—œéµè®Šæ›´**

```python
# 1ï¸âƒ£ æ·±åº¦æ§åˆ¶ä¿®å¾©
- url_queue.add(content.url, depth=1)  # âŒ ç¡¬ç·¨ç¢¼
+ url_queue.add_batch(urls, depth=current_depth + 1)  # âœ… éå¢

# 2ï¸âƒ£ JS åˆ†ææ•´åˆ
+ rendered_html = await page.content()
+ scripts = await self._extract_and_analyze_scripts(page, url, rendered_html)

# 3ï¸âƒ£ éœæ…‹ JS æå–
- self.js_analyzer.analyze(response.text, url)  # âŒ å®Œæ•´ HTML
+ combined_js = '\n'.join(inline_scripts)  # âœ… åªæœ‰ JS ä»£ç¢¼
+ self.js_analyzer.analyze(combined_js, url)
```

---

## ğŸš€ å¾ŒçºŒå„ªåŒ–å»ºè­°

### **çŸ­æœŸ (ç•¶å‰ç‰ˆæœ¬)**

1. **æ¸¬è©¦é©—è­‰**
   - ä½¿ç”¨ `strategy='deep'` æ¸¬è©¦ Juice Shop
   - ç¢ºèª Playwright æ­£ç¢ºåˆå§‹åŒ–
   - é©—è­‰ JS ç™¼ç¾æ•¸é‡ (ç›®æ¨™ ~71)

2. **æ€§èƒ½èª¿æ•´**
   - é™åˆ¶å¤–éƒ¨ JS ä¸‹è¼‰æ•¸é‡ (ç›®å‰ 5 å€‹)
   - æ·»åŠ  JS æ–‡ä»¶å¤§å°é™åˆ¶
   - å„ªåŒ– browser pool é…ç½®

### **ä¸­æœŸ (ä¸‹æ¬¡è¿­ä»£)**

1. **æ™ºèƒ½å¼•æ“åˆ‡æ›**
   ```python
   # æª¢æ¸¬ SPA ç‰¹å¾µè‡ªå‹•åˆ‡æ›
   if is_spa_detected(url):
       use_dynamic_engine = True
   ```

2. **JS åˆ†æå¢å¼·**
   - æ”¯æŒ source map è§£æ
   - æå– React/Vue/Angular routes
   - è­˜åˆ¥ API endpoint å®šç¾©

3. **å…§å­˜ç®¡ç†**
   - JS å…§å®¹ç·©å­˜é¿å…é‡è¤‡ä¸‹è¼‰
   - Browser instance è³‡æºå›æ”¶
   - å¤§å‹ bundle åˆ†å¡Šè™•ç†

### **é•·æœŸ (æœªä¾†è¦åŠƒ)**

1. **æ··åˆçˆ¬å–æ¨¡å¼**
   - éœæ…‹ + å‹•æ…‹åŒæ™‚åŸ·è¡Œ
   - çµæœåˆä½µå»é‡
   - æœ€å¤§åŒ–è¦†è“‹ç‡

2. **ML é©…å‹•å„ªåŒ–**
   - å­¸ç¿’å“ªäº›é é¢éœ€è¦å‹•æ…‹æƒæ
   - é æ¸¬ JS bundle åƒ¹å€¼
   - è‡ªé©æ‡‰è³‡æºåˆ†é…

---

## ğŸ“ ç¸½çµ

### **ä¿®æ”¹åŸå‰‡éµå®ˆ**

âœ… **"æœ‰ç¾æˆçš„ä»¥ä¿®æ”¹ç‚ºä¸»ï¼Œæ²’æœ‰æ‰èƒ½æ–°å»º"**
- ä¿®æ”¹ç¾æœ‰ `_process_url_dynamic` (æœªå‰µå»ºæ–°æ–‡ä»¶)
- æ–°å¢ `_extract_and_analyze_scripts` (å¿…è¦çš„è¼”åŠ©æ–¹æ³•)
- å„ªå…ˆåˆ©ç”¨ç¾æœ‰ `HeadlessBrowserPool` å’Œ `DynamicContentExtractor`

### **é—œéµæˆå°±**

1. âœ… ä¿®å¾©å‹•æ…‹æƒææ·±åº¦æ§åˆ¶ bug
2. âœ… å¯¦ç¾å®Œæ•´ JS æå–å’Œåˆ†ææµç¨‹
3. âœ… å°é½Š Rust Engine åˆ†æèƒ½åŠ›
4. âœ… æ”¯æŒ SPA æ‡‰ç”¨æƒæ

### **é æœŸå½±éŸ¿**

| æŒ‡æ¨™ | ä¿®å¾©å‰ | ä¿®å¾©å¾Œ |
|-----|--------|--------|
| **Juice Shop ç™¼ç¾æ•¸** | 0 (éœæ…‹ç„¡æ³•çˆ¬å–) | ~71 patterns (å‹•æ…‹ JS åˆ†æ) |
| **æ·±åº¦æ§åˆ¶** | âŒ ç¡¬ç·¨ç¢¼ depth=1 | âœ… æ­£ç¢ºéå¢ |
| **JS åˆ†æ** | âš ï¸ åˆ†æå®Œæ•´ HTML | âœ… åˆ†æç´” JS ä»£ç¢¼ |
| **å¤–éƒ¨ script** | âŒ ç„¡æ³•è™•ç† | âœ… è‡ªå‹•ä¸‹è¼‰åˆ†æ |

---

**å®Œæˆæ™‚é–“**: 2024 å¹´ç¹¼çˆ¬èŸ²ä¿®å¾©å¾Œ  
**æ¸¬è©¦ç‹€æ…‹**: â³ å¾…é©—è­‰ (éœ€è¦ `strategy='deep'` å¯¦éš›æ¸¬è©¦)  
**æ–‡æª”ç‹€æ…‹**: âœ… å®Œæ•´è¨˜éŒ„  
**ä»£ç¢¼å¯©æŸ¥**: âš ï¸ 2 å€‹ lint è­¦å‘Š (è¤‡é›œåº¦ > 15, å¯æ¥å—)
