#!/usr/bin/env python3
"""
çœŸå¯¦çš„ç¥ç¶“ç¶²è·¯æ ¸å¿ƒ - æ›¿æ›AIVAçš„å‡AIå¯¦ç¾

é€™å€‹æ¨¡çµ„åŒ…å«çœŸæ­£çš„PyTorchç¥ç¶“ç¶²è·¯ï¼Œå…·æœ‰ï¼š
- çœŸå¯¦çš„æ¬Šé‡æª”æ¡ˆ (18-20MB)
- å¯¦éš›çš„æ¢¯åº¦ä¸‹é™è¨“ç·´
- çœŸæ­£çš„çŸ©é™£ä¹˜æ³•é‹ç®— (y = Wx + b)
- å¯æŒä¹…åŒ–çš„æ¬Šé‡å„²å­˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import json
import time

# aiva_common è¦ç¯„å°å…¥ - ä½¿ç”¨æ¨™æº–æšèˆ‰
try:
    from aiva_common.enums.common import Severity, Confidence
    from aiva_common.enums.security import VulnerabilityType
    from aiva_common.error_handling import AIVAError, ErrorType, ErrorSeverity, create_error_context
    AIVA_COMMON_AVAILABLE = True
except ImportError:
    # é™ç´šæ–¹æ¡ˆï¼šå¦‚æœ aiva_common ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°å¸¸é‡
    AIVA_COMMON_AVAILABLE = False
    logging.warning("aiva_common ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°å¸¸é‡")
    
    # ä½¿ç”¨æ¨¡çµ„åˆ¥åé¿å…é¡å‹è¡çª
    class _LocalSeverity:
        CRITICAL = "critical"
        HIGH = "high"
        MEDIUM = "medium"
        LOW = "low"
    
    class _LocalConfidence:
        CERTAIN = "certain"
        FIRM = "firm" 
        POSSIBLE = "possible"
    
    # è¨­ç½®åˆ¥åï¼ˆåƒ…ç”¨æ–¼ Severity å’Œ Confidenceï¼‰
    Severity = _LocalSeverity  # type: ignore
    Confidence = _LocalConfidence  # type: ignore
    
    # éŒ¯èª¤è™•ç†é™ç´š - ç›´æ¥ä½¿ç”¨æ¨™æº–ç•°å¸¸è€Œéé‡å®šç¾©
    # åœ¨é™ç´šæ¨¡å¼ä¸‹ï¼Œä»ç„¶ä½¿ç”¨æ¨™æº–ç•°å¸¸ï¼Œä½†ä¸æœƒæœ‰ AIVAError çš„é¡å¤–åŠŸèƒ½

MODULE_NAME = "real_neural_core"

# P0 ä¿®å¾©: èªæ„ç·¨ç¢¼æ”¯æ´
try:
    from sentence_transformers import SentenceTransformer
    SEMANTIC_ENCODING_AVAILABLE = True
except ImportError:
    SEMANTIC_ENCODING_AVAILABLE = False
    logging.warning("sentence-transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨é™ç´šç·¨ç¢¼æ–¹æ¡ˆ")

logger = logging.getLogger(__name__)

class RealAICore(nn.Module):
    """çœŸå¯¦çš„AIæ ¸å¿ƒ - ä½¿ç”¨5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¨¡å‹"""
    
    def __init__(self, 
                 input_size: int = 512,
                 hidden_sizes: Optional[list] = None, 
                 output_size: int = 100,  # 5Mæ¨¡å‹ä¸»è¼¸å‡ºç¶­åº¦
                 aux_output_size: int = 531,  # 5Mæ¨¡å‹è¼”åŠ©è¼¸å‡ºç¶­åº¦
                 use_5m_model: bool = True,  # æ˜¯å¦ä½¿ç”¨5Mæ¨¡å‹
                 weights_path: Optional[str] = None):
        """
        åˆå§‹åŒ–çœŸå¯¦çš„ç¥ç¶“ç¶²è·¯æ ¸å¿ƒ
        
        Args:
            input_size: è¼¸å…¥ç‰¹å¾µç¶­åº¦ (512)
            hidden_sizes: éš±è—å±¤å°ºå¯¸åˆ—è¡¨ (5Mæ¨¡å‹å°ˆç”¨)
            output_size: ä¸»è¼¸å‡ºç¶­åº¦ (100)
            aux_output_size: è¼”åŠ©è¼¸å‡ºç¶­åº¦ (531)
            use_5m_model: æ˜¯å¦ä½¿ç”¨5Mæ¨¡å‹
            weights_path: 5Mæ¨¡å‹æ¬Šé‡è·¯å¾‘
        """
        super(RealAICore, self).__init__()
        
        self.input_size = input_size
        self.output_size = output_size
        self.aux_output_size = aux_output_size
        self.use_5m_model = use_5m_model
        
        if use_5m_model:
            # 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¶æ§‹
            self.hidden_sizes = [1650, 1200, 1000, 600, 300]
            self._build_5m_network()
            self.weights_path = weights_path or "ai_engine/aiva_5M_weights.pth"
        else:
            # åŸå§‹ç¶²è·¯æ¶æ§‹ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            if hidden_sizes is None:
                hidden_sizes = [2048, 1024, 512]
            self.hidden_sizes = hidden_sizes
            self._build_legacy_network()
        
        # è¨ˆç®—ç¸½åƒæ•¸æ•¸é‡
        self.total_params = sum(p.numel() for p in self.parameters())
        
        logger.info("çœŸå¯¦AIæ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"  - æ¨¡å‹é¡å‹: {'5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯' if use_5m_model else 'åŸå§‹æ¶æ§‹'}")
        logger.info(f"  - ç¸½åƒæ•¸: {self.total_params:,} ({self.total_params/1_000_000:.2f}M)")
        if use_5m_model:
            logger.info(f"  - ç¶²è·¯çµæ§‹: {input_size} -> {' -> '.join(map(str, self.hidden_sizes))} -> {output_size}(ä¸»)/{aux_output_size}(è¼”)")
        else:
            logger.info(f"  - ç¶²è·¯çµæ§‹: {input_size} -> {' -> '.join(map(str, self.hidden_sizes))} -> {output_size}")
    
    def _build_5m_network(self):
        """æ§‹å»º5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯ï¼ˆæ”¹é€²ç‰ˆï¼šè§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼‰"""
        # æ ¹æ“š5Mæ¨¡å‹æ¬Šé‡çµæ§‹æ§‹å»ºç¶²è·¯ï¼ˆåŒ¹é…æ¬Šé‡éµåï¼‰
        self.layer1 = nn.Linear(512, 1650)
        self.bn1 = nn.BatchNorm1d(1650)  # æ‰¹æ¬¡æ­£è¦åŒ–
        
        self.layer2 = nn.Linear(1650, 1200)
        self.bn2 = nn.BatchNorm1d(1200)
        
        self.layer3 = nn.Linear(1200, 1000)
        self.bn3 = nn.BatchNorm1d(1000)
        
        self.layer4 = nn.Linear(1000, 600)
        self.bn4 = nn.BatchNorm1d(600)
        
        self.layer5 = nn.Linear(600, 300)
        self.bn5 = nn.BatchNorm1d(300)
        
        # é›™è¼¸å‡ºå±¤ï¼ˆåŒ¹é…æ¬Šé‡æª”æ¡ˆä¸­çš„éµåï¼‰
        self.output = nn.Linear(300, 100)  # ä¸»è¼¸å‡º (åŒ¹é… "output.weight")
        self.aux = nn.Linear(300, 531)     # è¼”åŠ©è¼¸å‡º (åŒ¹é… "aux.weight")
        
        # ä½¿ç”¨æ›´å¥½çš„æ¿€æ´»å‡½æ•¸çµ„åˆ
        self.activation = nn.SiLU()  # SiLU/Swish æ¿€æ´»å‡½æ•¸ï¼Œæœ‰åŠ©æ–¼æ¢¯åº¦æµå‹•
        self.dropout = nn.Dropout(0.1)  # é™ä½dropoutç‡
        
        # æ¬Šé‡åˆå§‹åŒ–
        self._initialize_weights()
        
    def _initialize_weights(self):
        """æ”¹é€²çš„æ¬Šé‡åˆå§‹åŒ–ç­–ç•¥"""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieræ­£è¦åˆå§‹åŒ–ï¼ˆå°SiLUæ¿€æ´»å‡½æ•¸æ›´é©åˆï¼‰
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)  # å°çš„æ­£åå·®
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
        
    def _build_legacy_network(self):
        """æ§‹å»ºåŸå§‹ç¶²è·¯æ¶æ§‹ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
        layers = []
        prev_size = self.input_size
        
        for hidden_size in self.hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.2))
            prev_size = hidden_size
        
        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_size, self.output_size))
        self.network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­ - æ”¹é€²ç‰ˆï¼šä½¿ç”¨æ‰¹æ¬¡æ­£è¦åŒ–å’Œæ›´å¥½çš„æ¿€æ´»å‡½æ•¸
        
        Args:
            x: è¼¸å…¥å¼µé‡ [batch_size, input_size]
            
        Returns:
            è¼¸å‡ºå¼µé‡ [batch_size, output_size] (ä¸»è¼¸å‡º)
        """
        if self.use_5m_model:
            # 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯å‰å‘å‚³æ’­ï¼ˆæ”¹é€²ç‰ˆï¼‰
            x = self.activation(self.bn1(self.layer1(x)))
            x = self.dropout(x)
            
            x = self.activation(self.bn2(self.layer2(x)))
            x = self.dropout(x)
            
            x = self.activation(self.bn3(self.layer3(x)))
            x = self.dropout(x)
            
            x = self.activation(self.bn4(self.layer4(x)))
            x = self.dropout(x)
            
            x = self.activation(self.bn5(self.layer5(x)))
            
            # ä¸»è¼¸å‡ºï¼ˆä¸ä½¿ç”¨dropoutï¼Œä¿æŒæ±ºç­–ç©©å®šæ€§ï¼‰
            main_output = self.output(x)
            return main_output
        else:
            # åŸå§‹ç¶²è·¯æ¶æ§‹
            return self.network(x)
    
    def forward_with_aux(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘å‚³æ’­ä¸¦è¿”å›é›™è¼¸å‡º - æ”¹é€²ç‰ˆï¼ˆåƒ…5Mæ¨¡å‹æ”¯æ´ï¼‰
        
        Args:
            x: è¼¸å…¥å¼µé‡ [batch_size, input_size]
            
        Returns:
            (main_output, aux_output): ä¸»è¼¸å‡ºå’Œè¼”åŠ©è¼¸å‡º
        """
        if not self.use_5m_model:
            if AIVA_COMMON_AVAILABLE:
                raise AIVAError(
                    "é›™è¼¸å‡ºåƒ…åœ¨5Mæ¨¡å‹æ¨¡å¼ä¸‹æ”¯æ´",
                    error_type=ErrorType.VALIDATION,
                    severity=ErrorSeverity.MEDIUM,
                    context=create_error_context(module=MODULE_NAME, function="dual_forward")
                )
            else:
                raise AIVAError(
                    "é›™è¼¸å‡ºåƒ…åœ¨5Mæ¨¡å‹æ¨¡å¼ä¸‹æ”¯æ´",
                    error_type=ErrorType.VALIDATION,
                    severity=ErrorSeverity.MEDIUM,
                    context=create_error_context(module=MODULE_NAME, function="dual_forward")
                )
            
        # ä½¿ç”¨ç›¸åŒçš„å‰å‘å‚³æ’­è·¯å¾‘ç¢ºä¿ä¸€è‡´æ€§
        x = self.activation(self.bn1(self.layer1(x)))
        x = self.dropout(x)
        
        x = self.activation(self.bn2(self.layer2(x)))
        x = self.dropout(x)
        
        x = self.activation(self.bn3(self.layer3(x)))
        x = self.dropout(x)
        
        x = self.activation(self.bn4(self.layer4(x)))
        x = self.dropout(x)
        
        x = self.activation(self.bn5(self.layer5(x)))
        
        main_output = self.output(x)  # ä¸»è¼¸å‡º (100ç¶­)
        aux_output = self.aux(x)      # è¼”åŠ©è¼¸å‡º (531ç¶­)
        
        return main_output, aux_output
    
    def save_weights(self, filepath: str) -> None:
        """å„²å­˜çœŸå¯¦çš„æ¬Šé‡æª”æ¡ˆ"""
        try:
            # å„²å­˜æ¨¡å‹ç‹€æ…‹
            state_dict = {
                'model_state_dict': self.state_dict(),
                'architecture': {
                    'input_size': self.input_size,
                    'hidden_sizes': self.hidden_sizes,
                    'output_size': self.output_size,
                    'dropout_rate': self.dropout_rate
                },
                'total_params': self.total_params,
                'timestamp': time.time()
            }
            
            torch.save(state_dict, filepath)
            file_size = Path(filepath).stat().st_size
            logger.info(f"æ¬Šé‡å·²å„²å­˜: {filepath} ({file_size/1024/1024:.1f} MB)")
            
        except Exception as e:
            logger.error(f"å„²å­˜æ¬Šé‡å¤±æ•—: {e}")
            raise
    
    def load_weights(self, filepath: Optional[str] = None) -> None:
        """è¼‰å…¥çœŸå¯¦çš„æ¬Šé‡æª”æ¡ˆ"""
        try:
            # ç¢ºå®šæ¬Šé‡æª”æ¡ˆè·¯å¾‘
            if filepath is None:
                if self.use_5m_model:
                    filepath = self.weights_path
                else:
                    logger.warning("æœªæŒ‡å®šæ¬Šé‡æª”æ¡ˆè·¯å¾‘")
                    return
            
            if not Path(filepath).exists():
                logger.warning(f"æ¬Šé‡æª”æ¡ˆä¸å­˜åœ¨: {filepath}")
                return
            
            logger.info(f"è¼‰å…¥æ¬Šé‡: {filepath}")
            checkpoint = torch.load(filepath, map_location='cpu')
            
            if self.use_5m_model and 'model_state_dict' not in checkpoint:
                # 5Mæ¨¡å‹æ¬Šé‡æ˜¯ç›´æ¥çš„state_dictæ ¼å¼
                logger.info("è¼‰å…¥5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¬Šé‡...")
                self.load_state_dict(checkpoint)
                
                # è¨ˆç®—æª”æ¡ˆå¤§å°
                file_size = Path(filepath).stat().st_size
                total_params = sum(tensor.numel() for tensor in checkpoint.values())
                
                logger.info("5Mæ¨¡å‹è¼‰å…¥å®Œæˆ:")
                logger.info(f"  - æª”æ¡ˆå¤§å°: {file_size/1024/1024:.1f} MB")
                logger.info(f"  - ç¸½åƒæ•¸: {total_params:,}")
                logger.info(f"  - ä¸»è¼¸å‡ºç¶­åº¦: {self.output_size}")
                logger.info(f"  - è¼”åŠ©è¼¸å‡ºç¶­åº¦: {self.aux_output_size}")
                
            else:
                # æ¨™æº–æ ¼å¼æ¬Šé‡
                if 'model_state_dict' in checkpoint:
                    self.load_state_dict(checkpoint['model_state_dict'])
                    total_params = checkpoint.get('total_params', self.total_params)
                else:
                    self.load_state_dict(checkpoint)
                    total_params = self.total_params
                
                file_size = Path(filepath).stat().st_size
                logger.info(f"æ¬Šé‡å·²è¼‰å…¥: {filepath} ({file_size/1024/1024:.1f} MB)")
                logger.info(f"æ¨¡å‹åƒæ•¸: {total_params:,}")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¬Šé‡å¤±æ•—: {e}")
            raise

class RealDecisionEngine:
    """çœŸå¯¦çš„æ±ºç­–å¼•æ“ - æ”¯æ´5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯"""
    
    def __init__(self, weights_path: Optional[str] = None, use_5m_model: bool = True):
        """
        åˆå§‹åŒ–çœŸå¯¦çš„æ±ºç­–å¼•æ“
        
        Args:
            weights_path: é è¨“ç·´æ¬Šé‡è·¯å¾‘
            use_5m_model: æ˜¯å¦ä½¿ç”¨5Mæ¨¡å‹
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_5m_model = use_5m_model
        
        # å‰µå»ºçœŸå¯¦çš„AIæ ¸å¿ƒ
        if use_5m_model:
            self.ai_core = RealAICore(
                input_size=512,
                output_size=100,  # 5Mæ¨¡å‹ä¸»è¼¸å‡º
                aux_output_size=531,  # 5Mæ¨¡å‹è¼”åŠ©è¼¸å‡º
                use_5m_model=True,
                weights_path=weights_path or "ai_engine/aiva_5M_weights.pth"
            ).to(self.device)
            logger.info("ä½¿ç”¨5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ±ºç­–å¼•æ“")
        else:
            self.ai_core = RealAICore(
                input_size=512,
                hidden_sizes=[2048, 1024, 512],  # åŸå§‹æ¶æ§‹
                output_size=128,
                use_5m_model=False
            ).to(self.device)
            logger.info("ä½¿ç”¨åŸå§‹æ¶æ§‹æ±ºç­–å¼•æ“")
        
        # å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸ï¼ˆæ”¹é€²ç‰ˆï¼Œè§£æ±ºæ¢¯åº¦æ¶ˆå¤±ï¼‰
        self.optimizer = optim.AdamW(
            self.ai_core.parameters(), 
            lr=3e-4,  # ç•¥å¾®æé«˜å­¸ç¿’ç‡
            weight_decay=0.01,
            betas=(0.9, 0.999),  # Adamåƒæ•¸
            eps=1e-8
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆé¤˜å¼¦é€€ç«ï¼‰
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2
        )
        
        # æ¢¯åº¦è£å‰ªé–¾å€¼
        self.grad_clip_value = 1.0
        
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ¨™ç±¤å¹³æ»‘
        
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if weights_path and Path(weights_path).exists():
            self.ai_core.load_weights(weights_path)
        
        self.training_history = []
        
        # P0 ä¿®å¾©: åˆå§‹åŒ–èªæ„ç·¨ç¢¼å™¨
        self.semantic_encoder = None
        if SEMANTIC_ENCODING_AVAILABLE:
            try:
                # ä½¿ç”¨ all-MiniLM-L6-v2 (è¼•é‡ç´š, 384ç¶­, é©åˆä»£ç¢¼)
                self.semantic_encoder = SentenceTransformer('all-MiniLM-L6-v2')
                # ç§»å‹•åˆ°ç›¸åŒè¨­å‚™
                self.semantic_encoder.to(self.device)
                logger.info("âœ… èªæ„ç·¨ç¢¼å™¨å·²è¼‰å…¥: all-MiniLM-L6-v2 (384ç¶­)")
            except Exception as e:
                logger.warning(f"èªæ„ç·¨ç¢¼å™¨è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é™ç´šæ–¹æ¡ˆ: {e}")
                self.semantic_encoder = None
        
        logger.info(f"çœŸå¯¦æ±ºç­–å¼•æ“åˆå§‹åŒ–å®Œæˆ (Device: {self.device})")
        logger.info(f"  ç·¨ç¢¼æ¨¡å¼: {'èªæ„ç·¨ç¢¼ (Semantic)' if self.semantic_encoder else 'å­—ç¬¦ç·¨ç¢¼ (Fallback)'}")
    
    def encode_input(self, text: str) -> torch.Tensor:
        """
        å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºå‘é‡ - å°ˆç‚º 5M Bug Bounty ç‰¹åŒ–ç¥ç¶“ç¶²çµ¡å„ªåŒ–
        
        P0 ä¿®å¾©: å¾å­—ç¬¦ç´¯åŠ å‡ç´šç‚ºèªæ„ç†è§£ + Bug Bounty ç‰¹åŒ–
        - ä¿®å¾©å‰: AI ç„¡æ³•å€åˆ† 'def' å’Œ 'fed' çš„èªæ„å·®ç•°
        - ä¿®å¾©å¾Œ: AI ç†è§£é—œéµå­—ã€çµæ§‹ã€èªæ„ï¼Œå°ˆæ”» Bug Bounty å ´æ™¯
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬ (ç¨‹å¼ç¢¼æˆ–è‡ªç„¶èªè¨€)
            
        Returns:
            ç·¨ç¢¼å¾Œçš„å‘é‡å¼µé‡ (512ç¶­)
        """
        # æ¸…ç†ä¸¦æ¨™æº–åŒ–æ–‡æœ¬
        text = text.strip()
        if not text:
            return torch.zeros(1, 512, dtype=torch.float32).to(self.device)
        
        # æ–¹æ¡ˆ A: èªæ„ç·¨ç¢¼ + Bug Bounty ç‰¹åŒ– (å„ªå…ˆ)
        if self.semantic_encoder is not None:
            try:
                # Bug Bounty ä¸Šä¸‹æ–‡å¢å¼·
                bug_bounty_context = self._enhance_bug_bounty_context(text)
                
                # ä½¿ç”¨ Sentence Transformers é€²è¡Œèªæ„ç·¨ç¢¼
                embedding = self.semantic_encoder.encode(
                    bug_bounty_context,
                    convert_to_tensor=True,
                    show_progress_bar=False,
                    device=str(self.device)
                )
                
                # èª¿æ•´ç¶­åº¦è‡³ 512 å°ˆç‚º 5M ç¶²çµ¡
                if embedding.shape[0] != 512:
                    # ä½¿ç”¨ç·šæ€§è®Šæ›è€Œéæ± åŒ–ï¼Œä¿æŒæ›´å¤šèªæ„ä¿¡æ¯
                    if embedding.shape[0] < 512:
                        # æ“´å±•ç¶­åº¦ï¼šé‡è¤‡é—œéµç‰¹å¾µ
                        repeat_factor = 512 // embedding.shape[0] + 1
                        embedding = embedding.repeat(repeat_factor)[:512]
                    else:
                        # ç¸®æ¸›ç¶­åº¦ï¼šä¿ç•™æœ€é‡è¦ç‰¹å¾µ
                        embedding = embedding[:512]
                
                # æ·»åŠ  Bug Bounty å°ˆæ¥­ç‰¹å¾µ
                bug_bounty_features = self._extract_bug_bounty_features(text)
                embedding[-32:] = bug_bounty_features  # æœ€å¾Œ32ç¶­å°ˆé–€ç”¨æ–¼å°ˆæ¥­ç‰¹å¾µ
                
                # ç¢ºä¿å½¢ç‹€æ­£ç¢ºä¸¦æ­¸ä¸€åŒ–
                embedding = torch.clamp(embedding, -1.0, 1.0)
                return embedding.unsqueeze(0).to(self.device)
                
            except Exception as e:
                logger.warning(f"èªæ„ç·¨ç¢¼å¤±æ•—ï¼Œé™ç´šè‡³å¢å¼·å­—ç¬¦ç·¨ç¢¼: {e}")
                # é™ç´šåˆ°æ–¹æ¡ˆ B
        
        # æ–¹æ¡ˆ B: å¢å¼·å­—ç¬¦ç·¨ç¢¼ (Fallback) - å°ˆç‚º 5M ç¶²çµ¡å„ªåŒ–
        logger.debug("ä½¿ç”¨å¢å¼·å­—ç¬¦ç·¨ç¢¼æ–¹æ¡ˆ")
        text_lower = text.lower()
        vector = np.zeros(512, dtype=np.float32)
        
        # å¤šå±¤ç‰¹å¾µç·¨ç¢¼ç­–ç•¥
        # [0:128] æ”»æ“Šæ„åœ–ç‰¹å¾µ
        attack_features = self._extract_attack_intent_features(text_lower)
        vector[0:128] = attack_features
        
        # [128:256] ç›®æ¨™ç³»çµ±ç‰¹å¾µ  
        target_features = self._extract_target_features(text_lower)
        vector[128:256] = target_features
        
        # [256:384] å·¥å…·å’ŒæŠ€è¡“ç‰¹å¾µ
        tool_features = self._extract_tool_features(text_lower)
        vector[256:384] = tool_features
        
        # [384:512] ä¸Šä¸‹æ–‡å’Œçµ±è¨ˆç‰¹å¾µ
        context_features = self._extract_context_features(text_lower)
        vector[384:512] = context_features
        
        return torch.tensor(vector, dtype=torch.float32).unsqueeze(0).to(self.device)
    
    def _enhance_bug_bounty_context(self, text: str) -> str:
        """ç‚º Bug Bounty å ´æ™¯å¢å¼·è¼¸å…¥æ–‡æœ¬ä¸Šä¸‹æ–‡"""
        # Bug Bounty é—œéµè©å’Œä¸Šä¸‹æ–‡å¢å¼·
        bug_bounty_keywords = {
            'sql': 'SQL injection vulnerability analysis',
            'xss': 'Cross-site scripting attack vector', 
            'csrf': 'Cross-site request forgery exploitation',
            'ssrf': 'Server-side request forgery testing',
            'lfi': 'Local file inclusion vulnerability',
            'rfi': 'Remote file inclusion exploitation',
            'upload': 'File upload security bypass',
            'scan': 'Security vulnerability scanning',
            'exploit': 'Security exploitation technique',
            'payload': 'Attack payload generation',
            'bypass': 'Security control bypass method'
        }
        
        enhanced_text = text.lower()
        for keyword, context in bug_bounty_keywords.items():
            if keyword in enhanced_text:
                enhanced_text = f"{context}: {text}"
                break
                
        return enhanced_text
    
    def _extract_bug_bounty_features(self, text: str) -> torch.Tensor:
        """æå– Bug Bounty å°ˆæ¥­ç‰¹å¾µ (32ç¶­)"""
        features = torch.zeros(32)
        text_lower = text.lower()
        
        # æ”»æ“Šé¡å‹ç‰¹å¾µ (16ç¶­)
        attack_types = [
            'sql', 'xss', 'csrf', 'ssrf', 'lfi', 'rfi', 'xxe', 'ssti',
            'upload', 'auth', 'session', 'crypto', 'logic', 'race', 'dos', 'info'
        ]
        for i, attack_type in enumerate(attack_types):
            if attack_type in text_lower:
                features[i] = 1.0
                
        # å·¥å…·å’ŒæŠ€è¡“ç‰¹å¾µ (16ç¶­)  
        tools_techniques = [
            'burp', 'nmap', 'sqlmap', 'nikto', 'dirb', 'gobuster', 'john', 'hydra',
            'metasploit', 'netcat', 'wireshark', 'hashcat', 'payload', 'exploit',
            'reverse', 'shell'
        ]
        for i, tool in enumerate(tools_techniques):
            if tool in text_lower:
                features[16 + i] = 1.0
                
        return features
    
    def _extract_attack_intent_features(self, text: str) -> np.ndarray:
        """æå–æ”»æ“Šæ„åœ–ç‰¹å¾µ (128ç¶­)"""
        features = np.zeros(128)
        
        # Web æ”»æ“Šæ¨¡å¼ (32ç¶­)
        web_patterns = ['sql', 'xss', 'csrf', 'ssrf', 'lfi', 'rfi', 'upload', 'auth']
        for i, pattern in enumerate(web_patterns):
            if pattern in text:
                features[i * 4:(i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]  # æ¢¯åº¦ç‰¹å¾µ
                
        # ç¶²çµ¡æ”»æ“Šæ¨¡å¼ (32ç¶­)
        network_patterns = ['scan', 'enum', 'brute', 'dos', 'mitm', 'spoofing', 'sniffing', 'tunnel']
        for i, pattern in enumerate(network_patterns):
            if pattern in text:
                features[32 + i * 4:32 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # ææ¬Šå’Œå¾Œæ»²é€ (32ç¶­)
        post_exploit = ['privilege', 'escalation', 'persistence', 'lateral', 'exfiltration', 'covering', 'backdoor', 'rootkit']
        for i, pattern in enumerate(post_exploit):
            if pattern in text:
                features[64 + i * 4:64 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # ä¿¡æ¯æ”¶é›† (32ç¶­)
        info_gathering = ['recon', 'fingerprint', 'osint', 'social', 'phishing', 'discover', 'probe', 'passive']
        for i, pattern in enumerate(info_gathering):
            if pattern in text:
                features[96 + i * 4:96 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        return features
    
    def _extract_target_features(self, text: str) -> np.ndarray:
        """æå–ç›®æ¨™ç³»çµ±ç‰¹å¾µ (128ç¶­)"""
        features = np.zeros(128)
        
        # æ“ä½œç³»çµ±ç‰¹å¾µ (32ç¶­)
        os_types = ['linux', 'windows', 'macos', 'unix', 'android', 'ios', 'embedded', 'router']
        for i, os_type in enumerate(os_types):
            if os_type in text:
                features[i * 4:(i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # æœå‹™é¡å‹ (32ç¶­)
        services = ['http', 'https', 'ssh', 'ftp', 'smtp', 'dns', 'mysql', 'postgresql']
        for i, service in enumerate(services):
            if service in text:
                features[32 + i * 4:32 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # æ‡‰ç”¨æ¡†æ¶ (32ç¶­)
        frameworks = ['php', 'java', 'python', 'nodejs', 'ruby', 'asp', 'jsp', 'perl']
        for i, framework in enumerate(frameworks):
            if framework in text:
                features[64 + i * 4:64 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # æ•¸æ“šåº«é¡å‹ (32ç¶­) 
        databases = ['mysql', 'postgresql', 'oracle', 'mssql', 'mongodb', 'redis', 'sqlite', 'cassandra']
        for i, db in enumerate(databases):
            if db in text:
                features[96 + i * 4:96 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        return features
    
    def _extract_tool_features(self, text: str) -> np.ndarray:
        """æå–å·¥å…·å’ŒæŠ€è¡“ç‰¹å¾µ (128ç¶­)"""
        features = np.zeros(128)
        
        # æƒæå·¥å…· (32ç¶­)
        scan_tools = ['nmap', 'masscan', 'zmap', 'nikto', 'dirb', 'gobuster', 'dirbuster', 'wfuzz']
        for i, tool in enumerate(scan_tools):
            if tool in text:
                features[i * 4:(i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # æ¼æ´åˆ©ç”¨å·¥å…· (32ç¶­)
        exploit_tools = ['metasploit', 'burp', 'sqlmap', 'xss', 'beef', 'setoolkit', 'social', 'custom']
        for i, tool in enumerate(exploit_tools):
            if tool in text:
                features[32 + i * 4:32 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # å¯†ç¢¼ç ´è§£å·¥å…· (32ç¶­)
        crack_tools = ['john', 'hashcat', 'hydra', 'medusa', 'crunch', 'cewl', 'rockyou', 'wordlist']
        for i, tool in enumerate(crack_tools):
            if tool in text:
                features[64 + i * 4:64 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        # ç¶²çµ¡å·¥å…· (32ç¶­)
        network_tools = ['wireshark', 'tcpdump', 'netcat', 'socat', 'proxychains', 'tor', 'vpn', 'tunnel']
        for i, tool in enumerate(network_tools):
            if tool in text:
                features[96 + i * 4:96 + (i + 1) * 4] = [1.0, 0.8, 0.6, 0.4]
                
        return features
    
    def _extract_context_features(self, text: str) -> np.ndarray:
        """æå–ä¸Šä¸‹æ–‡å’Œçµ±è¨ˆç‰¹å¾µ (128ç¶­)"""
        features = np.zeros(128)
        
        if len(text) == 0:
            return features
            
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ (32ç¶­)
        features[0] = min(len(text) / 1000.0, 1.0)  # æ–‡æœ¬é•·åº¦
        features[1] = text.count(' ') / max(len(text), 1)  # ç©ºæ ¼å¯†åº¦
        features[2] = text.count('.') / max(len(text), 1)  # å¥è™Ÿå¯†åº¦
        features[3] = text.count('(') / max(len(text), 1)  # æ‹¬è™Ÿå¯†åº¦
        features[4] = text.count('{') / max(len(text), 1)  # å¤§æ‹¬è™Ÿå¯†åº¦
        features[5] = text.count('[') / max(len(text), 1)  # ä¸­æ‹¬è™Ÿå¯†åº¦
        features[6] = text.count('"') / max(len(text), 1)  # å¼•è™Ÿå¯†åº¦
        features[7] = text.count("'") / max(len(text), 1)  # å–®å¼•è™Ÿå¯†åº¦
        
        # å­—ç¬¦é¡å‹åˆ†å¸ƒ (32ç¶­)
        features[32] = sum(1 for c in text if c.isalpha()) / max(len(text), 1)  # å­—æ¯æ¯”ä¾‹
        features[33] = sum(1 for c in text if c.isdigit()) / max(len(text), 1)  # æ•¸å­—æ¯”ä¾‹
        features[34] = sum(1 for c in text if c.isupper()) / max(len(text), 1)  # å¤§å¯«æ¯”ä¾‹
        features[35] = sum(1 for c in text if c.islower()) / max(len(text), 1)  # å°å¯«æ¯”ä¾‹
        
        # ç‰¹æ®Šå­—ç¬¦æ¨¡å¼ (32ç¶­)
        special_chars = ['/', '\\', '|', '&', ';', ':', '=', '+', '-', '*', '%', '@', '#', '$', '!', '?']
        for i, char in enumerate(special_chars[:16]):
            features[64 + i] = text.count(char) / max(len(text), 1)
            
        # n-gram ç‰¹å¾µ (32ç¶­)
        # è¨ˆç®—å¸¸è¦‹ 2-gram å’Œ 3-gram çš„å‡ºç¾é »ç‡
        if len(text) >= 2:
            bigrams = [text[i:i+2] for i in range(len(text)-1)]
            common_bigrams = ['th', 'he', 'in', 'er', 'an', 're', 'ed', 'nd', 'ha', 'to', 'ou', 'ea', 'hi', 'ng', 'se', 'on']
            for i, bigram in enumerate(common_bigrams):
                features[96 + i] = bigrams.count(bigram) / max(len(bigrams), 1)
                
        return features
    
    def generate_decision(self, 
                         task_description: str, 
                         context: str = "") -> Dict[str, Any]:
        """
        ä½¿ç”¨ 5M ç‰¹åŒ–ç¥ç¶“ç¶²çµ¡ç”Ÿæˆ Bug Bounty å°ˆæ¥­æ±ºç­–
        
        ä¿®å¾©å„ªåŒ–ï¼š
        - å¢å¼·ç½®ä¿¡åº¦è¨ˆç®— (é›™é‡è¼¸å‡ºåˆ†æ)
        - Bug Bounty å°ˆæ¥­æ±ºç­–é‚è¼¯
        - æ›´æ™ºèƒ½çš„éŒ¯èª¤è™•ç†
        - ä¿æŒç¾æœ‰æ¥å£å…¼å®¹æ€§
        
        Args:
            task_description: ä»»å‹™æè¿°
            context: ä¸Šä¸‹æ–‡è³‡è¨Š
            
        Returns:
            æ±ºç­–çµæœå­—å…¸ (å…¼å®¹ç¾æœ‰æ ¼å¼)
        """
        try:
            self.ai_core.eval()
            
            with torch.no_grad():
                # å¢å¼·çš„å‘é‡åŒ–ï¼šçµåˆä»»å‹™å’Œä¸Šä¸‹æ–‡
                combined_input = self._prepare_decision_input(task_description, context)
                input_vector = self.encode_input(combined_input)
                
                # 5M ç¶²çµ¡é›™é‡è¼¸å‡ºæ±ºç­–
                if self.use_5m_model:
                    main_output, aux_output = self.ai_core.forward_with_aux(input_vector)
                    
                    # å¢å¼·çš„ç½®ä¿¡åº¦è¨ˆç®—
                    confidence = self._calculate_enhanced_confidence(main_output, aux_output)
                    
                    # Bug Bounty å°ˆæ¥­æ±ºç­–è§£æ
                    decision_analysis = self._analyze_bug_bounty_decision(
                        main_output, aux_output, task_description, context
                    )
                    
                    return {
                        "decision": task_description,
                        "confidence": float(confidence),
                        "reasoning": decision_analysis["reasoning"],
                        "context_used": context,
                        "decision_index": decision_analysis["decision_index"],
                        "attack_vector": decision_analysis.get("attack_vector", "unknown"),
                        "risk_level": decision_analysis.get("risk_level", "medium"),
                        "recommended_tools": decision_analysis.get("recommended_tools", []),
                        "is_real_ai": True,
                        "model_type": "5M_specialized"
                    }
                else:
                    # åŸå§‹æ¨¡å¼å…¼å®¹æ€§
                    output = self.ai_core(input_vector)
                    probabilities = F.softmax(output, dim=1)
                    confidence = float(torch.max(probabilities))
                    decision_index = torch.argmax(output, dim=1).item()
                    
                    return {
                        "decision": task_description,
                        "confidence": confidence,
                        "reasoning": f"Legacy AI æ±ºç­–ï¼Œä¿¡å¿ƒåº¦: {confidence:.3f}",
                        "context_used": context,
                        "decision_index": decision_index,
                        "is_real_ai": True,
                        "model_type": "legacy"
                    }
                    
        except Exception as e:
            logger.error(f"æ±ºç­–ç”Ÿæˆå¤±æ•—: {e}")
            # é™ç´šæ±ºç­–ï¼šåŸºæ–¼è¦å‰‡çš„ Bug Bounty æ±ºç­–
            return self._fallback_bug_bounty_decision(task_description, context, str(e))
    
    def _prepare_decision_input(self, task_description: str, context: str) -> str:
        """ç‚º 5M ç¶²çµ¡æº–å‚™æœ€å„ªè¼¸å…¥æ ¼å¼"""
        # Bug Bounty ä»»å‹™é¡å‹è­˜åˆ¥å’Œå¢å¼·
        task_lower = task_description.lower()
        
        # è­˜åˆ¥ Bug Bounty éšæ®µ
        if any(keyword in task_lower for keyword in ['scan', 'discover', 'enum', 'recon']):
            phase_prefix = "[RECONNAISSANCE]"
        elif any(keyword in task_lower for keyword in ['exploit', 'attack', 'payload']):
            phase_prefix = "[EXPLOITATION]"
        elif any(keyword in task_lower for keyword in ['privilege', 'escalation', 'lateral']):
            phase_prefix = "[POST_EXPLOITATION]"
        elif any(keyword in task_lower for keyword in ['report', 'document', 'evidence']):
            phase_prefix = "[REPORTING]"
        else:
            phase_prefix = "[ANALYSIS]"
            
        # çµ„åˆå¢å¼·è¼¸å…¥
        enhanced_input = f"{phase_prefix} {task_description}"
        if context:
            enhanced_input += f" [CONTEXT: {context}]"
            
        return enhanced_input
    
    def _calculate_enhanced_confidence(self, main_output: torch.Tensor, aux_output: torch.Tensor) -> float:
        """åŸºæ–¼é›™é‡è¼¸å‡ºè¨ˆç®—å¢å¼·ç½®ä¿¡åº¦"""
        # ä¸»è¼¸å‡ºç½®ä¿¡åº¦ (100ç¶­æ±ºç­–å‘é‡)
        main_probs = F.softmax(main_output, dim=1)
        main_confidence = float(torch.max(main_probs))
        
        # è¼”åŠ©è¼¸å‡ºä¸€è‡´æ€§ (531ç¶­ä¸Šä¸‹æ–‡å‘é‡)
        aux_stability = float(1.0 - torch.std(aux_output, dim=1).mean())  # ç©©å®šæ€§æŒ‡æ¨™
        aux_magnitude = float(torch.mean(torch.abs(aux_output)))  # æ¿€æ´»å¼·åº¦
        
        # ç¶œåˆç½®ä¿¡åº¦è¨ˆç®—: 70% ä¸»æ±ºç­– + 20% ç©©å®šæ€§ + 10% æ¿€æ´»å¼·åº¦
        enhanced_confidence = (
            0.7 * main_confidence +
            0.2 * max(0.0, min(1.0, aux_stability)) +
            0.1 * max(0.0, min(1.0, aux_magnitude))
        )
        
        return max(0.0, min(1.0, enhanced_confidence))
    
    def _analyze_bug_bounty_decision(self, main_output: torch.Tensor, aux_output: torch.Tensor, 
                                   task: str, context: str) -> Dict[str, Any]:
        """åˆ†æ Bug Bounty å°ˆæ¥­æ±ºç­–"""
        # è§£æä¸»æ±ºç­–å‘é‡ (100ç¶­)
        decision_probs = F.softmax(main_output, dim=1).squeeze()
        decision_index = int(torch.argmax(decision_probs))
        
        # æ”»æ“Šå‘é‡åˆ†æ (åŸºæ–¼ä»»å‹™æè¿°)
        task_lower = task.lower()
        if 'sql' in task_lower:
            attack_vector = 'sql_injection'
        elif 'xss' in task_lower:
            attack_vector = 'cross_site_scripting'
        elif 'ssrf' in task_lower:
            attack_vector = 'server_side_request_forgery'
        elif 'upload' in task_lower:
            attack_vector = 'file_upload'
        elif 'auth' in task_lower:
            attack_vector = 'authentication_bypass'
        else:
            attack_vector = 'reconnaissance'
            
        # é¢¨éšªç­‰ç´šè©•ä¼° - ä½¿ç”¨ aiva_common æ¨™æº–æšèˆ‰
        confidence_score = float(torch.max(decision_probs))
        if confidence_score > 0.8:
            risk_level = Severity.HIGH
        elif confidence_score > 0.6:
            risk_level = Severity.MEDIUM
        else:
            risk_level = Severity.LOW
            
        # æ¨è–¦å·¥å…· (åŸºæ–¼æ”»æ“Šå‘é‡)
        tool_recommendations = {
            'sql_injection': ['sqlmap', 'burp_suite', 'manual_testing'],
            'cross_site_scripting': ['burp_suite', 'xss_hunter', 'beef_framework'],
            'server_side_request_forgery': ['burp_suite', 'ssrf_sheriff', 'manual_testing'],
            'file_upload': ['burp_suite', 'upload_scanner', 'file_analysis'],
            'authentication_bypass': ['burp_suite', 'auth_analyzer', 'credential_testing'],
            'reconnaissance': ['nmap', 'dirb', 'whatweb', 'nikto']
        }
        
        recommended_tools = tool_recommendations.get(attack_vector, ['manual_analysis'])
        
        # ç”Ÿæˆæ¨ç†èªªæ˜
        reasoning = (
            f"5M ç‰¹åŒ–ç¥ç¶“ç¶²çµ¡åˆ†æ: æ”»æ“Šå‘é‡ '{attack_vector}' "
            f"(ç½®ä¿¡åº¦ {confidence_score:.3f}), é¢¨éšªç­‰ç´š {risk_level}, "
            f"æ¨è–¦å·¥å…·: {', '.join(recommended_tools)}"
        )
        
        return {
            "decision_index": decision_index,
            "attack_vector": attack_vector,
            "risk_level": risk_level,
            "recommended_tools": recommended_tools,
            "reasoning": reasoning,
            "confidence_breakdown": {
                "main_decision": float(confidence_score),
                "aux_stability": float(1.0 - torch.std(aux_output, dim=1).mean()),
                "aux_magnitude": float(torch.mean(torch.abs(aux_output)))
            }
        }
    
    def _fallback_bug_bounty_decision(self, task: str, context: str, error: str) -> Dict[str, Any]:
        """é™ç´š Bug Bounty æ±ºç­– (åŸºæ–¼è¦å‰‡)"""
        task_lower = task.lower()
        
        # åŸºæ–¼é—œéµè©çš„ç°¡å–®åˆ†é¡
        if any(keyword in task_lower for keyword in ['sql', 'injection', 'database']):
            return {
                "decision": task,
                "confidence": 0.6,
                "reasoning": f"åŸºæ–¼è¦å‰‡çš„é™ç´šæ±ºç­–: SQLæ³¨å…¥æ¸¬è©¦ (éŒ¯èª¤: {error[:50]})",
                "context_used": context,
                "decision_index": 1,
                "attack_vector": "sql_injection",
                "risk_level": Severity.MEDIUM,
                "recommended_tools": ["sqlmap", "manual_testing"],
                "is_real_ai": False,
                "model_type": "fallback_rules",
                "error": error
            }
        else:
            return {
                "decision": task,
                "confidence": 0.5,
                "reasoning": f"åŸºæ–¼è¦å‰‡çš„é™ç´šæ±ºç­–: ä¸€èˆ¬å®‰å…¨æ¸¬è©¦ (éŒ¯èª¤: {error[:50]})",
                "context_used": context,
                "decision_index": 0,
                "attack_vector": "reconnaissance",
                "risk_level": Severity.LOW,
                "recommended_tools": ["manual_analysis"],
                "is_real_ai": False,
                "model_type": "fallback_rules",
                "error": error
            }
    
    def train_step(self, 
                   inputs: torch.Tensor, 
                   targets: torch.Tensor,
                   aux_targets: Optional[torch.Tensor] = None) -> Dict[str, float]:
        """
        åŸ·è¡Œå„ªåŒ–çš„ 5M ç‰¹åŒ–ç¶²çµ¡è¨“ç·´æ­¥é©Ÿ
        
        Args:
            inputs: è¼¸å…¥æ•¸æ“š [batch_size, 512]
            targets: ä¸»è¦ç›®æ¨™æ¨™ç±¤ [batch_size, output_size]
            aux_targets: è¼”åŠ©ç›®æ¨™æ¨™ç±¤ [batch_size, aux_output_size] (å¯é¸)
            
        Returns:
            æå¤±çµ±è¨ˆå­—å…¸
        """
        self.ai_core.train()
        
        # å‰å‘å‚³æ’­è¨ˆç®—æå¤±
        loss_breakdown = self._compute_training_loss(inputs, targets, aux_targets)
        total_loss_tensor = loss_breakdown.pop('total_loss_tensor', None)
        
        # åå‘å‚³æ’­èˆ‡æ¢¯åº¦è™•ç†
        if total_loss_tensor is not None:
            self._perform_backward_pass(total_loss_tensor)
        
        # æ›´æ–°çµ±è¨ˆè³‡è¨Š
        self._update_training_statistics(loss_breakdown)
        
        return loss_breakdown
    
    def _compute_training_loss(self, inputs: torch.Tensor, targets: torch.Tensor, 
                              aux_targets: Optional[torch.Tensor]) -> Dict[str, Any]:
        """è¨ˆç®—è¨“ç·´æå¤±"""
        if self.use_5m_model and aux_targets is not None:
            return self._compute_dual_output_loss(inputs, targets, aux_targets)
        else:
            return self._compute_single_output_loss(inputs, targets)
    
    def _compute_dual_output_loss(self, inputs: torch.Tensor, targets: torch.Tensor,
                                 aux_targets: torch.Tensor) -> Dict[str, Any]:
        """è¨ˆç®—é›™é‡è¼¸å‡ºæå¤± (5M æ¨¡å‹)"""
        main_output, aux_output = self.ai_core.forward_with_aux(inputs)
        
        # ä¸»è¦æå¤± (æ±ºç­–æº–ç¢ºæ€§)
        if targets.dim() == 1:
            main_loss = self.criterion(main_output, targets.long())
        else:
            main_loss = F.mse_loss(main_output, targets)
        
        # è¼”åŠ©æå¤± (ä¸Šä¸‹æ–‡ç†è§£)
        aux_loss = F.mse_loss(aux_output, aux_targets)
        
        # Bug Bounty ç‰¹åŒ–æå¤±åŠ æ¬Š: 70% ä¸»æ±ºç­– + 30% ä¸Šä¸‹æ–‡ç†è§£
        total_loss = 0.7 * main_loss + 0.3 * aux_loss
        
        return {
            'main_loss': float(main_loss.item()),
            'aux_loss': float(aux_loss.item()),
            'total_loss': float(total_loss.item()),
            'total_loss_tensor': total_loss,  # ä¿ç•™ tensor ç”¨æ–¼ backward
            'loss_ratio': float(main_loss.item() / (aux_loss.item() + 1e-8))
        }
    
    def _compute_single_output_loss(self, inputs: torch.Tensor, targets: torch.Tensor) -> Dict[str, Any]:
        """è¨ˆç®—å–®ä¸€è¼¸å‡ºæå¤± (èˆŠå¼æ¨¡å‹)"""
        output = self.ai_core(inputs)
        
        if targets.dim() == 1:
            total_loss = self.criterion(output, targets.long())
        else:
            total_loss = F.mse_loss(output, targets)
            
        return {
            'main_loss': float(total_loss.item()),
            'total_loss': float(total_loss.item()),
            'total_loss_tensor': total_loss  # ä¿ç•™ tensor ç”¨æ–¼ backward
        }
    
    def _perform_backward_pass(self, total_loss: torch.Tensor) -> None:
        """åŸ·è¡Œåå‘å‚³æ’­ä¸¦è™•ç†æ¢¯åº¦ï¼ˆè§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼‰"""
        # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
        self.optimizer.zero_grad()
        
        # åå‘å‚³æ’­
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé—œéµä¿®å¾©ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.ai_core.parameters(), self.grad_clip_value)
        
        # æ¢¯åº¦ç›£æ§ï¼ˆæª¢æ¸¬æ¢¯åº¦æ¶ˆå¤±ï¼‰
        grad_norms = []
        for name, param in self.ai_core.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2)
                grad_norms.append(grad_norm.item())
        
        avg_grad_norm = sum(grad_norms) / len(grad_norms) if grad_norms else 0.0
        
        # å¦‚æœæ¢¯åº¦å¤ªå°ï¼Œèª¿æ•´å­¸ç¿’ç‡
        if avg_grad_norm < 1e-7:
            logger.warning(f"æª¢æ¸¬åˆ°æ¢¯åº¦æ¶ˆå¤± (å¹³å‡æ¢¯åº¦ç¯„æ•¸: {avg_grad_norm:.2e})ï¼Œå‹•æ…‹èª¿æ•´å­¸ç¿’ç‡")
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= 1.1  # ç•¥å¾®æé«˜å­¸ç¿’ç‡
        
        # åƒæ•¸æ›´æ–°
        self.optimizer.step()
        
        # å­¸ç¿’ç‡èª¿åº¦
        self.scheduler.step()
        
        # è¨˜éŒ„æ¢¯åº¦ä¿¡æ¯ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        if hasattr(self, 'training_history'):
            self.training_history.append({
                'avg_grad_norm': avg_grad_norm,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'timestamp': time.time()
            })
    
    def _update_training_statistics(self, loss_breakdown: Dict[str, Any]) -> None:
        """æ›´æ–°è¨“ç·´çµ±è¨ˆä¿¡æ¯"""
        # åŸºæœ¬çµ±è¨ˆæ›´æ–°
        if not hasattr(self, 'training_stats'):
            self.training_stats = {
                'total_steps': 0,
                'avg_loss': 0.0,
                'min_loss': float('inf'),
                'max_loss': 0.0
            }
        
        current_loss = loss_breakdown.get('total_loss', 0.0)
        self.training_stats['total_steps'] += 1
        self.training_stats['avg_loss'] = (
            (self.training_stats['avg_loss'] * (self.training_stats['total_steps'] - 1) + current_loss) /
            self.training_stats['total_steps']
        )
        self.training_stats['min_loss'] = min(self.training_stats['min_loss'], current_loss)
        self.training_stats['max_loss'] = max(self.training_stats['max_loss'], current_loss)
        
        # æ·»åŠ æ¢¯åº¦ä¿¡æ¯åˆ°loss_breakdown
        grad_norm = self._calculate_gradient_norm()
        loss_breakdown['gradient_norm'] = float(grad_norm)
        loss_breakdown['learning_rate'] = float(self.optimizer.param_groups[0]['lr'])
        loss_breakdown['model_type'] = '5M_specialized' if self.use_5m_model else 'legacy'
    
    def _calculate_gradient_norm(self) -> float:
        """è¨ˆç®—æ¢¯åº¦ç¯„æ•¸"""
        total_norm = 0.0
        param_count = 0
        
        for p in self.ai_core.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        if param_count == 0:
            return 0.0
            
        return (total_norm ** 0.5) / param_count
    
    def save_model(self, filepath: str) -> None:
        """å„²å­˜å®Œæ•´çš„çœŸå¯¦AIæ¨¡å‹"""
        self.ai_core.save_weights(filepath)
        
        # å„²å­˜è¨“ç·´æ­·å²
        history_path = filepath.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)

def create_real_ai_replacement() -> RealDecisionEngine:
    """
    å‰µå»ºçœŸå¯¦çš„AIä¾†æ›¿æ›AIVAçš„å‡AI
    
    Returns:
        çœŸå¯¦çš„æ±ºç­–å¼•æ“å¯¦ä¾‹
    """
    logger.info("æ­£åœ¨å‰µå»ºçœŸå¯¦AIä¾†æ›¿æ›å‡AI...")
    
    # æ¬Šé‡æª”æ¡ˆè·¯å¾‘
    weights_path = "aiva_real_ai_core.pth"
    
    # å‰µå»ºçœŸå¯¦çš„æ±ºç­–å¼•æ“
    engine = RealDecisionEngine(weights_path)
    
    # å¦‚æœæ²’æœ‰é è¨“ç·´æ¬Šé‡ï¼Œä¿å­˜ä¸€å€‹åˆå§‹ç‰ˆæœ¬
    if not Path(weights_path).exists():
        logger.info("å‰µå»ºåˆå§‹æ¬Šé‡æª”æ¡ˆ...")
        engine.save_model(weights_path)
    
    return engine

# æ¸¬è©¦å‡½æ•¸
def test_real_vs_fake_ai():
    """æ¸¬è©¦çœŸå¯¦AI vs å‡AIçš„å·®ç•°"""
    
    print("=" * 60)
    print("çœŸå¯¦AI vs å‡AI å°æ¯”æ¸¬è©¦")
    print("=" * 60)
    
    # å‰µå»ºçœŸå¯¦AI
    real_engine = create_real_ai_replacement()
    
    # æ¸¬è©¦æ•¸æ“š
    test_input = "åˆ†æç³»çµ±æ€§èƒ½ä¸¦æä¾›å„ªåŒ–å»ºè­°"
    
    # çœŸå¯¦AIæ±ºç­–
    real_result = real_engine.generate_decision(test_input, "ç³»çµ±CPUä½¿ç”¨ç‡90%")
    
    print("\nğŸ¤– çœŸå¯¦AIçµæœ:")
    print(f"  æ±ºç­–: {real_result['decision']}")
    print(f"  ä¿¡å¿ƒåº¦: {real_result['confidence']:.3f}")
    print(f"  æ¨ç†: {real_result['reasoning']}")
    print(f"  æ˜¯å¦ç‚ºçœŸå¯¦AI: {real_result['is_real_ai']}")
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆå¤§å°
    weights_file = "aiva_real_ai_core.pth"
    if Path(weights_file).exists():
        file_size = Path(weights_file).stat().st_size
        print("\nğŸ“ æ¬Šé‡æª”æ¡ˆè³‡è¨Š:")
        print(f"  æª”æ¡ˆ: {weights_file}")
        print(f"  å¤§å°: {file_size/1024/1024:.1f} MB")
        print(f"  åƒæ•¸: ~{real_engine.ai_core.total_params:,}")
    
    print("\nâœ… çœŸå¯¦AIæ ¸å¿ƒå‰µå»ºå®Œæˆï¼")
    print("   - ä½¿ç”¨PyTorchç¥ç¶“ç¶²è·¯ (ä¸æ˜¯MD5 hash)")
    print("   - çœŸå¯¦æ¬Šé‡æª”æ¡ˆ (~18MB, ä¸æ˜¯43KB)")
    print("   - çœŸæ­£çš„çŸ©é™£ä¹˜æ³•é‹ç®—")
    print("   - å¯è¨“ç·´çš„åƒæ•¸")

if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åŸ·è¡Œæ¸¬è©¦
    test_real_vs_fake_ai()