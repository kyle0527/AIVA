# âš™ï¸ Processing - çµæœè™•ç†ç³»çµ±

**å°èˆª**: [â† è¿”å› Core Capabilities](../README.md) | [â† è¿”å› AIVA Core](../../README.md)

> **ç‰ˆæœ¬**: 3.0.0-alpha  
> **ä»£ç¢¼é‡**: 1 å€‹ Python æª”æ¡ˆï¼Œç´„ 290 è¡Œä»£ç¢¼  
> **è§’è‰²**: AIVA çš„ã€Œæ•¸æ“šè™•ç†å™¨ã€- æƒæçµæœçš„æ™ºèƒ½åˆ†æå’Œè™•ç†

---

## ğŸ“‹ ç›®éŒ„

- [æ¨¡çµ„æ¦‚è¿°](#æ¨¡çµ„æ¦‚è¿°)
- [æª”æ¡ˆåˆ—è¡¨](#æª”æ¡ˆåˆ—è¡¨)
- [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
- [è™•ç†æµç¨‹](#è™•ç†æµç¨‹)
- [ä½¿ç”¨ç¯„ä¾‹](#ä½¿ç”¨ç¯„ä¾‹)

---

## ğŸ¯ æ¨¡çµ„æ¦‚è¿°

**Processing** å­æ¨¡çµ„è² è²¬è™•ç†å¾ Ingestion æ¨¡çµ„æ”å–çš„æƒæçµæœï¼ŒåŒ…æ‹¬å»é‡ã€å„ªå…ˆç´šæ’åºã€é¢¨éšªè©•ä¼°ã€é—œè¯åˆ†æå’Œçµæœèšåˆç­‰æ™ºèƒ½è™•ç†åŠŸèƒ½ã€‚

### æ ¸å¿ƒèƒ½åŠ›
1. **çµæœå»é‡** - è­˜åˆ¥å’Œåˆä½µé‡è¤‡çš„ç™¼ç¾
2. **å„ªå…ˆç´šæ’åº** - æ ¹æ“šåš´é‡ç¨‹åº¦å’Œå½±éŸ¿ç¯„åœæ’åº
3. **é¢¨éšªè©•ä¼°** - è¨ˆç®—ç¶œåˆé¢¨éšªè©•åˆ†
4. **é—œè¯åˆ†æ** - é—œè¯ç›¸é—œçš„æ¼æ´ç™¼ç¾
5. **çµæœèšåˆ** - ç”Ÿæˆçµ±è¨ˆå ±å‘Šå’Œæ‘˜è¦

---

## ğŸ“‚ æª”æ¡ˆåˆ—è¡¨

| æª”æ¡ˆå | è¡Œæ•¸ | æ ¸å¿ƒåŠŸèƒ½ | ç‹€æ…‹ |
|--------|------|----------|------|
| **scan_result_processor.py** | 290 | æƒæçµæœè™•ç†å™¨ - æ™ºèƒ½åˆ†æå’Œè™•ç† | âœ… ç”Ÿç”¢ |
| **__init__.py** | - | æ¨¡çµ„åˆå§‹åŒ– | - |

---

## ğŸ”§ æ ¸å¿ƒçµ„ä»¶

### ScanResultProcessor - æƒæçµæœè™•ç†å™¨

**æª”æ¡ˆ**: `scan_result_processor.py` (290 è¡Œ)

æä¾›æƒæçµæœçš„æ™ºèƒ½è™•ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å»é‡ã€æ’åºã€è©•ä¼°å’Œèšåˆã€‚

#### æ ¸å¿ƒé¡åˆ¥

```python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ProcessingAction(Enum):
    """è™•ç†å‹•ä½œ"""
    DEDUPLICATE = "deduplicate"     # å»é‡
    PRIORITIZE = "prioritize"       # å„ªå…ˆç´šæ’åº
    ASSESS_RISK = "assess_risk"     # é¢¨éšªè©•ä¼°
    CORRELATE = "correlate"         # é—œè¯åˆ†æ
    AGGREGATE = "aggregate"         # èšåˆçµ±è¨ˆ

@dataclass
class ProcessedResult:
    """è™•ç†å¾Œçš„çµæœ"""
    original_findings: List[Dict]
    deduplicated_findings: List[Dict]
    prioritized_findings: List[Dict]
    risk_scores: Dict[str, float]
    correlations: List[Dict]
    summary: Dict[str, Any]

class ScanResultProcessor:
    """æƒæçµæœè™•ç†å™¨
    
    åŠŸèƒ½:
    - å»é‡è™•ç†
    - å„ªå…ˆç´šæ’åº
    - é¢¨éšªè©•ä¼°
    - æ¼æ´é—œè¯
    - çµæœèšåˆ
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–è™•ç†å™¨"""
        self.config = config or {}
        self.dedup_threshold = self.config.get("dedup_similarity", 0.9)
        self.risk_weights = self.config.get("risk_weights", {
            "severity": 0.4,
            "exploitability": 0.3,
            "impact": 0.2,
            "confidence": 0.1
        })
    
    async def process(
        self,
        findings: List[Dict],
        actions: List[ProcessingAction] = None
    ) -> ProcessedResult:
        """è™•ç†æƒæçµæœ"""
        
    def deduplicate(self, findings: List[Dict]) -> List[Dict]:
        """å»é‡è™•ç†"""
        
    def prioritize(self, findings: List[Dict]) -> List[Dict]:
        """å„ªå…ˆç´šæ’åº"""
        
    def assess_risk(self, finding: Dict) -> float:
        """è©•ä¼°å–®å€‹ç™¼ç¾çš„é¢¨éšªè©•åˆ†"""
        
    def correlate(self, findings: List[Dict]) -> List[Dict]:
        """é—œè¯åˆ†æ"""
        
    def aggregate(self, findings: List[Dict]) -> Dict[str, Any]:
        """èšåˆçµ±è¨ˆ"""
```

---

## ğŸ”„ è™•ç†æµç¨‹

```
åŸå§‹ç™¼ç¾
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1. å»é‡è™•ç†     â”‚  ç›¸ä¼¼åº¦æª¢æ¸¬ â†’ åˆä½µé‡è¤‡é …
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å„ªå…ˆç´šæ’åº     â”‚  åš´é‡ç¨‹åº¦ + å¯åˆ©ç”¨æ€§ â†’ æ’åº
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. é¢¨éšªè©•ä¼°      â”‚  å¤šç¶­åº¦è©•åˆ† â†’ é¢¨éšªç­‰ç´š
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. é—œè¯åˆ†æ      â”‚  æ¼æ´éˆè­˜åˆ¥ â†’ æ”»æ“Šè·¯å¾‘
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. çµæœèšåˆ      â”‚  çµ±è¨ˆæ‘˜è¦ â†’ å¯è¦–åŒ–æ•¸æ“š
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    è™•ç†å®Œæˆ
```

---

## ğŸš€ ä½¿ç”¨ç¯„ä¾‹

### å®Œæ•´è™•ç†æµç¨‹

```python
from core_capabilities.processing import (
    ScanResultProcessor,
    ProcessingAction,
    ProcessedResult
)

# 1. åˆå§‹åŒ–è™•ç†å™¨
processor = ScanResultProcessor(config={
    "dedup_similarity": 0.85,  # ç›¸ä¼¼åº¦é–¾å€¼
    "risk_weights": {
        "severity": 0.4,
        "exploitability": 0.3,
        "impact": 0.2,
        "confidence": 0.1
    }
})

# 2. æº–å‚™åŸå§‹ç™¼ç¾æ•¸æ“š
findings = [
    {
        "id": "finding-001",
        "name": "SQL Injection",
        "severity": "high",
        "url": "https://example.com/login",
        "parameter": "username"
    },
    {
        "id": "finding-002",
        "name": "SQL Injection",  # é‡è¤‡
        "severity": "high",
        "url": "https://example.com/login",
        "parameter": "username"
    },
    {
        "id": "finding-003",
        "name": "XSS",
        "severity": "medium",
        "url": "https://example.com/search",
        "parameter": "query"
    }
]

# 3. åŸ·è¡Œè™•ç†
result = await processor.process(
    findings=findings,
    actions=[
        ProcessingAction.DEDUPLICATE,
        ProcessingAction.PRIORITIZE,
        ProcessingAction.ASSESS_RISK,
        ProcessingAction.CORRELATE,
        ProcessingAction.AGGREGATE
    ]
)

# 4. æŸ¥çœ‹è™•ç†çµæœ
print(f"åŸå§‹ç™¼ç¾æ•¸: {len(result.original_findings)}")
print(f"å»é‡å¾Œ: {len(result.deduplicated_findings)}")
print(f"\n=== å„ªå…ˆç´šæ’åº ===")
for i, finding in enumerate(result.prioritized_findings[:5], 1):
    risk_score = result.risk_scores.get(finding['id'], 0)
    print(f"{i}. [{finding['severity'].upper()}] {finding['name']}")
    print(f"   é¢¨éšªè©•åˆ†: {risk_score:.2f}")
    print(f"   URL: {finding['url']}")

print(f"\n=== çµ±è¨ˆæ‘˜è¦ ===")
summary = result.summary
print(f"ç¸½ç™¼ç¾æ•¸: {summary['total']}")
print(f"Critical: {summary['by_severity']['critical']}")
print(f"High: {summary['by_severity']['high']}")
print(f"Medium: {summary['by_severity']['medium']}")
print(f"Low: {summary['by_severity']['low']}")
```

### å»é‡è™•ç†

```python
# å»é‡ç®—æ³•
def deduplicate(findings: List[Dict]) -> List[Dict]:
    """åŸºæ–¼ç›¸ä¼¼åº¦çš„å»é‡
    
    æ¯”è¼ƒç¶­åº¦:
    - æ¼æ´é¡å‹ (name)
    - URL è·¯å¾‘
    - åƒæ•¸åç¨±
    - HTTP æ–¹æ³•
    """
    
    deduplicated = []
    seen_signatures = set()
    
    for finding in findings:
        # ç”Ÿæˆç‰¹å¾µç°½å
        signature = (
            finding.get('name', '').lower(),
            finding.get('url', '').split('?')[0],  # å¿½ç•¥æŸ¥è©¢åƒæ•¸
            finding.get('parameter', ''),
            finding.get('method', 'GET')
        )
        
        if signature not in seen_signatures:
            deduplicated.append(finding)
            seen_signatures.add(signature)
        else:
            # åˆä½µé‡è¤‡ç™¼ç¾çš„è­‰æ“š
            existing = next(f for f in deduplicated 
                          if self._match_signature(f, signature))
            if 'evidence' in finding:
                existing.setdefault('evidence', []).extend(finding['evidence'])
    
    return deduplicated

# ä½¿ç”¨
original_count = len(findings)
deduplicated = processor.deduplicate(findings)
print(f"å»é‡: {original_count} â†’ {len(deduplicated)} (-{original_count - len(deduplicated)})")
```

### é¢¨éšªè©•ä¼°

```python
def assess_risk(finding: Dict) -> float:
    """å¤šç¶­åº¦é¢¨éšªè©•åˆ† (0-10)
    
    è©•åˆ†å› å­:
    - åš´é‡ç¨‹åº¦ (40%)
    - å¯åˆ©ç”¨æ€§ (30%)
    - å½±éŸ¿ç¯„åœ (20%)
    - ç½®ä¿¡åº¦ (10%)
    """
    
    # 1. åš´é‡ç¨‹åº¦è©•åˆ†
    severity_scores = {
        "critical": 10,
        "high": 8,
        "medium": 5,
        "low": 3,
        "info": 1
    }
    severity_score = severity_scores.get(
        finding.get("severity", "info").lower(), 
        1
    )
    
    # 2. å¯åˆ©ç”¨æ€§è©•åˆ†
    exploitability = finding.get("exploitability", "medium")
    exploit_scores = {
        "high": 10,
        "medium": 6,
        "low": 3
    }
    exploit_score = exploit_scores.get(exploitability, 6)
    
    # 3. å½±éŸ¿ç¯„åœè©•åˆ†
    impact = finding.get("impact", "limited")
    impact_scores = {
        "complete": 10,
        "high": 8,
        "partial": 5,
        "limited": 3
    }
    impact_score = impact_scores.get(impact, 5)
    
    # 4. ç½®ä¿¡åº¦è©•åˆ†
    confidence = finding.get("confidence", "certain")
    confidence_scores = {
        "certain": 10,
        "firm": 8,
        "tentative": 5
    }
    confidence_score = confidence_scores.get(confidence, 8)
    
    # åŠ æ¬Šè¨ˆç®—
    weights = self.risk_weights
    final_score = (
        severity_score * weights["severity"] +
        exploit_score * weights["exploitability"] +
        impact_score * weights["impact"] +
        confidence_score * weights["confidence"]
    )
    
    return round(final_score, 2)

# æ‰¹é‡è©•ä¼°
for finding in findings:
    risk_score = processor.assess_risk(finding)
    finding["risk_score"] = risk_score
    print(f"{finding['name']}: {risk_score}/10")
```

### é—œè¯åˆ†æ

```python
def correlate(findings: List[Dict]) -> List[Dict]:
    """è­˜åˆ¥æ¼æ´ä¹‹é–“çš„é—œè¯é—œä¿‚
    
    é—œè¯é¡å‹:
    - æ”»æ“Šéˆ (ä¸€å€‹æ¼æ´å¯åˆ©ç”¨å¦ä¸€å€‹)
    - åŒæºæ¼æ´ (ç›¸åŒæ ¹æœ¬åŸå› )
    - çµ„åˆæ”»æ“Š (å¤šå€‹æ¼æ´çµ„åˆåˆ©ç”¨)
    """
    
    correlations = []
    
    # 1. è­˜åˆ¥æ”»æ“Šéˆ
    for i, finding_a in enumerate(findings):
        for finding_b in findings[i+1:]:
            if self._is_attack_chain(finding_a, finding_b):
                correlations.append({
                    "type": "attack_chain",
                    "findings": [finding_a["id"], finding_b["id"]],
                    "description": f"{finding_a['name']} å¯ç”¨æ–¼åˆ©ç”¨ {finding_b['name']}",
                    "severity": "high"
                })
    
    # 2. è­˜åˆ¥åŒæºæ¼æ´
    by_root_cause = {}
    for finding in findings:
        root = self._identify_root_cause(finding)
        by_root_cause.setdefault(root, []).append(finding)
    
    for root, related in by_root_cause.items():
        if len(related) > 1:
            correlations.append({
                "type": "common_root_cause",
                "findings": [f["id"] for f in related],
                "root_cause": root,
                "count": len(related)
            })
    
    # 3. è­˜åˆ¥çµ„åˆæ”»æ“Š
    # ä¾‹å¦‚: XSS + CSRF = å®Œæ•´æ”»æ“Šéˆ
    xss_findings = [f for f in findings if "xss" in f.get("name", "").lower()]
    csrf_findings = [f for f in findings if "csrf" in f.get("name", "").lower()]
    
    if xss_findings and csrf_findings:
        correlations.append({
            "type": "combined_attack",
            "findings": [xss_findings[0]["id"], csrf_findings[0]["id"]],
            "description": "XSS + CSRF å¯å¯¦ç¾å®Œæ•´çš„è·¨ç«™è«‹æ±‚å½é€ æ”»æ“Š",
            "severity": "critical"
        })
    
    return correlations

# ä½¿ç”¨
correlations = processor.correlate(findings)
print(f"\n=== ç™¼ç¾ {len(correlations)} å€‹é—œè¯ ===")
for corr in correlations:
    print(f"[{corr['type'].upper()}] {corr.get('description', '')}")
    print(f"  æ¶‰åŠç™¼ç¾: {', '.join(corr['findings'])}")
```

### çµæœèšåˆ

```python
def aggregate(findings: List[Dict]) -> Dict[str, Any]:
    """ç”Ÿæˆçµ±è¨ˆæ‘˜è¦"""
    
    from collections import Counter
    
    # æŒ‰åš´é‡ç¨‹åº¦çµ±è¨ˆ
    by_severity = Counter(f.get("severity", "info") for f in findings)
    
    # æŒ‰é¡å‹çµ±è¨ˆ
    by_type = Counter(f.get("name", "Unknown") for f in findings)
    
    # æŒ‰ URL çµ±è¨ˆ
    by_url = Counter(f.get("url", "Unknown") for f in findings)
    
    # é«˜é¢¨éšªç™¼ç¾
    high_risk = [
        f for f in findings 
        if f.get("risk_score", 0) >= 8.0
    ]
    
    # è¨ˆç®—å¹³å‡é¢¨éšªè©•åˆ†
    avg_risk = sum(f.get("risk_score", 0) for f in findings) / len(findings) if findings else 0
    
    return {
        "total": len(findings),
        "by_severity": dict(by_severity),
        "by_type": dict(by_type.most_common(10)),
        "by_url": dict(by_url.most_common(10)),
        "high_risk_count": len(high_risk),
        "average_risk_score": round(avg_risk, 2),
        "top_vulnerabilities": [
            {
                "name": f["name"],
                "url": f["url"],
                "risk": f.get("risk_score", 0)
            }
            for f in sorted(findings, key=lambda x: x.get("risk_score", 0), reverse=True)[:5]
        ]
    }

# ä½¿ç”¨
summary = processor.aggregate(findings)
print(json.dumps(summary, indent=2, ensure_ascii=False))
```

---

## ğŸ“Š è™•ç†çµ±è¨ˆç¤ºä¾‹

```json
{
  "total": 127,
  "deduplicated": 85,
  "by_severity": {
    "critical": 3,
    "high": 15,
    "medium": 42,
    "low": 20,
    "info": 5
  },
  "by_type": {
    "SQL Injection": 12,
    "XSS": 18,
    "CSRF": 8,
    "Authentication Bypass": 5,
    "Information Disclosure": 22
  },
  "high_risk_count": 18,
  "average_risk_score": 6.3,
  "correlations": [
    {
      "type": "attack_chain",
      "findings": ["finding-001", "finding-045"],
      "description": "XSS å¯ç”¨æ–¼ç¹é CSRF ä¿è­·"
    }
  ],
  "top_vulnerabilities": [
    {
      "name": "SQL Injection",
      "url": "https://api.example.com/login",
      "risk": 9.2
    },
    {
      "name": "Authentication Bypass",
      "url": "https://api.example.com/admin",
      "risk": 8.8
    }
  ]
}
```

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [Core Capabilities ä¸»æ–‡æª”](../README.md)
- [Ingestion å­æ¨¡çµ„](../ingestion/README.md) - æ•¸æ“šæ”å–
- [Output å­æ¨¡çµ„](../output/README.md) - è¼¸å‡ºè½‰æ›
- [Plugins å­æ¨¡çµ„](../plugins/README.md) - AI æ‘˜è¦æ’ä»¶

---

**ç‰ˆæ¬Šæ‰€æœ‰** Â© 2024 AIVA Project. ä¿ç•™æ‰€æœ‰æ¬Šåˆ©ã€‚
