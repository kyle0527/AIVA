# Training - è¨“ç·´ç·¨æ’

**å°èˆª**: [â† è¿”å› External Learning](../README.md) | [â† è¿”å› AIVA Core](../../README.md) | [â† è¿”å›é …ç›®æ ¹ç›®éŒ„](../../../../../README.md)

## ğŸ“‘ ç›®éŒ„

- [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸ“‚ æ–‡ä»¶çµæ§‹](#-æ–‡ä»¶çµæ§‹)
- [ğŸ¯ æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
  - [training_orchestrator.py](#training_orchestratorpy-1245-è¡Œ-)
  - [distributed_trainer.py](#distributed_trainerpy-633-è¡Œ-)
- [ğŸ”„ è¨“ç·´æµç¨‹](#-è¨“ç·´æµç¨‹)
- [ğŸ“Š è¨“ç·´ç›£æ§](#-è¨“ç·´ç›£æ§)
- [ğŸš€ æ€§èƒ½å„ªåŒ–](#-æ€§èƒ½å„ªåŒ–)
- [ğŸ”§ å®¹éŒ¯æ©Ÿåˆ¶](#-å®¹éŒ¯æ©Ÿåˆ¶)
- [ğŸ“š ç›¸é—œæ¨¡çµ„](#-ç›¸é—œæ¨¡çµ„)
- [ğŸ’¡ æœ€ä½³å¯¦è¸](#-æœ€ä½³å¯¦è¸)

---

## ğŸ“‹ æ¦‚è¿°

**å®šä½**: åˆ†å¸ƒå¼è¨“ç·´ç·¨æ’å’Œç®¡ç†  
**ç‹€æ…‹**: âœ… å·²å¯¦ç¾  
**æ–‡ä»¶æ•¸**: 2 å€‹ Python æ–‡ä»¶ (1,878 è¡Œ)

## ğŸ“‚ æ–‡ä»¶çµæ§‹

```
training/
â”œâ”€â”€ training_orchestrator.py (1,245 è¡Œ) â­â­â­ - è¨“ç·´ç·¨æ’å™¨
â”œâ”€â”€ distributed_trainer.py (633 è¡Œ) â­ - åˆ†å¸ƒå¼è¨“ç·´
â”œâ”€â”€ __init__.py
â””â”€â”€ README.md (æœ¬æ–‡æª”)
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### training_orchestrator.py (1,245 è¡Œ) â­â­â­

**è·è²¬**: çµ±ä¸€è¨“ç·´æµç¨‹ç·¨æ’å’Œèª¿åº¦

**æ ¸å¿ƒåŠŸèƒ½**:
- è¨“ç·´ä»»å‹™èª¿åº¦
- è³‡æºåˆ†é…
- ä¸¦è¡Œè¨“ç·´ç®¡ç†
- è¨“ç·´æµç¨‹è‡ªå‹•åŒ–

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.training import TrainingOrchestrator

orchestrator = TrainingOrchestrator()

# æäº¤è¨“ç·´ä»»å‹™
job = orchestrator.submit_training_job(
    name="capability_classifier_v2",
    config={
        "model": "random_forest",
        "data_path": "training_data.csv",
        "hyperparameters": {
            "n_estimators": 100,
            "max_depth": 20
        }
    },
    resources={
        "cpu": 4,
        "memory": "8GB",
        "gpu": 1
    }
)

# ç›£æ§è¨“ç·´ç‹€æ…‹
status = orchestrator.get_job_status(job.id)
# {"status": "running", "progress": 0.45, "eta": "10min"}

# ç­‰å¾…å®Œæˆ
orchestrator.wait_for_completion(job.id)

# ç²å–çµæœ
results = orchestrator.get_results(job.id)
```

**æ‰¹é‡è¨“ç·´**:
```python
# ä¸¦è¡Œè¨“ç·´å¤šå€‹æ¨¡å‹
jobs = orchestrator.submit_batch_training([
    {"model": "random_forest", "params": {...}},
    {"model": "svm", "params": {...}},
    {"model": "neural_network", "params": {...}}
])

# ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
orchestrator.wait_for_all(jobs)

# é¸æ“‡æœ€ä½³æ¨¡å‹
best_model = orchestrator.select_best_model(
    jobs,
    metric="f1_score"
)
```

**è¶…åƒæ•¸æœç´¢**:
```python
# è‡ªå‹•è¶…åƒæ•¸æœç´¢
search_job = orchestrator.hyperparameter_search(
    model="random_forest",
    param_space={
        "n_estimators": [50, 100, 200],
        "max_depth": [10, 20, 30],
        "min_samples_split": [2, 5, 10]
    },
    search_strategy="grid",  # or "random", "bayesian"
    metric="f1_score",
    cv=5
)
```

**è¨“ç·´æµæ°´ç·š**:
```python
# å®šç¾©è¨“ç·´æµæ°´ç·š
pipeline = orchestrator.create_pipeline([
    {"stage": "data_preprocessing", "script": "preprocess.py"},
    {"stage": "feature_engineering", "script": "features.py"},
    {"stage": "model_training", "script": "train.py"},
    {"stage": "model_evaluation", "script": "evaluate.py"},
    {"stage": "model_deployment", "script": "deploy.py"}
])

# åŸ·è¡Œæµæ°´ç·š
pipeline.run()
```

---

### distributed_trainer.py (633 è¡Œ) â­

**è·è²¬**: åˆ†å¸ƒå¼è¨“ç·´å¯¦ç¾

**æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥**:
- Data Parallelism (æ•¸æ“šä¸¦è¡Œ)
- Model Parallelism (æ¨¡å‹ä¸¦è¡Œ)
- Distributed Data Parallel (DDP)
- Horovod

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.training import DistributedTrainer

# åˆå§‹åŒ–åˆ†å¸ƒå¼è¨“ç·´
trainer = DistributedTrainer(
    backend="nccl",  # or "gloo", "mpi"
    num_gpus=4,
    strategy="ddp"
)

# åˆ†å¸ƒå¼è¨“ç·´
trainer.train(
    model=model,
    train_loader=train_loader,
    epochs=100,
    checkpoint_interval=10
)
```

**å¤šæ©Ÿè¨“ç·´**:
```python
# é…ç½®å¤šæ©Ÿè¨“ç·´
trainer = DistributedTrainer(
    backend="nccl",
    nodes=[
        {"host": "worker1", "gpus": [0, 1, 2, 3]},
        {"host": "worker2", "gpus": [0, 1, 2, 3]},
        {"host": "worker3", "gpus": [0, 1, 2, 3]}
    ],
    master="worker1:29500"
)

trainer.train(...)
```

**æ¢¯åº¦ç´¯ç©**:
```python
# ä½¿ç”¨æ¢¯åº¦ç´¯ç©è™•ç†å¤§æ‰¹é‡æ•¸æ“š
trainer = DistributedTrainer(
    accumulation_steps=4  # ç´¯ç© 4 æ­¥å¾Œæ›´æ–°
)
```

## ğŸ”„ è¨“ç·´æµç¨‹

```
ä»»å‹™æäº¤
  â†“
è³‡æºåˆ†é… (orchestrator)
  â†“
æ•¸æ“šåˆ†ç™¼ (distributed_trainer)
  â†“
ä¸¦è¡Œè¨“ç·´
  â†“
æ¢¯åº¦åŒæ­¥
  â†“
æ¨¡å‹æ›´æ–°
  â†“
Checkpoint ä¿å­˜
  â†“
è¨“ç·´å®Œæˆ
```

## ğŸ“Š è¨“ç·´ç›£æ§

```python
# å¯¦æ™‚ç›£æ§è¨“ç·´
from aiva_core.external_learning.training import TrainingMonitor

monitor = TrainingMonitor()

# ç›£æ§è³‡æºä½¿ç”¨
monitor.track_resource_usage(job.id)
# {"cpu": 75%, "memory": "6GB/8GB", "gpu": 90%}

# ç›£æ§è¨“ç·´æŒ‡æ¨™
monitor.track_metrics(job.id)
# {"epoch": 50, "loss": 0.3, "accuracy": 0.92}

# å¯è¦–åŒ–è¨“ç·´é€²åº¦
monitor.plot_training_progress(job.id)
```

## ğŸš€ æ€§èƒ½å„ªåŒ–

### 1. è‡ªå‹•æ··åˆç²¾åº¦ (AMP)

```python
trainer = DistributedTrainer(
    mixed_precision=True  # å•Ÿç”¨ FP16 è¨“ç·´
)
```

### 2. æ¢¯åº¦æª¢æŸ¥é»

```python
trainer = DistributedTrainer(
    gradient_checkpointing=True  # ç¯€çœ GPU å…§å­˜
)
```

### 3. æ•¸æ“šåŠ è¼‰å„ªåŒ–

```python
from torch.utils.data import DataLoader

loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,  # å¤šé€²ç¨‹æ•¸æ“šåŠ è¼‰
    pin_memory=True,  # å›ºå®šå…§å­˜
    prefetch_factor=2  # é å–æ•¸æ“š
)
```

## ğŸ”§ å®¹éŒ¯æ©Ÿåˆ¶

```python
# è‡ªå‹• Checkpoint å’Œæ¢å¾©
orchestrator = TrainingOrchestrator(
    checkpoint_dir="/checkpoints",
    checkpoint_interval=10,  # æ¯ 10 å€‹ epoch
    auto_resume=True  # è‡ªå‹•æ¢å¾©
)

# è¨“ç·´ä¸­æ–·å¾Œè‡ªå‹•æ¢å¾©
orchestrator.submit_training_job(..., resume_from_checkpoint=True)
```

## ğŸ“š ç›¸é—œæ¨¡çµ„

- [learning](../learning/README.md) - å­¸ç¿’å¼•æ“
- [tracing](../tracing/README.md) - è¨“ç·´è¿½è¹¤
- [service_backbone/messaging](../../service_backbone/messaging/README.md) - ä»»å‹™åˆ†ç™¼

---

## ğŸ”¨ aiva_common ä¿®å¾©è¦ç¯„

> **æ ¸å¿ƒåŸå‰‡**: æœ¬æ¨¡çµ„å¿…é ˆåš´æ ¼éµå¾ª [`services/aiva_common`](../../../../aiva_common/README.md#-é–‹ç™¼æŒ‡å—) çš„ä¿®å¾©è¦ç¯„ã€‚

```python
# âœ… æ­£ç¢ºï¼šä½¿ç”¨æ¨™æº–é¡å‹
from aiva_common import TaskStatus, AivaMessage

# âŒ ç¦æ­¢ï¼šè‡ªå®šç¾©è¨“ç·´ç‹€æ…‹
class TrainingJobStatus(str, Enum): pass
```

ğŸ“– **å®Œæ•´è¦ç¯„**: [aiva_common ä¿®å¾©æŒ‡å—](../../../../aiva_common/README.md#-é–‹ç™¼è¦ç¯„èˆ‡æœ€ä½³å¯¦è¸)

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### 1. è¨“ç·´é…ç½®ç®¡ç†

```yaml
# training_config.yaml
model:
  type: random_forest
  hyperparameters:
    n_estimators: 100
    max_depth: 20

data:
  train_path: data/train.csv
  val_path: data/val.csv
  batch_size: 32

training:
  epochs: 100
  learning_rate: 0.001
  early_stopping:
    patience: 10
    metric: val_loss

resources:
  gpus: 2
  memory: 16GB
```

### 2. åˆ†å¸ƒå¼è¨“ç·´å•Ÿå‹•

```python
# å–®æ©Ÿå¤šå¡
python train.py --distributed --gpus 4

# å¤šæ©Ÿå¤šå¡
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=3 \
    --node_rank=0 \
    --master_addr="worker1" \
    --master_port=29500 \
    train.py
```

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0  
**æœ€å¾Œæ›´æ–°**: 2025-11-16  
**ç¶­è­·è€…**: External Learning åœ˜éšŠ
