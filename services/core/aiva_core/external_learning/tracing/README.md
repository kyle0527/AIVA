# Tracing - è¨“ç·´è¿½è¹¤

**å°èˆª**: [â† è¿”å› External Learning](../README.md) | [â† è¿”å› AIVA Core](../../README.md) | [â† è¿”å›é …ç›®æ ¹ç›®éŒ„](../../../../../README.md)

## ğŸ“‘ ç›®éŒ„

- [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸ“‚ æ–‡ä»¶çµæ§‹](#-æ–‡ä»¶çµæ§‹)
- [ğŸ¯ æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
  - [experiment_tracker.py](#experiment_trackerpy-342-è¡Œ-)
  - [metrics_logger.py](#metrics_loggerpy-228-è¡Œ)
  - [model_versioning.py](#model_versioningpy-127-è¡Œ)
- [ğŸ“Š å¯¦é©—è¿½è¹¤æœ€ä½³å¯¦è¸](#-å¯¦é©—è¿½è¹¤æœ€ä½³å¯¦è¸)
- [ğŸ” MLflow é›†æˆ](#-mlflow-é›†æˆ)
- [ğŸ“ˆ å¯è¦–åŒ–](#-å¯è¦–åŒ–)
- [ğŸ“š ç›¸é—œæ¨¡çµ„](#-ç›¸é—œæ¨¡çµ„)

---

## ğŸ“‹ æ¦‚è¿°

**å®šä½**: è¨“ç·´éç¨‹è¿½è¹¤å’Œå¯¦é©—ç®¡ç†  
**ç‹€æ…‹**: âœ… å·²å¯¦ç¾  
**æ–‡ä»¶æ•¸**: 3 å€‹ Python æ–‡ä»¶ (697 è¡Œ)

## ğŸ“‚ æ–‡ä»¶çµæ§‹

```
tracing/
â”œâ”€â”€ experiment_tracker.py (342 è¡Œ) â­ - å¯¦é©—è¿½è¹¤
â”œâ”€â”€ metrics_logger.py (228 è¡Œ) - æŒ‡æ¨™è¨˜éŒ„
â”œâ”€â”€ model_versioning.py (127 è¡Œ) - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
â”œâ”€â”€ __init__.py
â””â”€â”€ README.md (æœ¬æ–‡æª”)
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### experiment_tracker.py (342 è¡Œ) â­

**è·è²¬**: è¿½è¹¤å’Œç®¡ç†æ©Ÿå™¨å­¸ç¿’å¯¦é©—

**åŠŸèƒ½**:
- è¨˜éŒ„è¶…åƒæ•¸
- è¿½è¹¤æŒ‡æ¨™
- æ¯”è¼ƒå¯¦é©—çµæœ
- å¯¦é©—å¯è¦–åŒ–

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.tracing import ExperimentTracker

tracker = ExperimentTracker(experiment_name="capability_classifier")

# è¨˜éŒ„è¶…åƒæ•¸
tracker.log_params({
    "model": "random_forest",
    "n_estimators": 100,
    "max_depth": 20
})

# è¨˜éŒ„æŒ‡æ¨™
tracker.log_metrics({
    "accuracy": 0.95,
    "f1_score": 0.93,
    "training_time": 120
})

# è¨˜éŒ„æ¨¡å‹
tracker.log_model(model, "classifier_v1.pkl")

# æ¯”è¼ƒå¯¦é©—
comparison = tracker.compare_experiments([
    "experiment_1",
    "experiment_2",
    "experiment_3"
])
```

**å¯¦é©—ç®¡ç†**:
```python
# åˆ—å‡ºæ‰€æœ‰å¯¦é©—
experiments = tracker.list_experiments()

# åŠ è¼‰æ­·å²å¯¦é©—
exp = tracker.load_experiment("experiment_id_123")

# æ¢å¾©æœ€ä½³æ¨¡å‹
best_model = tracker.load_best_model(metric="f1_score")
```

---

### metrics_logger.py (228 è¡Œ)

**è·è²¬**: è¨“ç·´æŒ‡æ¨™å¯¦æ™‚è¨˜éŒ„

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.tracing import MetricsLogger

logger = MetricsLogger()

# è¨˜éŒ„æ¯å€‹ epoch çš„æŒ‡æ¨™
for epoch in range(100):
    loss = train_one_epoch()
    val_accuracy = validate()
    
    logger.log_metric("loss", loss, step=epoch)
    logger.log_metric("val_accuracy", val_accuracy, step=epoch)

# å¯è¦–åŒ–æŒ‡æ¨™
logger.plot_metrics()

# å°å‡ºæŒ‡æ¨™
logger.export_to_csv("training_metrics.csv")
```

**æ”¯æŒçš„å¾Œç«¯**:
- MLflow
- TensorBoard
- Weights & Biases
- æœ¬åœ°æ–‡ä»¶ç³»çµ±

---

### model_versioning.py (127 è¡Œ)

**è·è²¬**: æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.tracing import ModelVersioning

versioning = ModelVersioning(model_name="capability_classifier")

# ä¿å­˜æ–°ç‰ˆæœ¬
versioning.save_version(
    model=trained_model,
    version="v1.2.0",
    metadata={
        "training_data": "dataset_2025_01",
        "accuracy": 0.96,
        "notes": "å¢åŠ äº†æ–°çš„èƒ½åŠ›é¡åˆ¥"
    }
)

# åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬
versions = versioning.list_versions()
# [
#   {"version": "v1.0.0", "date": "2025-01-01", "accuracy": 0.92},
#   {"version": "v1.1.0", "date": "2025-02-15", "accuracy": 0.94},
#   {"version": "v1.2.0", "date": "2025-03-20", "accuracy": 0.96}
# ]

# åŠ è¼‰ç‰¹å®šç‰ˆæœ¬
model_v1 = versioning.load_version("v1.0.0")

# å›æ»¾åˆ°ä¸Šä¸€ç‰ˆæœ¬
versioning.rollback()
```

## ğŸ“Š å¯¦é©—è¿½è¹¤æœ€ä½³å¯¦è¸

### 1. å®Œæ•´è¨˜éŒ„

```python
tracker = ExperimentTracker("my_experiment")

# è¨˜éŒ„ç’°å¢ƒä¿¡æ¯
tracker.log_system_info()

# è¨˜éŒ„æ•¸æ“šé›†
tracker.log_dataset_info({
    "name": "training_data_2025",
    "size": 10000,
    "features": 50
})

# è¨˜éŒ„è¶…åƒæ•¸
tracker.log_params({
    "model": "random_forest",
    "n_estimators": 100,
    "max_depth": 20,
    "learning_rate": 0.01
})

# è¨˜éŒ„æŒ‡æ¨™
tracker.log_metrics({
    "train_accuracy": 0.98,
    "val_accuracy": 0.95,
    "test_accuracy": 0.94
})
```

### 2. è‡ªå‹•è¿½è¹¤

```python
# ä½¿ç”¨è£é£¾å™¨è‡ªå‹•è¿½è¹¤
@tracker.track_experiment
def train_model(params):
    model = create_model(params)
    model.train(...)
    return model, metrics
```

### 3. å¯¦é©—å°æ¯”

```python
# å°æ¯”å¤šå€‹å¯¦é©—
comparison = tracker.compare_experiments(
    experiment_ids=["exp1", "exp2", "exp3"],
    metrics=["accuracy", "f1_score", "training_time"]
)

# ç”Ÿæˆå°æ¯”å ±å‘Š
tracker.generate_comparison_report(comparison, output="report.html")
```

## ğŸ” MLflow é›†æˆ

```python
import mlflow
from aiva_core.external_learning.tracing import ExperimentTracker

# ä½¿ç”¨ MLflow å¾Œç«¯
tracker = ExperimentTracker(
    backend="mlflow",
    tracking_uri="http://mlflow-server:5000"
)

with mlflow.start_run():
    tracker.log_params(params)
    tracker.log_metrics(metrics)
    tracker.log_model(model)
```

## ğŸ“ˆ å¯è¦–åŒ–

```python
# è¨“ç·´æ›²ç·š
tracker.plot_training_curve(metric="loss")

# æŒ‡æ¨™å°æ¯”
tracker.plot_metric_comparison(
    experiments=["exp1", "exp2"],
    metric="accuracy"
)

# è¶…åƒæ•¸é‡è¦æ€§
tracker.plot_hyperparameter_importance()
```

## ğŸ“š ç›¸é—œæ¨¡çµ„

- [learning](../learning/README.md) - å­¸ç¿’å¼•æ“
- [training](../training/README.md) - è¨“ç·´ç·¨æ’

---

## ğŸ”¨ aiva_common ä¿®å¾©è¦ç¯„

> **æ ¸å¿ƒåŸå‰‡**: æœ¬æ¨¡çµ„å¿…é ˆåš´æ ¼éµå¾ª [`services/aiva_common`](../../../../aiva_common/README.md#-é–‹ç™¼æŒ‡å—) çš„ä¿®å¾©è¦ç¯„ã€‚

```python
# âœ… æ­£ç¢ºï¼šä½¿ç”¨æ¨™æº–é¡å‹
from aiva_common import TaskStatus

# âŒ ç¦æ­¢ï¼šè‡ªå®šç¾©è¿½è¹¤ç‹€æ…‹
class TraceStatus(str, Enum): pass
```

ğŸ“– **å®Œæ•´è¦ç¯„**: [aiva_common ä¿®å¾©æŒ‡å—](../../../../aiva_common/README.md#-é–‹ç™¼è¦ç¯„èˆ‡æœ€ä½³å¯¦è¸)

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0  
**æœ€å¾Œæ›´æ–°**: 2025-11-16  
**ç¶­è­·è€…**: External Learning åœ˜éšŠ
