# Learning - å­¸ç¿’å¼•æ“

**å°èˆª**: [â† è¿”å› External Learning](../README.md) | [â† è¿”å› AIVA Core](../../README.md) | [â† è¿”å›é …ç›®æ ¹ç›®éŒ„](../../../../../README.md)

## ğŸ“‘ ç›®éŒ„

- [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸ“‚ æ–‡ä»¶çµæ§‹](#-æ–‡ä»¶çµæ§‹)
- [ğŸ¯ æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
  - [model_trainer.py](#model_trainerpy-892-è¡Œ-)
  - [learning_engine.py](#learning_enginepy-645-è¡Œ-)
  - [reinforcement_learning.py](#reinforcement_learningpy-312-è¡Œ)
  - [transfer_learning.py](#transfer_learningpy-172-è¡Œ)
- [ğŸ”„ å­¸ç¿’æµç¨‹](#-å­¸ç¿’æµç¨‹)
- [ğŸ“Š è¨“ç·´ç›£æ§](#-è¨“ç·´ç›£æ§)
- [ğŸ“š ç›¸é—œæ¨¡çµ„](#-ç›¸é—œæ¨¡çµ„)

---

## ğŸ“‹ æ¦‚è¿°

**å®šä½**: æ©Ÿå™¨å­¸ç¿’æ ¸å¿ƒå¼•æ“  
**ç‹€æ…‹**: âœ… å·²å¯¦ç¾  
**æ–‡ä»¶æ•¸**: 4 å€‹ Python æ–‡ä»¶ (2,021 è¡Œ)

## ğŸ“‚ æ–‡ä»¶çµæ§‹

```
learning/
â”œâ”€â”€ model_trainer.py (892 è¡Œ) â­â­ - æ¨¡å‹è¨“ç·´å™¨
â”œâ”€â”€ learning_engine.py (645 è¡Œ) â­ - å­¸ç¿’å¼•æ“
â”œâ”€â”€ reinforcement_learning.py (312 è¡Œ) - å¼·åŒ–å­¸ç¿’
â”œâ”€â”€ transfer_learning.py (172 è¡Œ) - é·ç§»å­¸ç¿’
â”œâ”€â”€ __init__.py
â””â”€â”€ README.md (æœ¬æ–‡æª”)
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### model_trainer.py (892 è¡Œ) â­â­

**è·è²¬**: çµ±ä¸€æ¨¡å‹è¨“ç·´æ¥å£

**æ”¯æŒçš„è¨“ç·´æ¨¡å¼**:
- ç›£ç£å­¸ç¿’ (Supervised Learning)
- ç„¡ç›£ç£å­¸ç¿’ (Unsupervised Learning)
- åŠç›£ç£å­¸ç¿’ (Semi-supervised Learning)
- åœ¨ç·šå­¸ç¿’ (Online Learning)

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.learning import ModelTrainer

trainer = ModelTrainer(
    model_type="random_forest",
    task="classification"
)

# è¨“ç·´æ¨¡å‹
trainer.train(
    X_train=training_data,
    y_train=labels,
    validation_split=0.2,
    epochs=100,
    early_stopping=True
)

# è©•ä¼°æ¨¡å‹
metrics = trainer.evaluate(X_test, y_test)
# {"accuracy": 0.95, "f1_score": 0.93, "precision": 0.94, "recall": 0.92}

# ä¿å­˜æ¨¡å‹
trainer.save("trained_model.pkl")
```

**è¨“ç·´å›èª¿**:
```python
# è‡ªå®šç¾©è¨“ç·´å›èª¿
class CustomCallback:
    def on_epoch_end(self, epoch, metrics):
        print(f"Epoch {epoch}: {metrics}")

trainer.train(..., callbacks=[CustomCallback()])
```

---

### learning_engine.py (645 è¡Œ) â­

**è·è²¬**: å­¸ç¿’æµç¨‹ç·¨æ’å’Œç®¡ç†

**åŠŸèƒ½**:
- è‡ªå‹•è¶…åƒæ•¸èª¿å„ª
- æ¨¡å‹é¸æ“‡
- äº¤å‰é©—è­‰
- ç‰¹å¾µå·¥ç¨‹

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.learning import LearningEngine

engine = LearningEngine()

# è‡ªå‹•è¨“ç·´ (è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹)
best_model = engine.auto_train(
    data=training_data,
    target="label",
    task="classification",
    metric="f1_score"
)

# è¶…åƒæ•¸èª¿å„ª
engine.tune_hyperparameters(
    model="random_forest",
    param_grid={
        "n_estimators": [100, 200, 300],
        "max_depth": [10, 20, 30]
    },
    cv=5  # 5-fold äº¤å‰é©—è­‰
)
```

---

### reinforcement_learning.py (312 è¡Œ)

**è·è²¬**: å¼·åŒ–å­¸ç¿’ç®—æ³•å¯¦ç¾

**æ”¯æŒç®—æ³•**:
- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.learning import RLAgent

# å‰µå»º RL æ™ºèƒ½é«”
agent = RLAgent(
    algorithm="dqn",
    state_dim=10,
    action_dim=4
)

# è¨“ç·´
for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)
        agent.learn(state, action, reward, next_state, done)
        state = next_state
```

---

### transfer_learning.py (172 è¡Œ)

**è·è²¬**: é·ç§»å­¸ç¿’å’Œæ¨¡å‹å¾®èª¿

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from aiva_core.external_learning.learning import TransferLearner

# åŠ è¼‰é è¨“ç·´æ¨¡å‹
learner = TransferLearner.from_pretrained("bert-base")

# å¾®èª¿
learner.fine_tune(
    train_data=new_data,
    epochs=5,
    freeze_layers=True  # å‡çµåº•å±¤
)
```

## ğŸ”„ å­¸ç¿’æµç¨‹

```
æ•¸æ“šæ”¶é›†
  â†“
æ•¸æ“šé è™•ç† (analysis/data_preprocessor.py)
  â†“
ç‰¹å¾µæå– (analysis/feature_analyzer.py)
  â†“
æ¨¡å‹è¨“ç·´ (model_trainer.py)
  â†“
æ¨¡å‹è©•ä¼°
  â†“
è¶…åƒæ•¸èª¿å„ª (learning_engine.py)
  â†“
æ¨¡å‹éƒ¨ç½²
```

## ğŸ“Š è¨“ç·´ç›£æ§

```python
from aiva_core.external_learning.learning import TrainingMonitor

monitor = TrainingMonitor()

# å¯¦æ™‚ç›£æ§è¨“ç·´
trainer.train(..., monitor=monitor)

# æŸ¥çœ‹è¨“ç·´æ›²ç·š
monitor.plot_metrics()

# ç²å–è¨“ç·´æ­·å²
history = monitor.get_history()
# {
#   "epoch": [1, 2, 3, ...],
#   "loss": [0.5, 0.3, 0.2, ...],
#   "accuracy": [0.8, 0.9, 0.95, ...]
# }
```

## ğŸ“š ç›¸é—œæ¨¡çµ„

- [training](../training/README.md) - è¨“ç·´ç·¨æ’
- [tracing](../tracing/README.md) - è¨“ç·´è¿½è¹¤
- [analysis](../analysis/README.md) - æ•¸æ“šåˆ†æ

---

## ğŸ”¨ aiva_common ä¿®å¾©è¦ç¯„

> **æ ¸å¿ƒåŸå‰‡**: æœ¬æ¨¡çµ„å¿…é ˆåš´æ ¼éµå¾ª [`services/aiva_common`](../../../../aiva_common/README.md#-é–‹ç™¼æŒ‡å—) çš„ä¿®å¾©è¦ç¯„ã€‚

```python
# âœ… æ­£ç¢ºï¼šä½¿ç”¨æ¨™æº–é¡å‹
from aiva_common import TaskStatus, ModuleName

# âŒ ç¦æ­¢ï¼šè‡ªå®šç¾©å­¸ç¿’ç‹€æ…‹
class TrainingStatus(str, Enum): pass
```

ğŸ“– **å®Œæ•´è¦ç¯„**: [aiva_common ä¿®å¾©æŒ‡å—](../../../../aiva_common/README.md#-é–‹ç™¼è¦ç¯„èˆ‡æœ€ä½³å¯¦è¸)

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0  
**æœ€å¾Œæ›´æ–°**: 2025-11-16  
**ç¶­è­·è€…**: External Learning åœ˜éšŠ
