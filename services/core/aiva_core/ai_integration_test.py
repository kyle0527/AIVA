#!/usr/bin/env python3
"""
AIVA AI æ•´åˆæ¸¬è©¦ç³»çµ±
æ¸¬è©¦çµ±ä¸€ AI æ§åˆ¶å™¨ã€è‡ªç„¶èªè¨€ç”Ÿæˆå™¨å’Œå¤šèªè¨€å”èª¿å™¨çš„æ•´åˆæ•ˆæœ
"""

import asyncio
from dataclasses import dataclass
import json
import os
from pathlib import Path
import sys
import time
from typing import Any

sys.path.append(str(Path(__file__).parent.parent.parent))

from services.core.aiva_core.ai_controller import UnifiedAIController
from services.core.aiva_core.ai_engine.bio_neuron_core import BioNeuronRAGAgent
from services.core.aiva_core.multilang_coordinator import MultiLanguageAICoordinator
from services.core.aiva_core.nlg_system import AIVANaturalLanguageGenerator


@dataclass
class IntegrationTestResult:
    """æ¸¬è©¦çµæœæ•¸æ“šé¡"""

    test_name: str
    success: bool
    execution_time: float
    details: dict[str, Any]
    error_message: str | None = None


class AIIntegrationTester:
    """AI æ•´åˆæ¸¬è©¦å™¨"""

    def __init__(self, aiva_root: str):
        """
        åˆå§‹åŒ–æ¸¬è©¦å™¨

        Args:
            aiva_root: AIVA æ ¹ç›®éŒ„è·¯å¾‘
        """
        self.aiva_root = Path(aiva_root)
        self.test_results: list[IntegrationTestResult] = []

        # åˆå§‹åŒ–å„ AI çµ„ä»¶
        self.bio_agent = BioNeuronRAGAgent(str(self.aiva_root))
        self.unified_controller = UnifiedAIController(str(self.aiva_root))
        self.nlg_generator = AIVANaturalLanguageGenerator()
        self.multilang_coordinator = MultiLanguageAICoordinator()

        print("ğŸš€ AI æ•´åˆæ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")

    async def run_all_tests(self) -> dict[str, Any]:
        """
        åŸ·è¡Œæ‰€æœ‰æ•´åˆæ¸¬è©¦

        Returns:
            æ¸¬è©¦çµæœçµ±è¨ˆ
        """
        print("ğŸ“‹ é–‹å§‹åŸ·è¡Œ AI æ•´åˆæ¸¬è©¦å¥—ä»¶...")
        start_time = time.time()

        # æ¸¬è©¦æ¸…å–®
        tests = [
            ("åŸºç¤çµ„ä»¶åˆå§‹åŒ–æ¸¬è©¦", self._test_component_initialization),
            ("çµ±ä¸€æ§åˆ¶å™¨å”èª¿æ¸¬è©¦", self._test_unified_controller),
            ("è‡ªç„¶èªè¨€ç”Ÿæˆæ¸¬è©¦", self._test_nlg_system),
            ("å¤šèªè¨€å”èª¿æ¸¬è©¦", self._test_multilang_coordination),
            ("AI è¡çªæª¢æ¸¬æ¸¬è©¦", self._test_ai_conflict_detection),
            ("ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦", self._test_end_to_end_integration),
            ("æ•ˆèƒ½å£“åŠ›æ¸¬è©¦", self._test_performance_stress),
            ("éŒ¯èª¤æ¢å¾©æ¸¬è©¦", self._test_error_recovery),
        ]

        for test_name, test_func in tests:
            print(f"\nğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
            try:
                await test_func()
                print(f"âœ… {test_name} - é€šé")
            except Exception as e:
                print(f"âŒ {test_name} - å¤±æ•—: {str(e)}")

        total_time = time.time() - start_time
        return self._generate_test_report(total_time)

    async def _test_component_initialization(self):
        """æ¸¬è©¦åŸºç¤çµ„ä»¶åˆå§‹åŒ–"""
        start_time = time.time()

        # æ¸¬è©¦å„çµ„ä»¶æ˜¯å¦æ­£å¸¸åˆå§‹åŒ–
        components = {
            "BioNeuronRAGAgent": self.bio_agent,
            "UnifiedAIController": self.unified_controller,
            "AIVANaturalLanguageGenerator": self.nlg_generator,
            "MultiLanguageAICoordinator": self.multilang_coordinator,
        }

        details = {}
        for name, component in components.items():
            if hasattr(component, "is_ready"):
                is_ready = component.is_ready()
            else:
                is_ready = component is not None

            details[name] = {"initialized": component is not None, "ready": is_ready}

        execution_time = time.time() - start_time

        all_ready = all(detail["ready"] for detail in details.values())

        self.test_results.append(
            IntegrationTestResult(
                test_name="åŸºç¤çµ„ä»¶åˆå§‹åŒ–æ¸¬è©¦",
                success=all_ready,
                execution_time=execution_time,
                details=details,
                error_message=None if all_ready else "éƒ¨åˆ†çµ„ä»¶æœªæº–å‚™å°±ç·’",
            )
        )

    async def _test_unified_controller(self):
        """æ¸¬è©¦çµ±ä¸€æ§åˆ¶å™¨å”èª¿èƒ½åŠ›"""
        start_time = time.time()

        # æ¸¬è©¦çµ±ä¸€æ§åˆ¶å™¨è™•ç†è¤‡é›œè«‹æ±‚
        test_requests = [
            {
                "type": "security_analysis",
                "content": "åˆ†æé€™æ®µä»£ç¢¼çš„ SQL æ³¨å…¥æ¼æ´",
                "priority": "high",
                "language": "python",
            },
            {
                "type": "vulnerability_scan",
                "content": "æƒæ SSRF æ¼æ´",
                "priority": "medium",
                "language": "go",
            },
        ]

        details = {}
        success_count = 0

        for i, request in enumerate(test_requests):
            try:
                response = await self.unified_controller.process_unified_request(
                    request
                )
                details[f"request_{i+1}"] = {
                    "success": True,
                    "response_type": type(response).__name__,
                    "has_result": response is not None,
                }
                success_count += 1
            except Exception as e:
                details[f"request_{i+1}"] = {"success": False, "error": str(e)}

        execution_time = time.time() - start_time
        success = success_count == len(test_requests)

        self.test_results.append(
            IntegrationTestResult(
                test_name="çµ±ä¸€æ§åˆ¶å™¨å”èª¿æ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_nlg_system(self):
        """æ¸¬è©¦è‡ªç„¶èªè¨€ç”Ÿæˆç³»çµ±"""
        start_time = time.time()

        # æ¸¬è©¦ä¸åŒé¡å‹çš„è‡ªç„¶èªè¨€ç”Ÿæˆ
        test_contexts = [
            {
                "type": "vulnerability_report",
                "severity": "high",
                "vulnerability_type": "SQLæ³¨å…¥",
                "affected_files": ["user_controller.py", "database.py"],
            },
            {
                "type": "scan_summary",
                "total_files": 156,
                "vulnerabilities_found": 3,
                "scan_duration": 45.2,
            },
        ]

        details = {}
        success_count = 0

        for i, context in enumerate(test_contexts):
            try:
                response = self.nlg_generator.generate_response(context)
                details[f"generation_{i+1}"] = {
                    "success": True,
                    "response_length": len(response),
                    "has_chinese": any(
                        "\u4e00" <= char <= "\u9fff" for char in response
                    ),
                    "template_type": context["type"],
                }
                success_count += 1
            except Exception as e:
                details[f"generation_{i+1}"] = {"success": False, "error": str(e)}

        execution_time = time.time() - start_time
        success = success_count == len(test_contexts)

        self.test_results.append(
            IntegrationTestResult(
                test_name="è‡ªç„¶èªè¨€ç”Ÿæˆæ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_multilang_coordination(self):
        """æ¸¬è©¦å¤šèªè¨€å”èª¿ç³»çµ±"""
        start_time = time.time()

        # æ¨¡æ“¬å¤šèªè¨€ AI ä»»å‹™
        test_task = {
            "task_id": "test_multilang_001",
            "description": "è·¨èªè¨€æ¼æ´æª¢æ¸¬ä»»å‹™",
            "target_languages": ["python", "go", "rust"],
            "priority": "medium",
        }

        details = {}

        try:
            # æ¸¬è©¦ä»»å‹™åˆ†é…
            coordination_result = (
                await self.multilang_coordinator.coordinate_multi_language_ai_task(
                    test_task
                )
            )

            details["task_distribution"] = {
                "success": True,
                "languages_coordinated": len(
                    coordination_result.get("language_results", {})
                ),
                "execution_successful": coordination_result.get("success", False),
            }

            success = True
        except Exception as e:
            details["task_distribution"] = {"success": False, "error": str(e)}
            success = False

        execution_time = time.time() - start_time

        self.test_results.append(
            IntegrationTestResult(
                test_name="å¤šèªè¨€å”èª¿æ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_ai_conflict_detection(self):
        """æ¸¬è©¦ AI è¡çªæª¢æ¸¬"""
        start_time = time.time()

        # æ¨¡æ“¬ä¸¦ç™¼ AI è«‹æ±‚æ¸¬è©¦è¡çªæª¢æ¸¬
        concurrent_requests = [
            {"type": "sast_analysis", "target": "file1.py"},
            {"type": "sast_analysis", "target": "file1.py"},  # ç›¸åŒç›®æ¨™ï¼Œæ‡‰è©²åˆä½µ
            {"type": "dast_scan", "target": "endpoint1"},
        ]

        details = {}

        try:
            # åŒæ™‚ç™¼é€å¤šå€‹è«‹æ±‚
            tasks = []
            for request in concurrent_requests:
                task = asyncio.create_task(
                    self.unified_controller.process_unified_request(request)
                )
                tasks.append(task)

            responses = await asyncio.gather(*tasks, return_exceptions=True)

            # åˆ†æè¡çªè™•ç†çµæœ
            successful_responses = [
                r for r in responses if not isinstance(r, Exception)
            ]

            details["conflict_handling"] = {
                "total_requests": len(concurrent_requests),
                "successful_responses": len(successful_responses),
                "conflicts_detected": len(concurrent_requests)
                - len(successful_responses),
                "deduplication_working": len(
                    {str(r) for r in successful_responses if r}
                )
                < len(concurrent_requests),
            }

            success = len(successful_responses) > 0

        except Exception as e:
            details["conflict_handling"] = {"success": False, "error": str(e)}
            success = False

        execution_time = time.time() - start_time

        self.test_results.append(
            IntegrationTestResult(
                test_name="AI è¡çªæª¢æ¸¬æ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_end_to_end_integration(self):
        """æ¸¬è©¦ç«¯åˆ°ç«¯æ•´åˆ"""
        start_time = time.time()

        # å®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦æµç¨‹
        test_scenario = {
            "input": "è«‹åˆ†æå°ˆæ¡ˆä¸­çš„å®‰å…¨æ¼æ´ä¸¦ç”Ÿæˆä¸­æ–‡å ±å‘Š",
            "expected_steps": [
                "ä»»å‹™è§£æ",
                "AI å”èª¿",
                "æ¼æ´æª¢æ¸¬",
                "çµæœæ•´åˆ",
                "ä¸­æ–‡å ±å‘Šç”Ÿæˆ",
            ],
        }

        details = {}

        try:
            # 1. é€éçµ±ä¸€æ§åˆ¶å™¨è™•ç†è¤‡é›œè«‹æ±‚
            complex_request = {
                "type": "comprehensive_security_analysis",
                "content": test_scenario["input"],
                "output_format": "chinese_report",
                "include_recommendations": True,
            }

            controller_response = await self.unified_controller.process_unified_request(
                complex_request
            )

            # 2. ä½¿ç”¨ NLG ç³»çµ±ç”Ÿæˆæœ€çµ‚å ±å‘Š
            if controller_response:
                nlg_context = {
                    "type": "comprehensive_report",
                    "analysis_result": controller_response,
                    "language": "chinese",
                }

                final_report = self.nlg_generator.generate_response(nlg_context)

                details["end_to_end_flow"] = {
                    "controller_success": True,
                    "nlg_success": True,
                    "final_report_generated": len(final_report) > 0,
                    "report_preview": (
                        final_report[:200] + "..."
                        if len(final_report) > 200
                        else final_report
                    ),
                }

                success = True
            else:
                details["end_to_end_flow"] = {
                    "controller_success": False,
                    "nlg_success": False,
                    "error": "æ§åˆ¶å™¨æœªè¿”å›æœ‰æ•ˆçµæœ",
                }
                success = False

        except Exception as e:
            details["end_to_end_flow"] = {"success": False, "error": str(e)}
            success = False

        execution_time = time.time() - start_time

        self.test_results.append(
            IntegrationTestResult(
                test_name="ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_performance_stress(self):
        """æ¸¬è©¦æ•ˆèƒ½å£“åŠ›"""
        start_time = time.time()

        # å£“åŠ›æ¸¬è©¦åƒæ•¸
        stress_requests = 10
        concurrent_limit = 5

        details = {}

        try:
            # å»ºç«‹å£“åŠ›æ¸¬è©¦è«‹æ±‚
            requests = []
            for i in range(stress_requests):
                request = {
                    "type": "quick_analysis",
                    "content": f"æ¸¬è©¦è«‹æ±‚ {i+1}",
                    "priority": "low",
                }
                requests.append(request)

            # åˆ†æ‰¹ä¸¦ç™¼åŸ·è¡Œ
            results = []
            for i in range(0, len(requests), concurrent_limit):
                batch = requests[i : i + concurrent_limit]
                batch_tasks = [
                    self.unified_controller.process_unified_request(req)
                    for req in batch
                ]
                batch_results = await asyncio.gather(
                    *batch_tasks, return_exceptions=True
                )
                results.extend(batch_results)

            # çµ±è¨ˆæ•ˆèƒ½çµæœ
            successful_results = [r for r in results if not isinstance(r, Exception)]
            failed_results = [r for r in results if isinstance(r, Exception)]

            details["performance_stats"] = {
                "total_requests": stress_requests,
                "successful_requests": len(successful_results),
                "failed_requests": len(failed_results),
                "success_rate": len(successful_results) / stress_requests * 100,
                "average_response_time": (time.time() - start_time) / stress_requests,
            }

            success = len(successful_results) >= stress_requests * 0.8  # 80% æˆåŠŸç‡

        except Exception as e:
            details["performance_stats"] = {"success": False, "error": str(e)}
            success = False

        execution_time = time.time() - start_time

        self.test_results.append(
            IntegrationTestResult(
                test_name="æ•ˆèƒ½å£“åŠ›æ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    async def _test_error_recovery(self):
        """æ¸¬è©¦éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶"""
        start_time = time.time()

        # æ¸¬è©¦å„ç¨®éŒ¯èª¤æƒ…æ³çš„æ¢å¾©èƒ½åŠ›
        error_scenarios = [
            {"type": "invalid_request", "data": {"invalid": "æ ¼å¼éŒ¯èª¤çš„è«‹æ±‚"}},
            {
                "type": "missing_parameters",
                "data": {"type": "analysis"},
            },  # ç¼ºå°‘å¿…è¦åƒæ•¸
            {
                "type": "timeout_simulation",
                "data": {"type": "long_running_task", "timeout": 0.1},
            },
        ]

        details = {}
        recovery_count = 0

        for i, scenario in enumerate(error_scenarios):
            try:
                response = await self.unified_controller.process_unified_request(
                    scenario["data"]
                )

                # æª¢æŸ¥æ˜¯å¦æœ‰é©ç•¶çš„éŒ¯èª¤è™•ç†
                details[f"scenario_{i+1}"] = {
                    "scenario_type": scenario["type"],
                    "handled_gracefully": True,
                    "response_received": response is not None,
                }
                recovery_count += 1

            except Exception as e:
                # é æœŸçš„éŒ¯èª¤ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰é©ç•¶çš„éŒ¯èª¤è¨Šæ¯
                error_msg = str(e)
                is_handled = (
                    len(error_msg) > 0 and "unexpected" not in error_msg.lower()
                )

                details[f"scenario_{i+1}"] = {
                    "scenario_type": scenario["type"],
                    "handled_gracefully": is_handled,
                    "error_message": error_msg[:100],
                }

                if is_handled:
                    recovery_count += 1

        execution_time = time.time() - start_time
        success = recovery_count >= len(error_scenarios) * 0.7  # 70% æ¢å¾©ç‡

        self.test_results.append(
            IntegrationTestResult(
                test_name="éŒ¯èª¤æ¢å¾©æ¸¬è©¦",
                success=success,
                execution_time=execution_time,
                details=details,
            )
        )

    def _generate_test_report(self, total_time: float) -> dict[str, Any]:
        """
        ç”Ÿæˆæ¸¬è©¦å ±å‘Š

        Args:
            total_time: ç¸½åŸ·è¡Œæ™‚é–“

        Returns:
            æ¸¬è©¦å ±å‘Šçµ±è¨ˆ
        """
        successful_tests = [r for r in self.test_results if r.success]
        failed_tests = [r for r in self.test_results if not r.success]

        report = {
            "summary": {
                "total_tests": len(self.test_results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "success_rate": len(successful_tests) / len(self.test_results) * 100,
                "total_execution_time": total_time,
            },
            "test_results": [
                {
                    "name": result.test_name,
                    "success": result.success,
                    "execution_time": result.execution_time,
                    "error": result.error_message,
                }
                for result in self.test_results
            ],
            "recommendations": self._generate_recommendations(failed_tests),
        }

        return report

    def _generate_recommendations(
        self, failed_tests: list[IntegrationTestResult]
    ) -> list[str]:
        """
        æ ¹æ“šå¤±æ•—çš„æ¸¬è©¦ç”Ÿæˆå»ºè­°

        Args:
            failed_tests: å¤±æ•—çš„æ¸¬è©¦åˆ—è¡¨

        Returns:
            æ”¹é€²å»ºè­°åˆ—è¡¨
        """
        recommendations = []

        if not failed_tests:
            recommendations.append("ğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼AI æ•´åˆç³»çµ±é‹ä½œè‰¯å¥½ã€‚")
            return recommendations

        for test in failed_tests:
            if "åˆå§‹åŒ–" in test.test_name:
                recommendations.append("ğŸ”§ å»ºè­°æª¢æŸ¥å„ AI çµ„ä»¶çš„åˆå§‹åŒ–é…ç½®å’Œä¾è³´é …ã€‚")
            elif "å”èª¿" in test.test_name:
                recommendations.append("âš™ï¸ å»ºè­°å„ªåŒ–çµ±ä¸€æ§åˆ¶å™¨çš„ä»»å‹™åˆ†é…é‚è¼¯ã€‚")
            elif "è‡ªç„¶èªè¨€" in test.test_name:
                recommendations.append("ğŸ“ å»ºè­°æª¢æŸ¥ NLG ç³»çµ±çš„æ¨¡æ¿é…ç½®å’Œä¸Šä¸‹æ–‡è™•ç†ã€‚")
            elif "å¤šèªè¨€" in test.test_name:
                recommendations.append("ğŸŒ å»ºè­°æª¢æŸ¥å¤šèªè¨€å”èª¿å™¨çš„èªè¨€æ¨¡çµ„è¨»å†Šã€‚")
            elif "è¡çª" in test.test_name:
                recommendations.append("âš ï¸ å»ºè­°å¢å¼· AI è¡çªæª¢æ¸¬å’Œå»é‡æ©Ÿåˆ¶ã€‚")
            elif "ç«¯åˆ°ç«¯" in test.test_name:
                recommendations.append("ğŸ”„ å»ºè­°æª¢æŸ¥æ•´å€‹ AI è™•ç†æµç¨‹çš„å„å€‹ç’°ç¯€ã€‚")
            elif "æ•ˆèƒ½" in test.test_name:
                recommendations.append("âš¡ å»ºè­°å„ªåŒ–ç³»çµ±æ•ˆèƒ½å’Œä¸¦ç™¼è™•ç†èƒ½åŠ›ã€‚")
            elif "éŒ¯èª¤æ¢å¾©" in test.test_name:
                recommendations.append("ğŸ›¡ï¸ å»ºè­°å¢å¼·éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶ã€‚")

        return list(set(recommendations))  # å»é™¤é‡è¤‡å»ºè­°


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ AIVA AI æ•´åˆæ¸¬è©¦ç³»çµ±")
    print("=" * 50)

    # åˆå§‹åŒ–æ¸¬è©¦å™¨
    tester = AIIntegrationTester("c:/AMD/AIVA")

    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
    report = await tester.run_all_tests()

    # è¼¸å‡ºæ¸¬è©¦å ±å‘Š
    print("\nğŸ“Š æ¸¬è©¦å ±å‘Š")
    print("=" * 50)
    print(f"ğŸ“‹ ç¸½æ¸¬è©¦æ•¸: {report['summary']['total_tests']}")
    print(f"âœ… æˆåŠŸæ¸¬è©¦: {report['summary']['successful_tests']}")
    print(f"âŒ å¤±æ•—æ¸¬è©¦: {report['summary']['failed_tests']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {report['summary']['success_rate']:.1f}%")
    print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {report['summary']['total_execution_time']:.2f}ç§’")

    print("\nğŸ“ è©³ç´°çµæœ:")
    for result in report["test_results"]:
        status = "âœ…" if result["success"] else "âŒ"
        print(f"{status} {result['name']} ({result['execution_time']:.2f}s)")
        if result["error"]:
            print(f"   éŒ¯èª¤: {result['error']}")

    print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
    for recommendation in report["recommendations"]:
        print(f"  {recommendation}")

    # ä¿å­˜è©³ç´°å ±å‘Š
    report_file = Path("c:/AMD/AIVA/_out/ai_integration_test_report.json")
    report_file.parent.mkdir(exist_ok=True)
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")


if __name__ == "__main__":
    asyncio.run(main())
