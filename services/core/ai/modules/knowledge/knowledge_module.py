"""
AIVA Knowledge Module v2.0 - å¯¦ç”¨ç¨‹å¼è¨­è¨ˆçŸ¥è­˜æ¨¡çµ„
çŸ¥è­˜æ¨¡çµ„ - å°ˆæ³¨æ–¼ç¨‹å¼é–‹ç™¼å¯¦éš›éœ€è¦çš„çŸ¥è­˜ç®¡ç†åŠŸèƒ½

å¯¦ç”¨åŠŸèƒ½å°å‘çš„çŸ¥è­˜æ¨¡çµ„ï¼Œåªå¯¦ç¾ç¨‹å¼é–‹ç™¼è€…çœŸæ­£éœ€è¦çš„åŠŸèƒ½ï¼š
1. ç¨‹å¼ç¢¼åˆ†æèˆ‡å»ºè­°
2. å¢å¼·å‹RAGçŸ¥è­˜æª¢ç´¢
3. ä¸Šä¸‹æ–‡ç›¸é—œæœå°‹
4. èªç¾©æ–‡æª”æŸ¥æ‰¾

Author: AIVA Team
Created: 2025-11-09
Version: 2.0.0
"""

import asyncio
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import ast
import os
import re
import time
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path
import hashlib
import pickle
from collections import defaultdict
import uuid

# å°å…¥äº‹ä»¶ç³»çµ±
from ...core.event_system.event_bus import AIEvent, AIEventBus, EventPriority
from ...core.controller.strangler_fig_controller import StranglerFigController, AIRequest, AIResponse, MessageType

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== ç¨‹å¼ç¢¼åˆ†æå¼•æ“ ====================

class CodeAnalyzer(nn.Module):
    """ç¨‹å¼ç¢¼åˆ†æå¼•æ“ - ç”¨æ–¼ç¨‹å¼å»ºè­°"""
    
    def __init__(self, code_dim: int = 512, suggestion_dim: int = 256):
        super().__init__()
        
        # ç¨‹å¼ç¢¼èªæ³•ç·¨ç¢¼å™¨
        self.syntax_encoder = nn.Sequential(
            nn.Linear(code_dim, suggestion_dim * 2),
            nn.LayerNorm(suggestion_dim * 2),
            nn.ReLU(),
            nn.Linear(suggestion_dim * 2, suggestion_dim)
        )
        
        # æ¨¡å¼è­˜åˆ¥ç¶²è·¯
        self.pattern_recognizer = nn.Sequential(
            nn.Linear(suggestion_dim, suggestion_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(suggestion_dim, suggestion_dim // 2)
        )
        
        # å»ºè­°ç”Ÿæˆå™¨
        self.suggestion_generator = nn.Linear(suggestion_dim // 2, suggestion_dim)
        
    def analyze_code(self, code_content: str, language: str = "python") -> Dict[str, Any]:
        """åˆ†æç¨‹å¼ç¢¼ä¸¦æä¾›å»ºè­°"""
        
        # åŸºæœ¬èªæ³•åˆ†æ
        syntax_analysis = self._analyze_syntax(code_content, language)
        
        # ç¨‹å¼ç¢¼å“è³ªè©•ä¼°
        quality_metrics = self._assess_code_quality(code_content, language)
        
        # æ•ˆèƒ½åˆ†æ
        performance_analysis = self._analyze_performance(code_content, language)
        
        # å®‰å…¨æ€§æª¢æŸ¥
        security_check = self._check_security(code_content, language)
        
        # ç”Ÿæˆæ”¹é€²å»ºè­°
        improvement_suggestions = self._generate_suggestions(
            syntax_analysis, quality_metrics, performance_analysis, security_check
        )
        
        return {
            'syntax_analysis': syntax_analysis,
            'quality_metrics': quality_metrics,
            'performance_analysis': performance_analysis,
            'security_check': security_check,
            'improvement_suggestions': improvement_suggestions,
            'overall_score': self._calculate_overall_score(quality_metrics),
            'language': language,
            'analysis_timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _analyze_syntax(self, code: str, language: str) -> Dict[str, Any]:
        """èªæ³•åˆ†æ"""
        if language.lower() == "python":
            return self._analyze_python_syntax(code)
        else:
            return self._analyze_generic_syntax(code)
    
    def _analyze_python_syntax(self, code: str) -> Dict[str, Any]:
        """Python èªæ³•åˆ†æ"""
        try:
            tree = ast.parse(code)
            
            # åˆ†æ AST çµæ§‹
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            imports = [node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))]
            
            # è¤‡é›œåº¦åˆ†æ
            complexity = self._calculate_cyclomatic_complexity(tree)
            
            return {
                'is_valid': True,
                'classes_count': len(classes),
                'functions_count': len(functions),
                'imports_count': len(imports),
                'cyclomatic_complexity': complexity,
                'class_names': [cls.name for cls in classes],
                'function_names': [func.name for func in functions],
                'has_main_block': any(isinstance(node, ast.If) and 
                                     isinstance(node.test, ast.Compare) and
                                     hasattr(node.test.left, 'id') and
                                     node.test.left.id == '__name__' 
                                     for node in ast.walk(tree))
            }
            
        except SyntaxError as e:
            return {
                'is_valid': False,
                'syntax_error': str(e),
                'line_number': e.lineno,
                'error_offset': e.offset
            }
        except Exception as e:
            return {
                'is_valid': False,
                'analysis_error': str(e)
            }
    
    def _analyze_generic_syntax(self, code: str) -> Dict[str, Any]:
        """é€šç”¨èªæ³•åˆ†æ"""
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        # åŸºæœ¬çµ±è¨ˆ
        return {
            'is_valid': True,  # ç°¡åŒ–å‡è¨­
            'total_lines': len(lines),
            'non_empty_lines': len(non_empty_lines),
            'comment_lines': len([line for line in lines if line.strip().startswith(('#', '//', '/*'))]),
            'average_line_length': sum(len(line) for line in non_empty_lines) / max(len(non_empty_lines), 1),
            'max_line_length': max(len(line) for line in lines) if lines else 0
        }
    
    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:
        """è¨ˆç®—åœˆè¤‡é›œåº¦"""
        complexity = 1  # åŸºç¤è¤‡é›œåº¦
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
        
        return complexity
    
    def _assess_code_quality(self, code: str, language: str) -> Dict[str, Any]:
        """ç¨‹å¼ç¢¼å“è³ªè©•ä¼°"""
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        # å‘½åè¦ç¯„æª¢æŸ¥
        naming_score = self._check_naming_conventions(code, language)
        
        # è¨»è§£å¯†åº¦
        comment_density = self._calculate_comment_density(lines)
        
        # å‡½å¼é•·åº¦æª¢æŸ¥
        function_length_score = self._check_function_length(code, language)
        
        # å·¢ç‹€æ·±åº¦æª¢æŸ¥
        nesting_depth = self._calculate_max_nesting_depth(code)
        
        return {
            'naming_score': naming_score,
            'comment_density': comment_density,
            'function_length_score': function_length_score,
            'max_nesting_depth': nesting_depth,
            'line_count': len(non_empty_lines),
            'readability_score': (naming_score + comment_density + function_length_score) / 3
        }
    
    def _analyze_performance(self, code: str, language: str) -> Dict[str, Any]:
        """æ•ˆèƒ½åˆ†æ"""
        
        performance_issues = []
        
        if language.lower() == "python":
            # æª¢æŸ¥å¸¸è¦‹æ•ˆèƒ½å•é¡Œ
            if 'for' in code and 'in range(len(' in code:
                performance_issues.append({
                    'type': 'inefficient_loop',
                    'description': 'Use direct iteration instead of range(len())',
                    'severity': 'medium'
                })
            
            if '+ str(' in code or 'str(' in code and '+' in code:
                performance_issues.append({
                    'type': 'string_concatenation',
                    'description': 'Consider using f-strings or join() for string concatenation',
                    'severity': 'low'
                })
            
            if 'global ' in code:
                performance_issues.append({
                    'type': 'global_variables',
                    'description': 'Global variables can impact performance and readability',
                    'severity': 'medium'
                })
        
        return {
            'performance_issues': performance_issues,
            'issue_count': len(performance_issues),
            'performance_score': max(0, 1 - len(performance_issues) * 0.2)  # æ¯å€‹å•é¡Œ-0.2åˆ†
        }
    
    def _check_security(self, code: str, language: str) -> Dict[str, Any]:
        """å®‰å…¨æ€§æª¢æŸ¥"""
        
        security_issues = []
        
        # æª¢æŸ¥æ½›åœ¨çš„å®‰å…¨å•é¡Œ
        if 'eval(' in code:
            security_issues.append({
                'type': 'dangerous_eval',
                'description': 'Use of eval() can be dangerous',
                'severity': 'high'
            })
        
        if 'exec(' in code:
            security_issues.append({
                'type': 'dangerous_exec',
                'description': 'Use of exec() can be dangerous',
                'severity': 'high'
            })
        
        if 'os.system(' in code or 'subprocess.call(' in code:
            security_issues.append({
                'type': 'command_execution',
                'description': 'Direct command execution may pose security risks',
                'severity': 'medium'
            })
        
        # æª¢æŸ¥ç¡¬ç·¨ç¢¼å¯†ç¢¼ç­‰
        password_patterns = ['password', 'passwd', 'pwd', 'secret', 'key', 'token']
        for pattern in password_patterns:
            if re.search(rf'{pattern}\s*=\s*["\'][^"\']+["\']', code, re.IGNORECASE):
                security_issues.append({
                    'type': 'hardcoded_secret',
                    'description': f'Potential hardcoded {pattern} detected',
                    'severity': 'high'
                })
                break
        
        return {
            'security_issues': security_issues,
            'issue_count': len(security_issues),
            'security_score': max(0, 1 - len(security_issues) * 0.3)  # æ¯å€‹å•é¡Œ-0.3åˆ†
        }
    
    def _generate_suggestions(self, syntax_analysis: Dict, quality_metrics: Dict, 
                            performance_analysis: Dict, security_check: Dict) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        suggestions = []
        
        # èªæ³•å»ºè­°
        if not syntax_analysis.get('is_valid', True):
            suggestions.append({
                'category': 'syntax',
                'priority': 'high',
                'suggestion': 'Fix syntax errors before proceeding',
                'details': syntax_analysis.get('syntax_error', 'Unknown syntax issue')
            })
        
        # å“è³ªå»ºè­°
        if quality_metrics.get('comment_density', 0) < 0.1:
            suggestions.append({
                'category': 'documentation',
                'priority': 'medium',
                'suggestion': 'Add more comments to improve code readability',
                'details': f"Current comment density: {quality_metrics.get('comment_density', 0):.1%}"
            })
        
        if quality_metrics.get('max_nesting_depth', 0) > 4:
            suggestions.append({
                'category': 'complexity',
                'priority': 'medium',
                'suggestion': 'Consider refactoring to reduce nesting depth',
                'details': f"Current max nesting depth: {quality_metrics.get('max_nesting_depth', 0)}"
            })
        
        # æ•ˆèƒ½å»ºè­°
        for issue in performance_analysis.get('performance_issues', []):
            suggestions.append({
                'category': 'performance',
                'priority': issue['severity'],
                'suggestion': issue['description'],
                'details': f"Issue type: {issue['type']}"
            })
        
        # å®‰å…¨å»ºè­°
        for issue in security_check.get('security_issues', []):
            suggestions.append({
                'category': 'security',
                'priority': issue['severity'],
                'suggestion': issue['description'],
                'details': f"Security risk: {issue['type']}"
            })
        
        return suggestions
    
    def _check_naming_conventions(self, code: str, language: str) -> float:
        """æª¢æŸ¥å‘½åè¦ç¯„"""
        if language.lower() == "python":
            # Python PEP 8 å‘½åæª¢æŸ¥
            snake_case_pattern = r'^[a-z_][a-z0-9_]*$'
            camel_case_pattern = r'^[A-Z][a-zA-Z0-9]*$'
            
            # ç°¡åŒ–æª¢æŸ¥ï¼ˆå¯¦éš›æ‡‰ç”¨å¯ä»¥æ›´å®Œæ•´ï¼‰
            try:
                tree = ast.parse(code)
                
                # æª¢æŸ¥å‡½å¼åç¨±
                function_names = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                valid_functions = sum(1 for name in function_names if re.match(snake_case_pattern, name))
                
                # æª¢æŸ¥é¡åˆ¥åç¨±
                class_names = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
                valid_classes = sum(1 for name in class_names if re.match(camel_case_pattern, name))
                
                total_names = len(function_names) + len(class_names)
                valid_names = valid_functions + valid_classes
                
                return valid_names / total_names if total_names > 0 else 1.0
                
            except (SyntaxError, ValueError, TypeError):
                return 0.5  # ç„¡æ³•è§£æå‰‡çµ¦äºˆä¸­ç­‰åˆ†æ•¸
        
        return 0.7  # å…¶ä»–èªè¨€çµ¦äºˆé è¨­åˆ†æ•¸
    
    def _calculate_comment_density(self, lines: List[str]) -> float:
        """è¨ˆç®—è¨»è§£å¯†åº¦"""
        non_empty_lines = [line for line in lines if line.strip()]
        comment_lines = [line for line in lines if line.strip().startswith(('#', '//', '/*', '"""', "'''"))]
        
        return len(comment_lines) / max(len(non_empty_lines), 1)
    
    def _check_function_length(self, code: str, language: str) -> float:
        """æª¢æŸ¥å‡½å¼é•·åº¦"""
        if language.lower() == "python":
            try:
                tree = ast.parse(code)
                functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                
                if not functions:
                    return 1.0
                
                # è¨ˆç®—å¹³å‡å‡½å¼é•·åº¦ï¼ˆä»¥è¡Œç‚ºå–®ä½ï¼‰
                total_length = 0
                for func in functions:
                    func_lines = func.end_lineno - func.lineno + 1
                    total_length += func_lines
                
                avg_length = total_length / len(functions)
                
                # ç†æƒ³é•·åº¦ <= 20è¡Œï¼Œè¶…éæ‰£åˆ†
                if avg_length <= 20:
                    return 1.0
                elif avg_length <= 50:
                    return 0.8
                else:
                    return 0.6
                    
            except (SyntaxError, ValueError, TypeError):
                return 0.7
        
        return 0.8  # å…¶ä»–èªè¨€é è¨­åˆ†æ•¸
    
    def _calculate_max_nesting_depth(self, code: str) -> int:
        """è¨ˆç®—æœ€å¤§å·¢ç‹€æ·±åº¦"""
        lines = code.split('\n')
        max_depth = 0
        current_depth = 0
        
        for line in lines:
            stripped = line.lstrip()
            if not stripped:
                continue
                
            # è¨ˆç®—ç¸®æ’æ·±åº¦
            indent_level = (len(line) - len(stripped)) // 4  # å‡è¨­4ç©ºæ ¼ç¸®æ’
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ§åˆ¶çµæ§‹
            if any(stripped.startswith(keyword) for keyword in ['if', 'for', 'while', 'try', 'with', 'def', 'class']):
                current_depth = indent_level + 1
                max_depth = max(max_depth, current_depth)
        
        return max_depth
    
    def _calculate_overall_score(self, quality_metrics: Dict[str, Any]) -> float:
        """è¨ˆç®—ç¸½é«”åˆ†æ•¸"""
        readability = quality_metrics.get('readability_score', 0.5)
        
        # ç¶œåˆè©•åˆ†
        score = readability
        
        # èª¿æ•´åˆ†æ•¸
        if quality_metrics.get('max_nesting_depth', 0) > 5:
            score -= 0.2
        
        if quality_metrics.get('line_count', 0) > 500:
            score -= 0.1
        
        return max(0, min(1, score))

# ==================== å¢å¼·å‹ RAG æª¢ç´¢å¼•æ“ ====================

class EnhancedRAGRetriever(nn.Module):
    """å¢å¼·å‹ RAG (Retrieval-Augmented Generation) æª¢ç´¢å¼•æ“"""
    
    def __init__(self, embedding_dim: int = 384, hidden_dim: int = 512):
        super().__init__()
        
        # æ–‡æœ¬ç·¨ç¢¼å™¨
        self.text_encoder = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        
        # æŸ¥è©¢ç·¨ç¢¼å™¨
        self.query_encoder = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        
        # ç›¸é—œæ€§è©•åˆ†å™¨
        self.relevance_scorer = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # çŸ¥è­˜åº«
        self.knowledge_base = {}
        self.document_embeddings = {}
        
    def store_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> bool:
        """å„²å­˜æ–‡æª”åˆ°çŸ¥è­˜åº«"""
        try:
            # ç°¡åŒ–çš„æ–‡æª”è™•ç†
            processed_content = self._preprocess_text(content)
            
            # ç”Ÿæˆæ–‡æª”åµŒå…¥ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
            doc_embedding = self._generate_embedding(processed_content)
            
            # å„²å­˜
            self.knowledge_base[doc_id] = {
                'content': content,
                'processed_content': processed_content,
                'metadata': metadata or {},
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'word_count': len(processed_content.split()),
                'doc_type': self._infer_doc_type(content, metadata)
            }
            
            self.document_embeddings[doc_id] = doc_embedding
            
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜æ–‡æª”å¤±æ•—: {str(e)}")
            return False
    
    def retrieve_relevant_docs(self, query: str, top_k: int = 5, 
                              min_relevance: float = 0.3) -> List[Dict[str, Any]]:
        """æª¢ç´¢ç›¸é—œæ–‡æª”"""
        
        if not self.knowledge_base:
            return []
        
        # è™•ç†æŸ¥è©¢
        processed_query = self._preprocess_text(query)
        query_embedding = self._generate_embedding(processed_query)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        doc_scores = []
        
        for doc_id, doc_info in self.knowledge_base.items():
            doc_embedding = self.document_embeddings[doc_id]
            
            # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ç‚ºé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
            similarity = self._calculate_cosine_similarity(query_embedding, doc_embedding)
            
            # ä¸Šä¸‹æ–‡ç›¸é—œæ€§åŠ æ¬Š
            context_boost = self._calculate_context_boost(query, doc_info)
            
            # æœ€çµ‚åˆ†æ•¸
            final_score = similarity * (1 + context_boost)
            
            if final_score >= min_relevance:
                doc_scores.append({
                    'doc_id': doc_id,
                    'score': final_score,
                    'similarity': similarity,
                    'context_boost': context_boost,
                    'doc_info': doc_info
                })
        
        # æŒ‰åˆ†æ•¸æ’åºä¸¦è¿”å› top_k
        doc_scores.sort(key=lambda x: x['score'], reverse=True)
        
        return doc_scores[:top_k]
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é è™•ç†"""
        # åŸºæœ¬æ¸…ç†
        text = re.sub(r'\s+', ' ', text)  # æ­£è¦åŒ–ç©ºç™½å­—ç¬¦
        text = text.lower().strip()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ç¨‹å¼ç¢¼ç›¸é—œç¬¦è™Ÿ
        allowed_chars = r'[^\w\s.,;:()\[\]{}+=<>-]'
        text = re.sub(allowed_chars, '', text)
        
        return text
    
    def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        # é€™è£¡ä½¿ç”¨ç°¡åŒ–çš„TF-IDFé¢¨æ ¼åµŒå…¥
        words = text.split()
        
        # å‰µå»ºè©é »å‘é‡
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        # ç°¡åŒ–åµŒå…¥ï¼šä½¿ç”¨å›ºå®šç¶­åº¦çš„å‘é‡
        embedding = np.zeros(384)  # å›ºå®š384ç¶­
        
        for i, (word, freq) in enumerate(word_freq.items()):
            if i >= 384:
                break
            # ç°¡å–®çš„é›œæ¹Šæ˜ å°„åˆ°å‘é‡ç¶­åº¦
            hash_val = hash(word) % 384
            embedding[hash_val] += freq
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except (ZeroDivisionError, ValueError, TypeError):
            return 0.0
    
    def _calculate_context_boost(self, query: str, doc_info: Dict[str, Any]) -> float:
        """è¨ˆç®—ä¸Šä¸‹æ–‡ç›¸é—œæ€§åŠ æ¬Š"""
        boost = 0.0
        
        # æ–‡æª”é¡å‹ç›¸é—œæ€§
        doc_type = doc_info.get('doc_type', 'unknown')
        query_lower = query.lower()
        
        # æ–‡æª”é¡å‹åŒ¹é…åŠ æ¬Š
        doc_type_boost_map = {
            ('code', 'code'): 0.2,
            ('documentation', 'documentation'): 0.15,  # ç¨å¾®ä¸åŒçš„å€¼é¿å…é‡è¤‡
            ('api', 'api'): 0.25  # API æ–‡æª”çµ¦äºˆæ›´é«˜æ¬Šé‡
        }
        
        for (query_term, target_type), boost_value in doc_type_boost_map.items():
            if query_term in query_lower and doc_type == target_type:
                boost += boost_value
                break
        
        # æ–°é®®åº¦åŠ æ¬Š
        timestamp = doc_info.get('timestamp', '')
        if timestamp:
            try:
                doc_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                age_days = (datetime.now(timezone.utc) - doc_time).days
                
                # è¼ƒæ–°çš„æ–‡æª”çµ¦äºˆå°å¹…åŠ æˆ
                if age_days < 7:
                    boost += 0.1
                elif age_days < 30:
                    boost += 0.05
            except (ValueError, KeyError, TypeError):
                pass
        
        # æ–‡æª”é•·åº¦é©ä¸­åŠ æˆ
        word_count = doc_info.get('word_count', 0)
        if 50 <= word_count <= 500:
            boost += 0.05
        
        return min(boost, 0.5)  # æœ€å¤§åŠ æˆ50%
    
    def _infer_doc_type(self, content: str, metadata: Optional[Dict[str, Any]]) -> str:
        """æ¨æ–·æ–‡æª”é¡å‹"""
        if metadata and 'type' in metadata:
            return metadata['type']
        
        content_lower = content.lower()
        
        # ç¨‹å¼ç¢¼åµæ¸¬
        code_indicators = ['def ', 'class ', 'import ', 'function', '():', 'return ', 'if __name__']
        if any(indicator in content_lower for indicator in code_indicators):
            return 'code'
        
        # API æ–‡æª”åµæ¸¬
        api_indicators = ['api', 'endpoint', 'request', 'response', 'parameter', 'http']
        if any(indicator in content_lower for indicator in api_indicators):
            return 'api'
        
        # æ–‡æª”åµæ¸¬
        doc_indicators = ['documentation', 'guide', 'tutorial', 'readme', 'manual']
        if any(indicator in content_lower for indicator in doc_indicators):
            return 'documentation'
        
        return 'general'

# ==================== èªç¾©æœç´¢å¼•æ“ ====================

class SemanticSearchEngine:
    """èªç¾©æœç´¢å¼•æ“ - ç”¨æ–¼æ–‡æª”æŸ¥æ‰¾"""
    
    def __init__(self, index_path: Optional[str] = None):
        self.index_path = index_path or "knowledge_index.pkl"
        self.search_index = {}
        self.document_store = {}
        
        # è¼‰å…¥ç¾æœ‰ç´¢å¼•
        self._load_index()
    
    def index_document(self, doc_id: str, content: str, 
                      metadata: Optional[Dict[str, Any]] = None) -> bool:
        """ç‚ºæ–‡æª”å»ºç«‹ç´¢å¼•"""
        try:
            # æ–‡æœ¬åˆ†æ
            words = self._extract_keywords(content)
            
            # å»ºç«‹å€’æ’ç´¢å¼•
            for word in words:
                if word not in self.search_index:
                    self.search_index[word] = set()
                self.search_index[word].add(doc_id)
            
            # å„²å­˜æ–‡æª”è³‡è¨Š
            self.document_store[doc_id] = {
                'content': content,
                'metadata': metadata or {},
                'keywords': words,
                'indexed_at': datetime.now(timezone.utc).isoformat(),
                'word_count': len(content.split())
            }
            
            # æŒä¹…åŒ–ç´¢å¼•
            self._save_index()
            
            return True
            
        except Exception as e:
            logger.error(f"æ–‡æª”ç´¢å¼•å¤±æ•—: {str(e)}")
            return False
    
    def semantic_search(self, query: str, max_results: int = 10, 
                       search_type: str = "fuzzy") -> List[Dict[str, Any]]:
        """èªç¾©æœç´¢"""
        
        query_words = self._extract_keywords(query)
        
        if search_type == "exact":
            results = self._exact_search(query_words)
        elif search_type == "fuzzy":
            results = self._fuzzy_search(query_words)
        else:  # semantic
            results = self._semantic_search(query_words)
        
        # æ’åºå’Œéæ¿¾çµæœ
        sorted_results = sorted(results, key=lambda x: x['relevance_score'], reverse=True)
        
        return sorted_results[:max_results]
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµè©"""
        # åŸºæœ¬æ–‡æœ¬æ¸…ç†
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # éæ¿¾åœç”¨è©
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may', 'might', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}
        
        # éæ¿¾ä¸¦è¿”å›é—œéµè©
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords
    
    def _exact_search(self, query_words: List[str]) -> List[Dict[str, Any]]:
        """ç²¾ç¢ºæœç´¢"""
        results = []
        
        for doc_id, doc_info in self.document_store.items():
            doc_keywords = set(doc_info['keywords'])
            query_set = set(query_words)
            
            # è¨ˆç®—å®Œå…¨åŒ¹é…
            exact_matches = len(query_set & doc_keywords)
            
            if exact_matches > 0:
                relevance = exact_matches / len(query_set)
                
                results.append({
                    'doc_id': doc_id,
                    'content': doc_info['content'][:200] + "...",  # é è¦½
                    'metadata': doc_info['metadata'],
                    'relevance_score': relevance,
                    'match_type': 'exact',
                    'matched_keywords': list(query_set & doc_keywords)
                })
        
        return results
    
    def _fuzzy_search(self, query_words: List[str]) -> List[Dict[str, Any]]:
        """æ¨¡ç³Šæœç´¢"""
        results = []
        
        for doc_id, doc_info in self.document_store.items():
            doc_keywords = doc_info['keywords']
            
            match_score = 0
            matched_words = []
            
            for query_word in query_words:
                best_match_score = 0
                best_match_word = None
                
                for doc_word in doc_keywords:
                    # è¨ˆç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                    similarity = self._calculate_string_similarity(query_word, doc_word)
                    if similarity > best_match_score:
                        best_match_score = similarity
                        best_match_word = doc_word
                
                # å¦‚æœç›¸ä¼¼åº¦è¶…éé–¾å€¼ï¼Œè¨ˆå…¥åŒ¹é…
                if best_match_score > 0.7:
                    match_score += best_match_score
                    matched_words.append((query_word, best_match_word, best_match_score))
            
            if match_score > 0:
                relevance = match_score / len(query_words)
                
                results.append({
                    'doc_id': doc_id,
                    'content': doc_info['content'][:200] + "...",
                    'metadata': doc_info['metadata'],
                    'relevance_score': relevance,
                    'match_type': 'fuzzy',
                    'matched_words': matched_words
                })
        
        return results
    
    def _semantic_search(self, query_words: List[str]) -> List[Dict[str, Any]]:
        """èªç¾©æœç´¢ï¼ˆåŸºæ–¼åŒç¾©è©å’Œç›¸é—œæ¦‚å¿µï¼‰"""
        
        # æ“´å±•æŸ¥è©¢è©ï¼ˆç°¡åŒ–çš„åŒç¾©è©æ“´å±•ï¼‰
        expanded_words = query_words.copy()
        
        # ç¨‹å¼è¨­è¨ˆç›¸é—œåŒç¾©è©
        synonyms = {
            'function': ['method', 'procedure', 'routine'],
            'class': ['object', 'type', 'struct'],
            'variable': ['var', 'field', 'property'],
            'error': ['exception', 'bug', 'issue'],
            'test': ['testing', 'unittest', 'spec'],
            'api': ['interface', 'service', 'endpoint'],
            'data': ['information', 'dataset', 'values'],
            'code': ['source', 'script', 'program']
        }
        
        for word in query_words:
            if word in synonyms:
                expanded_words.extend(synonyms[word])
        
        # ä½¿ç”¨æ“´å±•è©é€²è¡Œæœç´¢
        return self._fuzzy_search(expanded_words)
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è¨ˆç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰"""
        set1 = set(str1)
        set2 = set(str2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        try:
            index_data = {
                'search_index': {word: list(doc_set) for word, doc_set in self.search_index.items()},
                'document_store': self.document_store,
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
            with open(self.index_path, 'wb') as f:
                pickle.dump(index_data, f)
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±æ•—: {str(e)}")
    
    def _load_index(self):
        """å¾æ–‡ä»¶è¼‰å…¥ç´¢å¼•"""
        try:
            if os.path.exists(self.index_path):
                with open(self.index_path, 'rb') as f:
                    index_data = pickle.load(f)
                
                self.search_index = {
                    word: set(doc_list) 
                    for word, doc_list in index_data.get('search_index', {}).items()
                }
                self.document_store = index_data.get('document_store', {})
                
                logger.info(f"è¼‰å…¥ç´¢å¼•å®Œæˆï¼ŒåŒ…å« {len(self.document_store)} å€‹æ–‡æª”")
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ç´¢å¼•å¤±æ•—: {str(e)}")
            self.search_index = {}
            self.document_store = {}

# ==================== çŸ¥è­˜æ¨¡çµ„ä¸»é¡ ====================

class KnowledgeModuleV2:
    """çŸ¥è­˜æ¨¡çµ„ v2.0 - å¯¦ç”¨åŠŸèƒ½å°å‘"""
    
    def __init__(self, event_bus: Optional[AIEventBus] = None):
        self.module_name = "knowledge"
        self.module_version = "v2.0"
        
        # äº‹ä»¶ç³»çµ±
        self.event_bus = event_bus
        
        # åˆå§‹åŒ–åŠŸèƒ½çµ„ä»¶
        self.code_analyzer = CodeAnalyzer(512, 256)
        self.rag_retriever = EnhancedRAGRetriever(384, 512)
        self.semantic_engine = SemanticSearchEngine()
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'code_analyses': 0,
            'document_retrievals': 0,
            'semantic_searches': 0,
            'documents_stored': 0,
            'avg_processing_time': 0.0,
            'success_rate': 0.0
        }
        
        logger.info(f"çŸ¥è­˜æ¨¡çµ„ {self.module_version} åˆå§‹åŒ–å®Œæˆ")
    
    async def process_request(self, request: AIRequest) -> AIResponse:
        """è™•ç†çŸ¥è­˜æ¨¡çµ„è«‹æ±‚"""
        start_time = time.time()
        
        try:
            operation = request.operation
            payload = request.payload
            
            if operation == 'analyze_code':
                result = self.analyze_code(payload)
            elif operation == 'retrieve_knowledge':
                result = self.retrieve_knowledge(payload)
            elif operation == 'semantic_search':
                result = self.semantic_search(payload)
            elif operation == 'store_document':
                result = self.store_document(payload)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ“ä½œ: {operation}")
            
            # ç™¼å¸ƒæˆåŠŸäº‹ä»¶
            if self.event_bus:
                await self._publish_event(f'knowledge.{operation}.completed', {
                    'request_id': request.request_id,
                    'operation': operation,
                    'result_summary': self._summarize_result(result),
                    'processing_time': (time.time() - start_time) * 1000
                })
            
            processing_time = (time.time() - start_time) * 1000
            self._update_stats(operation, processing_time, True)
            
            return AIResponse(
                request_id=request.request_id,
                status="success",
                processed_by=f"{self.module_name}@{self.module_version}",
                execution_time_ms=processing_time,
                result=result,
                metadata={
                    'module': self.module_name,
                    'version': self.module_version,
                    'operation': operation
                }
            )
            
        except Exception as e:
            # ç™¼å¸ƒéŒ¯èª¤äº‹ä»¶
            if self.event_bus:
                await self._publish_event('knowledge.error.occurred', {
                    'request_id': request.request_id,
                    'operation': request.operation,
                    'error_type': type(e).__name__,
                    'error_message': str(e)
                }, priority=EventPriority.HIGH)
            
            processing_time = (time.time() - start_time) * 1000
            self._update_stats(request.operation, processing_time, False)
            
            logger.error(f"çŸ¥è­˜æ¨¡çµ„è™•ç†éŒ¯èª¤: {str(e)}")
            
            return AIResponse(
                request_id=request.request_id,
                status="error",
                processed_by=f"{self.module_name}@{self.module_version}",
                execution_time_ms=processing_time,
                error={
                    "type": type(e).__name__,
                    "message": str(e)
                }
            )
    
    def analyze_code(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """ç¨‹å¼ç¢¼åˆ†æ"""
        logger.info("é–‹å§‹ç¨‹å¼ç¢¼åˆ†æ...")
        
        code_content = payload.get('code', '')
        language = payload.get('language', 'python')
        
        if not code_content:
            raise ValueError("ç¨‹å¼ç¢¼å…§å®¹ä¸èƒ½ç‚ºç©º")
        
        # åŸ·è¡Œç¨‹å¼ç¢¼åˆ†æ
        analysis_result = self.code_analyzer.analyze_code(code_content, language)
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats['code_analyses'] += 1
        
        result = {
            'analysis': analysis_result,
            'recommendations': self._generate_code_recommendations(analysis_result),
            'confidence': analysis_result['overall_score'],
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        logger.info(f"ç¨‹å¼ç¢¼åˆ†æå®Œæˆï¼Œç¸½åˆ†: {analysis_result['overall_score']:.2f}")
        
        return result
    
    def retrieve_knowledge(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """çŸ¥è­˜æª¢ç´¢ï¼ˆRAGï¼‰"""
        logger.info("é–‹å§‹çŸ¥è­˜æª¢ç´¢...")
        
        query = payload.get('query', '')
        top_k = payload.get('top_k', 5)
        min_relevance = payload.get('min_relevance', 0.3)
        
        if not query:
            raise ValueError("æª¢ç´¢æŸ¥è©¢ä¸èƒ½ç‚ºç©º")
        
        # åŸ·è¡ŒRAGæª¢ç´¢
        relevant_docs = self.rag_retriever.retrieve_relevant_docs(query, top_k, min_relevance)
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats['document_retrievals'] += 1
        
        result = {
            'query': query,
            'relevant_documents': relevant_docs,
            'total_found': len(relevant_docs),
            'retrieval_quality': self._assess_retrieval_quality(relevant_docs),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        logger.info(f"çŸ¥è­˜æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(relevant_docs)} å€‹ç›¸é—œæ–‡æª”")
        
        return result
    
    def semantic_search(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """èªç¾©æœç´¢"""
        logger.info("é–‹å§‹èªç¾©æœç´¢...")
        
        query = payload.get('query', '')
        max_results = payload.get('max_results', 10)
        search_type = payload.get('search_type', 'fuzzy')
        
        if not query:
            raise ValueError("æœç´¢æŸ¥è©¢ä¸èƒ½ç‚ºç©º")
        
        # åŸ·è¡Œèªç¾©æœç´¢
        search_results = self.semantic_engine.semantic_search(query, max_results, search_type)
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats['semantic_searches'] += 1
        
        result = {
            'query': query,
            'search_type': search_type,
            'results': search_results,
            'total_found': len(search_results),
            'search_quality': self._assess_search_quality(search_results),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        logger.info(f"èªç¾©æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} å€‹çµæœ")
        
        return result
    
    def store_document(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """å„²å­˜æ–‡æª”"""
        logger.info("é–‹å§‹å„²å­˜æ–‡æª”...")
        
        content = payload.get('content', '')
        doc_id = payload.get('doc_id') or str(uuid.uuid4())
        metadata = payload.get('metadata', {})
        
        if not content:
            raise ValueError("æ–‡æª”å…§å®¹ä¸èƒ½ç‚ºç©º")
        
        # å„²å­˜åˆ°RAGç³»çµ±
        rag_success = self.rag_retriever.store_document(doc_id, content, metadata)
        
        # å»ºç«‹èªç¾©æœç´¢ç´¢å¼•
        semantic_success = self.semantic_engine.index_document(doc_id, content, metadata)
        
        if rag_success and semantic_success:
            self.stats['documents_stored'] += 1
            
            result = {
                'doc_id': doc_id,
                'storage_success': True,
                'rag_indexed': rag_success,
                'semantic_indexed': semantic_success,
                'content_length': len(content),
                'word_count': len(content.split()),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            logger.info(f"æ–‡æª”å„²å­˜æˆåŠŸ: {doc_id}")
        else:
            result = {
                'doc_id': doc_id,
                'storage_success': False,
                'error': 'æ–‡æª”å„²å­˜æˆ–ç´¢å¼•å¤±æ•—',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            logger.error(f"æ–‡æª”å„²å­˜å¤±æ•—: {doc_id}")
        
        return result
    
    def _generate_code_recommendations(self, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºæ–¼åˆ†æçµæœç”Ÿæˆç¨‹å¼ç¢¼å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ”¹é€²å»ºè­°ç”Ÿæˆå…·é«”å»ºè­°
        for suggestion in analysis.get('improvement_suggestions', []):
            recommendation = {
                'category': suggestion['category'],
                'priority': suggestion['priority'],
                'title': suggestion['suggestion'],
                'description': suggestion['details'],
                'actionable': True
            }
            
            # æ·»åŠ å…·é«”çš„æ“ä½œå»ºè­°
            if suggestion['category'] == 'performance':
                recommendation['actions'] = [
                    'Review loops and data structures',
                    'Consider algorithm optimization',
                    'Profile performance bottlenecks'
                ]
            elif suggestion['category'] == 'security':
                recommendation['actions'] = [
                    'Review security vulnerabilities',
                    'Use secure coding practices',
                    'Validate inputs and sanitize outputs'
                ]
            elif suggestion['category'] == 'documentation':
                recommendation['actions'] = [
                    'Add docstrings to functions',
                    'Include inline comments',
                    'Document complex logic'
                ]
            else:
                recommendation['actions'] = ['Review and improve code quality']
            
            recommendations.append(recommendation)
        
        return recommendations
    
    def _assess_retrieval_quality(self, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©•ä¼°æª¢ç´¢å“è³ª"""
        if not retrieved_docs:
            return {'score': 0.0, 'quality': 'poor'}
        
        # å¹³å‡ç›¸é—œæ€§åˆ†æ•¸
        avg_relevance = sum(doc['score'] for doc in retrieved_docs) / len(retrieved_docs)
        
        # æ–‡æª”å¤šæ¨£æ€§ï¼ˆç°¡åŒ–è©•ä¼°ï¼‰
        doc_types = {doc['doc_info'].get('doc_type', 'unknown') for doc in retrieved_docs}
        diversity_score = len(doc_types) / max(len(retrieved_docs), 1)
        
        # ç¶œåˆå“è³ªåˆ†æ•¸
        quality_score = (avg_relevance * 0.7 + diversity_score * 0.3)
        
        if quality_score > 0.8:
            quality_level = 'excellent'
        elif quality_score > 0.6:
            quality_level = 'good'
        elif quality_score > 0.4:
            quality_level = 'fair'
        else:
            quality_level = 'poor'
        
        return {
            'score': quality_score,
            'quality': quality_level,
            'avg_relevance': avg_relevance,
            'diversity_score': diversity_score
        }
    
    def _assess_search_quality(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©•ä¼°æœç´¢å“è³ª"""
        if not search_results:
            return {'score': 0.0, 'quality': 'poor'}
        
        # å¹³å‡ç›¸é—œæ€§åˆ†æ•¸
        avg_relevance = sum(result['relevance_score'] for result in search_results) / len(search_results)
        
        # åŒ¹é…é¡å‹åˆ†æ
        match_types = [result['match_type'] for result in search_results]
        exact_matches = match_types.count('exact')
        exact_ratio = exact_matches / len(match_types) if match_types else 0
        
        # ç¶œåˆå“è³ªåˆ†æ•¸
        quality_score = (avg_relevance * 0.8 + exact_ratio * 0.2)
        
        if quality_score > 0.8:
            quality_level = 'excellent'
        elif quality_score > 0.6:
            quality_level = 'good'
        elif quality_score > 0.4:
            quality_level = 'fair'
        else:
            quality_level = 'poor'
        
        return {
            'score': quality_score,
            'quality': quality_level,
            'avg_relevance': avg_relevance,
            'exact_match_ratio': exact_ratio
        }
    
    def _summarize_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç¸½çµçµæœç”¨æ–¼äº‹ä»¶"""
        summary = {
            'status': 'completed',
            'data_points': len(str(result))
        }
        
        # æ·»åŠ ç‰¹å®šæ“ä½œçš„ç¸½çµè³‡è¨Š
        if 'analysis' in result:
            summary['overall_score'] = result['analysis'].get('overall_score', 0)
        if 'total_found' in result:
            summary['results_count'] = result['total_found']
        if 'storage_success' in result:
            summary['storage_success'] = result['storage_success']
        
        return summary
    
    async def _publish_event(self, event_type: str, data: Dict[str, Any], 
                           priority: EventPriority = EventPriority.NORMAL):
        """ç™¼å¸ƒäº‹ä»¶"""
        if not self.event_bus:
            return
            
        event = AIEvent(
            event_type=event_type,
            source_module=self.module_name,
            source_version=self.module_version,
            data=data,
            priority=priority
        )
        
        await self.event_bus.publish(event)
    
    def _update_stats(self, operation: str, processing_time: float, success: bool):
        """æ›´æ–°çµ±è¨ˆè³‡è¨Š"""
        # è¨˜éŒ„æ“ä½œé¡å‹çµ±è¨ˆ
        if operation not in self.stats:
            self.stats[f'{operation}_count'] = 0
        self.stats[f'{operation}_count'] += 1
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        if self.stats['avg_processing_time'] == 0:
            self.stats['avg_processing_time'] = processing_time
        else:
            alpha = 0.1
            self.stats['avg_processing_time'] = (
                alpha * processing_time + 
                (1 - alpha) * self.stats['avg_processing_time']
            )
        
        # æ›´æ–°æˆåŠŸç‡
        total_ops = sum([
            self.stats['code_analyses'],
            self.stats['document_retrievals'],
            self.stats['semantic_searches'],
            self.stats['documents_stored']
        ])
        
        if total_ops > 0:
            successful_ops = total_ops - (0 if success else 1)
            self.stats['success_rate'] = successful_ops / total_ops
    
    def get_health_status(self) -> Dict[str, Any]:
        """ç²å–å¥åº·ç‹€æ…‹"""
        total_operations = sum([
            self.stats['code_analyses'],
            self.stats['document_retrievals'],
            self.stats['semantic_searches'],
            self.stats['documents_stored']
        ])
        
        if self.stats['success_rate'] > 0.9:
            status = 'healthy'
        elif self.stats['success_rate'] > 0.7:
            status = 'degraded'
        else:
            status = 'unhealthy'
        
        return {
            'module': self.module_name,
            'version': self.module_version,
            'status': status,
            'statistics': self.stats,
            'total_operations': total_operations,
            'knowledge_base': {
                'rag_documents': len(self.rag_retriever.knowledge_base),
                'indexed_documents': len(self.semantic_engine.document_store)
            },
            'last_updated': datetime.now(timezone.utc).isoformat()
        }

# ==================== æ¸¬è©¦å’Œç¤ºä¾‹ ====================

async def test_knowledge_module():
    """æ¸¬è©¦çŸ¥è­˜æ¨¡çµ„"""
    
    print("ğŸ“š æ¸¬è©¦çŸ¥è­˜æ¨¡çµ„ v2.0 - å¯¦ç”¨åŠŸèƒ½")
    print("=" * 50)
    
    # å‰µå»ºçŸ¥è­˜æ¨¡çµ„
    knowledge = KnowledgeModuleV2()
    
    # æ¸¬è©¦ç¨‹å¼ç¢¼åˆ†æ
    print("\nğŸ” æ¸¬è©¦ç¨‹å¼ç¢¼åˆ†æ...")
    sample_code = '''
def calculate_fibonacci(n):
    """Calculate Fibonacci number using recursion"""
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)

class DataProcessor:
    def __init__(self):
        self.data = []
    
    def process_data(self, items):
        for i in range(len(items)):  # æ•ˆèƒ½å•é¡Œï¼šæ‡‰è©²ç”¨ for item in items
            self.data.append(items[i])
    
    def get_result(self):
        return str(self.data[0]) + str(self.data[1])  # æ•ˆèƒ½å•é¡Œï¼šæ‡‰è©²ç”¨ f-string
'''
    
    request = AIRequest(
        message_type=MessageType.COMMAND,
        source_module="test",
        operation="analyze_code",
        payload={
            'code': sample_code,
            'language': 'python'
        }
    )
    
    response = await knowledge.process_request(request)
    print(f"âœ… ç¨‹å¼ç¢¼åˆ†æå®Œæˆ: {response.status}")
    if response.result:
        analysis = response.result['analysis']
        print(f"ğŸ“Š æ•´é«”åˆ†æ•¸: {analysis['overall_score']:.2f}")
        print(f"ğŸ”§ æ”¹é€²å»ºè­°æ•¸é‡: {len(response.result['recommendations'])}")
        print(f"âš ï¸ æ•ˆèƒ½å•é¡Œ: {len(analysis['performance_analysis']['performance_issues'])}")
    
    # æ¸¬è©¦æ–‡æª”å„²å­˜
    print("\nğŸ’¾ æ¸¬è©¦æ–‡æª”å„²å­˜...")
    sample_docs = [
        {
            'content': 'Pythonå‡½å¼å®šç¾©ä½¿ç”¨ def é—œéµå­—ã€‚å‡½å¼å¯ä»¥æ¥å—åƒæ•¸ä¸¦è¿”å›å€¼ã€‚è‰¯å¥½çš„å‡½å¼æ‡‰è©²æœ‰æ˜ç¢ºçš„æ–‡æª”å­—ç¬¦ä¸²ã€‚',
            'metadata': {'type': 'documentation', 'topic': 'python_functions', 'language': 'zh-tw'}
        },
        {
            'content': 'def example_function(param1, param2=None): """This is a docstring""" return param1 + (param2 or 0)',
            'metadata': {'type': 'code', 'topic': 'python_functions', 'language': 'python'}
        },
        {
            'content': 'APIè¨­è¨ˆæœ€ä½³å¯¦è¸ï¼šä½¿ç”¨RESTfulæ¶æ§‹ï¼Œæ˜ç¢ºçš„ç«¯é»å‘½åï¼Œé©ç•¶çš„HTTPç‹€æ…‹ç¢¼ï¼Œå®Œæ•´çš„éŒ¯èª¤è™•ç†ã€‚',
            'metadata': {'type': 'api', 'topic': 'api_design', 'language': 'zh-tw'}
        }
    ]
    
    stored_docs = 0
    for doc in sample_docs:
        request = AIRequest(
            message_type=MessageType.COMMAND,
            source_module="test",
            operation="store_document",
            payload=doc
        )
        
        response = await knowledge.process_request(request)
        if response.result and response.result.get('storage_success'):
            stored_docs += 1
    
    print(f"âœ… æˆåŠŸå„²å­˜ {stored_docs} å€‹æ–‡æª”")
    
    # æ¸¬è©¦çŸ¥è­˜æª¢ç´¢ (RAG)
    print("\nğŸ” æ¸¬è©¦çŸ¥è­˜æª¢ç´¢ (RAG)...")
    request = AIRequest(
        message_type=MessageType.QUERY,
        source_module="test",
        operation="retrieve_knowledge",
        payload={
            'query': 'python function definition parameters',
            'top_k': 3
        }
    )
    
    response = await knowledge.process_request(request)
    print(f"âœ… çŸ¥è­˜æª¢ç´¢å®Œæˆ: {response.status}")
    if response.result:
        print(f"ğŸ“„ æ‰¾åˆ°ç›¸é—œæ–‡æª”: {response.result['total_found']}")
        print(f"â­ æª¢ç´¢å“è³ª: {response.result['retrieval_quality']['quality']}")
    
    # æ¸¬è©¦èªç¾©æœç´¢
    print("\nğŸ” æ¸¬è©¦èªç¾©æœç´¢...")
    request = AIRequest(
        message_type=MessageType.QUERY,
        source_module="test",
        operation="semantic_search",
        payload={
            'query': 'API design best practices',
            'search_type': 'fuzzy'
        }
    )
    
    response = await knowledge.process_request(request)
    print(f"âœ… èªç¾©æœç´¢å®Œæˆ: {response.status}")
    if response.result:
        print(f"ğŸ” æœç´¢çµæœ: {response.result['total_found']}")
        print(f"â­ æœç´¢å“è³ª: {response.result['search_quality']['quality']}")
    
    # ç²å–å¥åº·ç‹€æ…‹
    health = knowledge.get_health_status()
    print(f"\nğŸ’š æ¨¡çµ„å¥åº·ç‹€æ…‹: {health['status']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {health['statistics']['success_rate']:.1%}")
    print(f"ğŸ“š çŸ¥è­˜åº«æ–‡æª”æ•¸: RAG={health['knowledge_base']['rag_documents']}, ç´¢å¼•={health['knowledge_base']['indexed_documents']}")

if __name__ == "__main__":
    asyncio.run(test_knowledge_module())