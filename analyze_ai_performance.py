#!/usr/bin/env python3
"""
TODO 8 - AI çµ„ä»¶æ€§èƒ½åˆ†æå™¨
åˆ†æ capability è©•ä¼°å’Œ experience ç®¡ç†çš„æ€§èƒ½ç“¶é ¸ï¼Œåˆ¶å®šå„ªåŒ–ç­–ç•¥
"""

import sys
import time
import asyncio
import json
import traceback
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
import cProfile
import pstats
from io import StringIO

# æ·»åŠ  AIVA æ¨¡çµ„è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "services"))

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    component_name: str
    operation: str
    execution_time: float
    memory_usage: Optional[int] = None
    cpu_usage: Optional[float] = None
    throughput: Optional[float] = None
    error_count: int = 0
    optimization_potential: str = "low"

class AIComponentPerformanceAnalyzer:
    """AI çµ„ä»¶æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
        self.profiler = cProfile.Profile()
    
    def analyze_capability_evaluator_performance(self) -> List[PerformanceMetrics]:
        """åˆ†æ CapabilityEvaluator æ€§èƒ½"""
        print("ğŸ” åˆ†æ CapabilityEvaluator æ€§èƒ½...")
        
        try:
            from aiva_common.ai.capability_evaluator import AIVACapabilityEvaluator
            
            # åŸºæœ¬å¯¦ä¾‹åŒ–æ€§èƒ½
            start_time = time.time()
            evaluator = AIVACapabilityEvaluator()
            init_time = time.time() - start_time
            
            init_metric = PerformanceMetrics(
                component_name="AIVACapabilityEvaluator",
                operation="initialization",
                execution_time=init_time,
                optimization_potential="medium" if init_time > 0.1 else "low"
            )
            self.metrics.append(init_metric)
            print(f"  âœ… åˆå§‹åŒ–æ™‚é–“: {init_time:.4f}s")
            
            # æ¸¬è©¦ evaluate_capability æ€§èƒ½
            test_capability_info = {
                "capability_id": "test_cap_001",
                "name": "Test Capability",
                "description": "Performance test capability",
                "category": "testing",
                "version": "1.0.0"
            }
            
            start_time = time.time()
            try:
                # ä½¿ç”¨ profiler è©³ç´°åˆ†æ
                self.profiler.enable()
                result = evaluator.evaluate_capability(test_capability_info)
                self.profiler.disable()
                eval_time = time.time() - start_time
                
                eval_metric = PerformanceMetrics(
                    component_name="AIVACapabilityEvaluator", 
                    operation="evaluate_capability",
                    execution_time=eval_time,
                    optimization_potential="high" if eval_time > 1.0 else "medium" if eval_time > 0.5 else "low"
                )
                self.metrics.append(eval_metric)
                print(f"  âœ… èƒ½åŠ›è©•ä¼°æ™‚é–“: {eval_time:.4f}s")
                
            except Exception as e:
                print(f"  âš ï¸ èƒ½åŠ›è©•ä¼°æ¸¬è©¦å¤±æ•—: {e}")
                error_metric = PerformanceMetrics(
                    component_name="AIVACapabilityEvaluator",
                    operation="evaluate_capability",
                    execution_time=0,
                    error_count=1,
                    optimization_potential="high"
                )
                self.metrics.append(error_metric)
            
            # æ¸¬è©¦é€£çºŒç›£æ§æ€§èƒ½
            start_time = time.time()
            try:
                evaluator.start_continuous_monitoring()
                time.sleep(0.1)  # çŸ­æš«ç›£æ§
                evaluator.stop_continuous_monitoring()
                monitor_time = time.time() - start_time
                
                monitor_metric = PerformanceMetrics(
                    component_name="AIVACapabilityEvaluator",
                    operation="continuous_monitoring",
                    execution_time=monitor_time,
                    optimization_potential="medium" if monitor_time > 0.2 else "low"
                )
                self.metrics.append(monitor_metric)
                print(f"  âœ… é€£çºŒç›£æ§å•Ÿåœæ™‚é–“: {monitor_time:.4f}s")
                
            except Exception as e:
                print(f"  âš ï¸ é€£çºŒç›£æ§æ¸¬è©¦å¤±æ•—: {e}")
        
        except ImportError as e:
            print(f"  âŒ ç„¡æ³•å°å…¥ CapabilityEvaluator: {e}")
        
        return [m for m in self.metrics if m.component_name == "AIVACapabilityEvaluator"]
    
    def analyze_experience_manager_performance(self) -> List[PerformanceMetrics]:
        """åˆ†æ ExperienceManager æ€§èƒ½"""
        print("\nğŸ” åˆ†æ ExperienceManager æ€§èƒ½...")
        
        try:
            from aiva_common.ai.experience_manager import AIVAExperienceManager
            
            # åŸºæœ¬å¯¦ä¾‹åŒ–æ€§èƒ½
            start_time = time.time()
            manager = AIVAExperienceManager()
            init_time = time.time() - start_time
            
            init_metric = PerformanceMetrics(
                component_name="AIVAExperienceManager",
                operation="initialization", 
                execution_time=init_time,
                optimization_potential="medium" if init_time > 0.1 else "low"
            )
            self.metrics.append(init_metric)
            print(f"  âœ… åˆå§‹åŒ–æ™‚é–“: {init_time:.4f}s")
            
            # æ¸¬è©¦ç¶“é©—æ¨£æœ¬æ·»åŠ æ€§èƒ½
            test_sample = {
                "sample_id": "test_sample_001",
                "session_id": "test_session_001",
                "timestamp": time.time(),
                "context_vectors": [0.1, 0.2, 0.3],
                "action_taken": "test_action",
                "reward": 0.8,
                "quality_score": 0.9
            }
            
            # å–®æ¬¡æ·»åŠ æ€§èƒ½
            start_time = time.time()
            try:
                manager.add_experience_sample(test_sample)
                add_time = time.time() - start_time
                
                add_metric = PerformanceMetrics(
                    component_name="AIVAExperienceManager",
                    operation="add_experience_sample",
                    execution_time=add_time,
                    optimization_potential="medium" if add_time > 0.1 else "low"
                )
                self.metrics.append(add_metric)
                print(f"  âœ… å–®æ¬¡æ¨£æœ¬æ·»åŠ æ™‚é–“: {add_time:.4f}s")
                
            except Exception as e:
                print(f"  âš ï¸ æ¨£æœ¬æ·»åŠ æ¸¬è©¦å¤±æ•—: {e}")
            
            # æ‰¹é‡æ·»åŠ æ€§èƒ½æ¸¬è©¦
            batch_samples = []
            for i in range(100):
                sample = test_sample.copy()
                sample["sample_id"] = f"batch_sample_{i:03d}"
                batch_samples.append(sample)
            
            start_time = time.time()
            try:
                for sample in batch_samples:
                    manager.add_experience_sample(sample)
                batch_time = time.time() - start_time
                throughput = len(batch_samples) / batch_time
                
                batch_metric = PerformanceMetrics(
                    component_name="AIVAExperienceManager",
                    operation="batch_add_samples",
                    execution_time=batch_time,
                    throughput=throughput,
                    optimization_potential="high" if throughput < 100 else "medium" if throughput < 500 else "low"
                )
                self.metrics.append(batch_metric)
                print(f"  âœ… æ‰¹é‡æ·»åŠ 100å€‹æ¨£æœ¬: {batch_time:.4f}s (ååé‡: {throughput:.1f} samples/s)")
                
            except Exception as e:
                print(f"  âš ï¸ æ‰¹é‡æ·»åŠ æ¸¬è©¦å¤±æ•—: {e}")
            
            # æ¸¬è©¦ç¶“é©—æª¢ç´¢æ€§èƒ½
            start_time = time.time()
            try:
                experiences = manager.retrieve_experiences(limit=50)
                retrieve_time = time.time() - start_time
                
                retrieve_metric = PerformanceMetrics(
                    component_name="AIVAExperienceManager",
                    operation="retrieve_experiences",
                    execution_time=retrieve_time,
                    optimization_potential="high" if retrieve_time > 0.5 else "medium" if retrieve_time > 0.1 else "low"
                )
                self.metrics.append(retrieve_metric)
                print(f"  âœ… æª¢ç´¢50å€‹ç¶“é©—æ¨£æœ¬: {retrieve_time:.4f}s")
                
            except Exception as e:
                print(f"  âš ï¸ ç¶“é©—æª¢ç´¢æ¸¬è©¦å¤±æ•—: {e}")
        
        except ImportError as e:
            print(f"  âŒ ç„¡æ³•å°å…¥ ExperienceManager: {e}")
        
        return [m for m in self.metrics if m.component_name == "AIVAExperienceManager"]
    
    def analyze_async_performance(self) -> List[PerformanceMetrics]:
        """åˆ†æç•°æ­¥æ“ä½œæ€§èƒ½"""
        print("\nğŸ” åˆ†æç•°æ­¥æ“ä½œæ€§èƒ½...")
        
        async def async_capability_evaluation():
            """ç•°æ­¥èƒ½åŠ›è©•ä¼°æ¸¬è©¦"""
            try:
                from aiva_common.ai.capability_evaluator import AIVACapabilityEvaluator
                evaluator = AIVACapabilityEvaluator()
                
                start_time = time.time()
                # æ¨¡æ“¬ç•°æ­¥è©•ä¼°æ“ä½œ
                await asyncio.sleep(0.01)  # æ¨¡æ“¬ I/O æ“ä½œ
                result = evaluator.evaluate_capability({
                    "capability_id": "async_test",
                    "name": "Async Test",
                    "category": "testing"
                })
                execution_time = time.time() - start_time
                
                return PerformanceMetrics(
                    component_name="AIVACapabilityEvaluator",
                    operation="async_evaluate_capability",
                    execution_time=execution_time,
                    optimization_potential="medium" if execution_time > 0.1 else "low"
                )
            except Exception as e:
                print(f"    âš ï¸ ç•°æ­¥èƒ½åŠ›è©•ä¼°å¤±æ•—: {e}")
                return None
        
        async def async_experience_batch_processing():
            """ç•°æ­¥ç¶“é©—æ‰¹è™•ç†æ¸¬è©¦"""
            try:
                from aiva_common.ai.experience_manager import AIVAExperienceManager
                manager = AIVAExperienceManager()
                
                # å‰µå»ºæ¸¬è©¦æ•¸æ“š
                samples = []
                for i in range(50):
                    samples.append({
                        "sample_id": f"async_sample_{i:03d}",
                        "session_id": "async_session",
                        "timestamp": time.time(),
                        "context_vectors": [0.1 * i, 0.2 * i],
                        "action_taken": f"async_action_{i}",
                        "reward": 0.5 + (i % 10) * 0.05
                    })
                
                start_time = time.time()
                # æ¨¡æ“¬ç•°æ­¥æ‰¹è™•ç†
                for sample in samples:
                    manager.add_experience_sample(sample)
                    await asyncio.sleep(0.001)  # æ¨¡æ“¬ç•°æ­¥é–“éš”
                execution_time = time.time() - start_time
                
                return PerformanceMetrics(
                    component_name="AIVAExperienceManager",
                    operation="async_batch_processing",
                    execution_time=execution_time,
                    throughput=len(samples) / execution_time,
                    optimization_potential="high" if execution_time > 1.0 else "low"
                )
            except Exception as e:
                print(f"    âš ï¸ ç•°æ­¥æ‰¹è™•ç†å¤±æ•—: {e}")
                return None
        
        # é‹è¡Œç•°æ­¥æ¸¬è©¦
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # ä¸¦ç™¼æ¸¬è©¦
            start_time = time.time()
            tasks = [
                async_capability_evaluation(),
                async_experience_batch_processing()
            ]
            results = loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True))
            concurrent_time = time.time() - start_time
            
            for result in results:
                if isinstance(result, PerformanceMetrics):
                    self.metrics.append(result)
                    print(f"  âœ… {result.operation}: {result.execution_time:.4f}s")
            
            concurrent_metric = PerformanceMetrics(
                component_name="AsyncOperations",
                operation="concurrent_execution",
                execution_time=concurrent_time,
                optimization_potential="medium" if concurrent_time > 2.0 else "low"
            )
            self.metrics.append(concurrent_metric)
            print(f"  âœ… ä¸¦ç™¼åŸ·è¡Œç¸½æ™‚é–“: {concurrent_time:.4f}s")
            
        finally:
            loop.close()
        
        return [m for m in self.metrics if "async" in m.operation.lower()]
    
    def generate_optimization_recommendations(self) -> Dict[str, Any]:
        """åŸºæ–¼æ€§èƒ½åˆ†æç”Ÿæˆå„ªåŒ–å»ºè­°"""
        print("\nğŸ“Š ç”Ÿæˆå„ªåŒ–å»ºè­°...")
        
        recommendations = {
            "high_priority": [],
            "medium_priority": [],
            "low_priority": [],
            "performance_summary": {},
            "optimization_strategies": {}
        }
        
        # æŒ‰çµ„ä»¶åˆ†çµ„åˆ†æ
        component_metrics = {}
        for metric in self.metrics:
            if metric.component_name not in component_metrics:
                component_metrics[metric.component_name] = []
            component_metrics[metric.component_name].append(metric)
        
        for component, metrics in component_metrics.items():
            avg_time = sum(m.execution_time for m in metrics) / len(metrics)
            error_count = sum(m.error_count for m in metrics)
            high_potential_ops = [m for m in metrics if m.optimization_potential == "high"]
            
            recommendations["performance_summary"][component] = {
                "average_execution_time": avg_time,
                "total_operations": len(metrics),
                "error_count": error_count,
                "high_optimization_potential": len(high_potential_ops)
            }
            
            # ç”Ÿæˆå…·é«”å»ºè­°
            if component == "AIVACapabilityEvaluator":
                if any(m.execution_time > 1.0 for m in metrics):
                    recommendations["high_priority"].append({
                        "component": component,
                        "issue": "èƒ½åŠ›è©•ä¼°æ“ä½œè€—æ™‚éé•·",
                        "recommendation": "å¯¦æ–½è©•ä¼°çµæœç·©å­˜å’Œä¸¦è¡Œè™•ç†",
                        "expected_improvement": "50-70% æ€§èƒ½æå‡"
                    })
                
                if any(m.operation == "continuous_monitoring" and m.execution_time > 0.2 for m in metrics):
                    recommendations["medium_priority"].append({
                        "component": component,
                        "issue": "é€£çºŒç›£æ§å•Ÿåœé–‹éŠ·è¼ƒå¤§",
                        "recommendation": "ä½¿ç”¨è¼•é‡ç´šç›£æ§æ¨¡å¼å’Œç‹€æ…‹ç·©å­˜",
                        "expected_improvement": "30-40% æ€§èƒ½æå‡"
                    })
            
            elif component == "AIVAExperienceManager":
                throughput_metrics = [m for m in metrics if m.throughput is not None]
                if throughput_metrics and min(m.throughput for m in throughput_metrics) < 100:
                    recommendations["high_priority"].append({
                        "component": component,
                        "issue": "ç¶“é©—æ¨£æœ¬è™•ç†ååé‡ä½",
                        "recommendation": "å¯¦æ–½æ‰¹é‡è™•ç†ã€ç•°æ­¥ I/O å’Œå…§å­˜ç·©å­˜",
                        "expected_improvement": "200-300% ååé‡æå‡"
                    })
                
                if any(m.operation == "retrieve_experiences" and m.execution_time > 0.1 for m in metrics):
                    recommendations["medium_priority"].append({
                        "component": component,
                        "issue": "ç¶“é©—æª¢ç´¢å»¶é²è¼ƒé«˜",
                        "recommendation": "æ·»åŠ ç´¢å¼•ã€å¯¦æ–½æŸ¥è©¢å„ªåŒ–å’Œçµæœé ç·©å­˜",
                        "expected_improvement": "60-80% æŸ¥è©¢æ€§èƒ½æå‡"
                    })
        
        # ç³»çµ±ç´šå„ªåŒ–å»ºè­°
        recommendations["optimization_strategies"] = {
            "caching": {
                "description": "å¯¦æ–½å¤šå±¤ç·©å­˜ç­–ç•¥",
                "targets": ["è©•ä¼°çµæœ", "ç¶“é©—æŸ¥è©¢", "é…ç½®æ•¸æ“š"],
                "implementation": "Redis + å…§å­˜ç·©å­˜",
                "priority": "high"
            },
            "async_processing": {
                "description": "å¢å¼·ç•°æ­¥è™•ç†èƒ½åŠ›",
                "targets": ["æ‰¹é‡æ“ä½œ", "I/O å¯†é›†ä»»å‹™", "ä¸¦ç™¼è©•ä¼°"],
                "implementation": "asyncio + å·¥ä½œéšŠåˆ—",
                "priority": "high"
            },
            "resource_pooling": {
                "description": "å¯¦æ–½è³‡æºæ± åŒ–ç®¡ç†",
                "targets": ["æ•¸æ“šåº«é€£æ¥", "è¨ˆç®—è³‡æº", "è‡¨æ™‚å°è±¡"],
                "implementation": "é€£æ¥æ±  + å°è±¡æ± ",
                "priority": "medium"
            },
            "monitoring_optimization": {
                "description": "å„ªåŒ–æ€§èƒ½ç›£æ§é–‹éŠ·",
                "targets": ["é€£çºŒç›£æ§", "æŒ‡æ¨™æ”¶é›†", "æ—¥èªŒè¨˜éŒ„"],
                "implementation": "æ¡æ¨£ç›£æ§ + è¼•é‡ç´šæŒ‡æ¨™",
                "priority": "medium"
            }
        }
        
        return recommendations
    
    def get_profiler_stats(self) -> str:
        """ç²å–è©³ç´°çš„ profiler çµ±è¨ˆä¿¡æ¯"""
        if not hasattr(self.profiler, 'getstats') or not self.profiler.getstats():
            return "No profiling data available"
        
        s = StringIO()
        stats = pstats.Stats(self.profiler, stream=s)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # é¡¯ç¤ºå‰20å€‹æœ€è€—æ™‚çš„å‡½æ•¸
        return s.getvalue()

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ TODO 8 - AI çµ„ä»¶æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    analyzer = AIComponentPerformanceAnalyzer()
    
    # åŸ·è¡Œæ€§èƒ½åˆ†æ
    capability_metrics = analyzer.analyze_capability_evaluator_performance()
    experience_metrics = analyzer.analyze_experience_manager_performance() 
    async_metrics = analyzer.analyze_async_performance()
    
    # ç”Ÿæˆå„ªåŒ–å»ºè­°
    recommendations = analyzer.generate_optimization_recommendations()
    
    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ æ€§èƒ½åˆ†æçµæœç¸½çµ")
    print("=" * 60)
    
    print(f"\nğŸ“Š ç¸½è¨ˆåˆ†æäº† {len(analyzer.metrics)} å€‹æ€§èƒ½æŒ‡æ¨™")
    
    # é¡¯ç¤ºé«˜å„ªå…ˆç´šå»ºè­°
    if recommendations["high_priority"]:
        print("\nğŸ”¥ é«˜å„ªå…ˆç´šå„ªåŒ–å»ºè­°:")
        for rec in recommendations["high_priority"]:
            print(f"  â€¢ {rec['component']}: {rec['issue']}")
            print(f"    è§£æ±ºæ–¹æ¡ˆ: {rec['recommendation']}")
            print(f"    é æœŸæ”¹å–„: {rec['expected_improvement']}")
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    report = {
        "analysis_timestamp": time.time(),
        "metrics": [
            {
                "component": m.component_name,
                "operation": m.operation,
                "execution_time": m.execution_time,
                "throughput": m.throughput,
                "error_count": m.error_count,
                "optimization_potential": m.optimization_potential
            }
            for m in analyzer.metrics
        ],
        "recommendations": recommendations,
        "profiler_stats": analyzer.get_profiler_stats()
    }
    
    with open("TODO8_AI_PERFORMANCE_ANALYSIS_REPORT.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è©³ç´°åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: TODO8_AI_PERFORMANCE_ANALYSIS_REPORT.json")
    
    # æ ¹æ“šåˆ†æçµæœè¿”å›é©ç•¶çš„é€€å‡ºç¢¼
    high_priority_count = len(recommendations["high_priority"])
    if high_priority_count > 0:
        print(f"\nâš ï¸  ç™¼ç¾ {high_priority_count} å€‹é«˜å„ªå…ˆç´šæ€§èƒ½å•é¡Œéœ€è¦å„ªåŒ–")
        return 1
    else:
        print("\nâœ… æ€§èƒ½åˆ†æå®Œæˆï¼Œæ•´é«”æ€§èƒ½è‰¯å¥½")
        return 0

if __name__ == "__main__":
    sys.exit(main())