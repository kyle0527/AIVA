#!/usr/bin/env python3
"""
ğŸ§  AIVA 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¼”ç¤ºè…³æœ¬

å±•ç¤º5Mæ¨¡å‹åœ¨å¯¦éš›æ±ºç­–å ´æ™¯ä¸­çš„æ‡‰ç”¨
"""

import torch
import numpy as np
import sys
from pathlib import Path

# è¨­å®šè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / 'services' / 'core' / 'aiva_core' / 'ai_engine'))

def demo_5m_neural_network():
    """æ¼”ç¤º5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯"""
    print("ğŸ§  AIVA 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¼”ç¤º")
    print("=" * 50)
    
    try:
        from real_neural_core import RealAICore, RealDecisionEngine
        print("âœ… æˆåŠŸè¼‰å…¥5M AIæ ¸å¿ƒæ¨¡çµ„")
    except ImportError as e:
        print(f"âŒ æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
        return
    
    # å‰µå»º5Mæ¨¡å‹
    print("\nğŸ”§ åˆå§‹åŒ–5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯...")
    model = RealAICore(
        input_size=512,
        output_size=100,
        aux_output_size=531,
        use_5m_model=True,
        weights_path="services/core/aiva_core/ai_engine/aiva_5M_weights.pth"
    )
    
    print(f"ğŸ“Š æ¨¡å‹çµ±è¨ˆ:")
    print(f"   - ç¸½åƒæ•¸: {model.total_params:,} ({model.total_params/1_000_000:.1f}M)")
    print(f"   - è¼¸å…¥ç¶­åº¦: {model.input_size}")
    print(f"   - ä¸»è¼¸å‡ºç¶­åº¦: {model.output_size}")
    print(f"   - è¼”åŠ©è¼¸å‡ºç¶­åº¦: {model.aux_output_size}")
    
    # è¼‰å…¥æ¬Šé‡
    print("\nğŸ“ è¼‰å…¥é è¨“ç·´æ¬Šé‡...")
    model.load_weights()
    
    # æ¼”ç¤ºå ´æ™¯1: ç¶²è·¯æƒæçµæœåˆ†æ
    print("\nğŸ¯ å ´æ™¯1: ç¶²è·¯æƒæçµæœåˆ†æ")
    print("-" * 30)
    
    # æ¨¡æ“¬æƒæçµæœç‰¹å¾µ (512ç¶­)
    scan_features = torch.randn(1, 512) * 0.5  # æ­£è¦åŒ–è¼¸å…¥
    
    with torch.no_grad():
        # ç²å¾—AIæ±ºç­–
        main_output = model.forward(scan_features)
        decision_class = torch.argmax(main_output, dim=1).item()
        confidence = torch.max(torch.softmax(main_output, dim=1)).item()
        
        # ç²å¾—ç´°ç¯€ç‰¹å¾µ
        main_out, aux_out = model.forward_with_aux(scan_features)
        
        print(f"ğŸ¤– AIåˆ†æçµæœ:")
        print(f"   - æ¨è–¦æ±ºç­–é¡åˆ¥: {decision_class}")
        print(f"   - æ±ºç­–ä¿¡å¿ƒåº¦: {confidence:.3f} ({confidence*100:.1f}%)")
        print(f"   - ä¸»è¼¸å‡ºç¯„åœ: [{main_output.min().item():.3f}, {main_output.max().item():.3f}]")
        print(f"   - è¼”åŠ©ç‰¹å¾µç¶­åº¦: {aux_out.shape[1]}")
    
    # æ¼”ç¤ºå ´æ™¯2: æ‰¹é‡æ±ºç­–è™•ç†
    print("\nğŸ¯ å ´æ™¯2: æ‰¹é‡æ±ºç­–è™•ç†")
    print("-" * 30)
    
    batch_size = 5
    batch_inputs = torch.randn(batch_size, 512) * 0.3
    
    with torch.no_grad():
        batch_outputs = model.forward(batch_inputs)
        batch_decisions = torch.argmax(batch_outputs, dim=1)
        batch_confidences = torch.max(torch.softmax(batch_outputs, dim=1), dim=1)[0]
    
    print(f"ğŸ“‹ æ‰¹é‡è™•ç†çµæœ (æ‰¹æ¬¡å¤§å°: {batch_size}):")
    for i in range(batch_size):
        print(f"   ç›®æ¨™ {i+1}: é¡åˆ¥ {batch_decisions[i].item():2d}, ä¿¡å¿ƒåº¦ {batch_confidences[i].item():.3f}")
    
    # æ¼”ç¤ºå ´æ™¯3: æ±ºç­–å¼•æ“æ•´åˆ
    print("\nğŸ¯ å ´æ™¯3: æ±ºç­–å¼•æ“æ•´åˆ")
    print("-" * 30)
    
    try:
        engine = RealDecisionEngine(use_5m_model=True)
        engine.ai_core.load_weights()
        
        # æ¨¡æ“¬æ±ºç­–å ´æ™¯
        test_context = {
            "target_type": "web_application",
            "open_ports": [80, 443, 22],
            "services": ["http", "https", "ssh"],
            "vulnerabilities": ["sql_injection", "xss"]
        }
        
        decision_result = engine.generate_decision(
            "åˆ†æç›®æ¨™ä¸¦é¸æ“‡æœ€ä½³æ”»æ“Šç­–ç•¥", 
            test_context
        )
        
        print(f"ğŸ¯ æ±ºç­–å¼•æ“çµæœ:")
        print(f"   - æ±ºç­–: {decision_result['decision']}")
        print(f"   - ä¿¡å¿ƒåº¦: {decision_result['confidence']:.3f}")
        print(f"   - æ¨ç†æ‘˜è¦: {decision_result['reasoning'][:80]}...")
        
    except Exception as e:
        print(f"âš ï¸  æ±ºç­–å¼•æ“æ¼”ç¤ºè·³é: {e}")
    
    # æ€§èƒ½æ¸¬è©¦
    print("\nâš¡ æ€§èƒ½æ¸¬è©¦")
    print("-" * 30)
    
    import time
    
    # æ¸¬è©¦æ¨ç†é€Ÿåº¦
    test_input = torch.randn(1, 512)
    num_inferences = 100
    
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_inferences):
            _ = model.forward(test_input)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_inferences * 1000  # æ¯«ç§’
    throughput = num_inferences / (end_time - start_time)
    
    print(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ¨™:")
    print(f"   - å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.2f} ms")
    print(f"   - æ¨ç†ååé‡: {throughput:.1f} FPS")
    print(f"   - è¨­å‚™: {'ğŸ® CUDA' if torch.cuda.is_available() else 'ğŸ’» CPU'}")
    
    print("\nğŸ‰ 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ”¥ AIVAç¾åœ¨æ“æœ‰çœŸæ­£çš„æ·±åº¦å­¸ç¿’æ±ºç­–èƒ½åŠ›!")

def compare_models():
    """æ¯”è¼ƒ5Mæ¨¡å‹èˆ‡åŸå§‹æ¨¡å‹"""
    print("\nğŸ“Š æ¨¡å‹æ¯”è¼ƒåˆ†æ")
    print("=" * 50)
    
    try:
        from real_neural_core import RealAICore
        
        # å‰µå»ºåŸå§‹æ¨¡å‹
        legacy_model = RealAICore(use_5m_model=False)
        print(f"ğŸ“˜ åŸå§‹æ¨¡å‹: {legacy_model.total_params:,} åƒæ•¸")
        
        # å‰µå»º5Mæ¨¡å‹  
        model_5m = RealAICore(use_5m_model=True)
        print(f"ğŸ§  5Mæ¨¡å‹: {model_5m.total_params:,} åƒæ•¸")
        
        print(f"\nğŸš€ æå‡å€æ•¸:")
        print(f"   - åƒæ•¸å¢é•·: {model_5m.total_params / legacy_model.total_params:.1f}x")
        print(f"   - æ±ºç­–é¡åˆ¥: {model_5m.output_size} vs {legacy_model.output_size}")
        print(f"   - ç‰¹å¾µæå–: {model_5m.aux_output_size if hasattr(model_5m, 'aux_output_size') else 'ç„¡'} vs ç„¡")
        
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹æ¯”è¼ƒè·³é: {e}")

if __name__ == "__main__":
    demo_5m_neural_network()
    compare_models()
    
    print("\n" + "="*50)
    print("ğŸ¯ æ•´åˆç¸½çµ:")
    print("âœ… 5Mç‰¹åŒ–ç¥ç¶“ç¶²è·¯å·²å®Œå…¨æ•´åˆåˆ°AIVA AIæ ¸å¿ƒ")
    print("âœ… æ”¯æ´100é¡æ±ºç­–åˆ†é¡å’Œ531ç¶­ç‰¹å¾µæå–")
    print("âœ… ä¿æŒå‘å¾Œå…¼å®¹æ€§ï¼Œç„¡ç ´å£æ€§æ›´æ”¹")
    print("âœ… å³æ™‚æ¨ç†èƒ½åŠ›ï¼Œæ¯«ç§’ç´šéŸ¿æ‡‰æ™‚é–“")
    print("ğŸš€ AIVA AIæ™ºèƒ½åŒ–ç¨‹åº¦å¤§å¹…æå‡ï¼")