"""
åˆ†æ AIVA Common çš„è·¨èªè¨€åŠŸèƒ½å’Œ AI æ¨¡çµ„å®Œå‚™æ€§
"""

from pathlib import Path
import re

# å¸¸é‡å®šç¾©
CLASS_PATTERN = r"^class (\w+)\("


def analyze_ai_schemas(aiva_common):
    """åˆ†æç•¶å‰ AI schemas"""
    print("\nğŸ¤– æ­¥é©Ÿ 1: åˆ†æç•¶å‰ AI Schemas")
    print("-" * 50)

    ai_file = aiva_common / "schemas" / "ai.py"
    if not ai_file.exists():
        print("âŒ ai.py æª”æ¡ˆä¸å­˜åœ¨")
        return [], []

    content = ai_file.read_text(encoding="utf-8")
    ai_classes = re.findall(CLASS_PATTERN, content, re.MULTILINE)

    print(f"ğŸ“„ ai.py ä¸­çš„é¡åˆ¥ ({len(ai_classes)} å€‹):")
    for cls in sorted(ai_classes):
        print(f"   - {cls}")

    # æª¢æŸ¥æ˜¯å¦æœ‰å¤šèªè¨€ç›¸é—œçš„æ¨¡å¼
    multilang_patterns = [
        r"language|lang|locale",
        r"i18n|l10n|translation",
        r"cross.?language|multi.?language",
        r"encoding|charset|unicode",
    ]

    multilang_found = []
    for pattern in multilang_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
        if matches:
            multilang_found.extend(matches)

    if multilang_found:
        print("\nğŸŒ ç™¼ç¾å¤šèªè¨€ç›¸é—œå…§å®¹:")
        for match in set(multilang_found):
            print(f"   - {match}")
    else:
        print("\nâš ï¸  æœªç™¼ç¾å¤šèªè¨€ç›¸é—œå…§å®¹")

    return ai_classes, multilang_found


def analyze_cross_language_requirements():
    """åˆ†æè·¨èªè¨€æ”¯æ´éœ€æ±‚"""
    print("\nğŸŒ æ­¥é©Ÿ 2: è·¨èªè¨€æ”¯æ´éœ€æ±‚åˆ†æ")
    print("-" * 50)

    cross_lang_requirements = {
        "Programming Languages": [
            "Python",
            "JavaScript",
            "Go",
            "Java",
            "C#",
            "Ruby",
            "PHP",
            "Rust",
            "C++",
            "TypeScript",
        ],
        "Web Technologies": [
            "HTML",
            "CSS",
            "SASS",
            "LESS",
            "React",
            "Vue",
            "Angular",
            "Node.js",
        ],
        "Database Languages": [
            "SQL",
            "NoSQL",
            "GraphQL",
            "MongoDB Query",
            "Redis Commands",
        ],
        "Infrastructure": [
            "Docker",
            "Kubernetes",
            "Terraform",
            "CloudFormation",
            "Helm",
        ],
        "Natural Languages": [
            "English",
            "Chinese",
            "Japanese",
            "Korean",
            "German",
            "French",
            "Spanish",
        ],
    }

    print("AIVA å¹³å°æ‡‰æ”¯æ´çš„è·¨èªè¨€é¡åˆ¥:")
    for category, languages in cross_lang_requirements.items():
        print(f"\nğŸ“‚ {category}:")
        for lang in languages[:5]:  # åªé¡¯ç¤ºå‰5å€‹
            print(f"   - {lang}")
        if len(languages) > 5:
            print(f"   ... é‚„æœ‰ {len(languages) - 5} å€‹")

    return cross_lang_requirements


def suggest_multilingual_schemas():
    """å»ºè­°çš„å¤šèªè¨€ Schema æ“´å±•"""
    print("\nğŸ“ æ­¥é©Ÿ 3: å»ºè­°çš„å¤šèªè¨€ Schema æ“´å±•")
    print("-" * 50)

    suggested_schemas = {
        "è·¨èªè¨€åˆ†æ": [
            "CodeLanguageDetection",
            "MultiLanguageCodeAnalysis",
            "CrossLanguageVulnerability",
            "LanguageSpecificPayload",
        ],
        "åœ‹éš›åŒ–æ”¯æ´": [
            "I18nMessage",
            "LocalizedFinding",
            "MultiLanguageReport",
            "CulturalSecurityContext",
        ],
        "AI å¤šèªè¨€èƒ½åŠ›": [
            "MultiLingualAIModel",
            "LanguageAdaptiveStrategy",
            "CrossLangLearning",
            "CodeTranslationTask",
        ],
    }

    for category, schemas in suggested_schemas.items():
        print(f"\nğŸ”§ {category}:")
        for schema in schemas:
            print(f"   - {schema}")

    return suggested_schemas


def analyze_enums_multilang_support(aiva_common):
    """æª¢æŸ¥ Enums çš„å¤šèªè¨€æ”¯æ´"""
    print("\nğŸ·ï¸  æ­¥é©Ÿ 4: æª¢æŸ¥ Enums çš„å¤šèªè¨€æ”¯æ´")
    print("-" * 50)

    enum_files = list((aiva_common / "enums").glob("*.py"))
    all_enums = []

    for enum_file in enum_files:
        if enum_file.name == "__init__.py":
            continue
        try:
            content = enum_file.read_text(encoding="utf-8")
            enums = re.findall(CLASS_PATTERN, content, re.MULTILINE)
            all_enums.extend(enums)
        except OSError:
            continue

    print(f"ğŸ“Š ç•¶å‰ Enums ç¸½è¨ˆ: {len(all_enums)} å€‹")

    # æª¢æŸ¥æ˜¯å¦æœ‰èªè¨€ç›¸é—œçš„æšèˆ‰
    language_related = [
        enum
        for enum in all_enums
        if any(
            word in enum.lower() for word in ["lang", "language", "locale", "encoding"]
        )
    ]

    if language_related:
        print("ğŸŒ èªè¨€ç›¸é—œçš„æšèˆ‰:")
        for enum in language_related:
            print(f"   - {enum}")
    else:
        print("âš ï¸  ç¼ºå°‘èªè¨€ç›¸é—œçš„æšèˆ‰")

    return all_enums, language_related


def analyze_ai_capabilities(ai_file):
    """åˆ†æ AI æ¨¡çµ„å®Œå‚™æ€§"""
    print("\nğŸ§  æ­¥é©Ÿ 5: AI æ¨¡çµ„å®Œå‚™æ€§æª¢æŸ¥")
    print("-" * 50)

    ai_capabilities_needed = {
        "æ©Ÿå™¨å­¸ç¿’": ["è¨“ç·´", "æ¨ç†", "æ¨¡å‹ç®¡ç†", "ç‰¹å¾µå·¥ç¨‹"],
        "æ·±åº¦å­¸ç¿’": ["ç¥ç¶“ç¶²è·¯", "CNN", "RNN", "Transformer"],
        "å¼·åŒ–å­¸ç¿’": ["çå‹µç³»çµ±", "ç­–ç•¥å„ªåŒ–", "ç’°å¢ƒäº’å‹•", "ç¶“é©—å›æ”¾"],
        "è‡ªç„¶èªè¨€è™•ç†": ["æ–‡æœ¬åˆ†æ", "èªç¾©ç†è§£", "ä»£ç¢¼ç†è§£", "å¤šèªè¨€"],
        "å®‰å…¨AI": ["å°æŠ—æ”»æ“Šæª¢æ¸¬", "æ¨¡å‹é­¯æ£’æ€§", "éš±ç§ä¿è­·", "å¯è§£é‡‹æ€§"],
    }

    print("AI æ¨¡çµ„æ‡‰å…·å‚™çš„èƒ½åŠ›:")
    for category, capabilities in ai_capabilities_needed.items():
        print(f"\nğŸ¯ {category}:")
        for cap in capabilities:
            print(f"   - {cap}")

    # æª¢æŸ¥ç•¶å‰ AI schemas æ˜¯å¦æ¶µè“‹é€™äº›èƒ½åŠ›
    covered_capabilities = []
    if ai_file.exists():
        ai_content = ai_file.read_text(encoding="utf-8")

        for category, capabilities in ai_capabilities_needed.items():
            for cap in capabilities:
                # ç°¡å–®çš„é—œéµå­—åŒ¹é…
                if any(
                    keyword in ai_content.lower() for keyword in cap.lower().split()
                ):
                    covered_capabilities.append(f"{category}: {cap}")

        print(f"\nâœ… å·²æ¶µè“‹çš„èƒ½åŠ› ({len(covered_capabilities)} é …):")
        for cap in covered_capabilities[:10]:  # åªé¡¯ç¤ºå‰10é …
            print(f"   - {cap}")

    return ai_capabilities_needed, covered_capabilities


def analyze_cross_language_and_ai():
    """åˆ†æè·¨èªè¨€åŠŸèƒ½å’ŒAIæ¨¡çµ„å®Œå‚™æ€§"""
    project_root = Path(__file__).parent.parent
    aiva_common = project_root / "services" / "aiva_common"

    print("=" * 100)
    print("ğŸŒ AIVA Common è·¨èªè¨€åŠŸèƒ½å’Œ AI æ¨¡çµ„å®Œå‚™æ€§åˆ†æ")
    print("=" * 100)

    # åˆ†æå„å€‹éƒ¨åˆ†
    ai_classes, multilang_found = analyze_ai_schemas(aiva_common)
    cross_lang_requirements = analyze_cross_language_requirements()
    suggested_schemas = suggest_multilingual_schemas()
    all_enums, language_related = analyze_enums_multilang_support(aiva_common)
    ai_capabilities, covered_capabilities = analyze_ai_capabilities(
        aiva_common / "schemas" / "ai.py"
    )

    return {
        "ai_classes_count": len(ai_classes) if "ai_classes" in locals() else 0,
        "multilang_support": len(multilang_found) > 0
        if "multilang_found" in locals()
        else False,
        "suggested_enhancements": suggested_schemas,
        "cross_lang_requirements": cross_lang_requirements,
    }


def generate_enhancement_recommendations():
    """ç”Ÿæˆå¢å¼·å»ºè­°"""

    print("\nğŸš€ æ­¥é©Ÿ 6: å¢å¼·å»ºè­°")
    print("=" * 100)

    recommendations = {
        "ç«‹å³å¯¦æ–½": [
            "æ·»åŠ  ProgrammingLanguage æšèˆ‰",
            "å‰µå»º CodeLanguageDetection schema",
            "æ“´å±• AI æ¨¡å‹çš„å¤šèªè¨€æ”¯æ´",
            "æ·»åŠ  LocaleContext åˆ°æ‰€æœ‰ç”¨æˆ¶ç›¸é—œçš„ schema",
        ],
        "çŸ­æœŸç›®æ¨™": [
            "å¯¦æ–½è·¨èªè¨€ä»£ç¢¼åˆ†æ schema",
            "æ·»åŠ æ–‡åŒ–ç‰¹å®šçš„å®‰å…¨ä¸Šä¸‹æ–‡",
            "å‰µå»ºå¤šèªè¨€å ±å‘Šæ ¼å¼",
            "å¯¦æ–½ AI æ¨¡å‹çš„èªè¨€é©æ‡‰æ€§",
        ],
        "é•·æœŸç›®æ¨™": [
            "å®Œæ•´çš„åœ‹éš›åŒ–æ¡†æ¶",
            "è·¨èªè¨€æ¼æ´é—œè¯åˆ†æ",
            "å¤šèªè¨€ AI è¨“ç·´ç®¡é“",
            "å…¨çƒåŒ–å®‰å…¨æ¨™æº–æ”¯æ´",
        ],
    }

    for category, items in recommendations.items():
        print(f"\nğŸ“‹ {category}:")
        for item in items:
            print(f"   - {item}")

    return recommendations


if __name__ == "__main__":
    analysis = analyze_cross_language_and_ai()
    recommendations = generate_enhancement_recommendations()
