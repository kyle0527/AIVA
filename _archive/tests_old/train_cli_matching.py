#!/usr/bin/env python3
"""
CLI é…å°è¨“ç·´ - æ•™æœƒ AI æ­£ç¢ºé…å° CLI å‘½ä»¤èˆ‡å·¥å…·

é€šéç›£ç£å­¸ç¿’è¨“ç·´ç¥ç¶“ç¶²è·¯æ­£ç¢ºé…å° CLI å‘½ä»¤
"""
import sys
from pathlib import Path
import numpy as np
from datetime import datetime
import json

sys.path.insert(0, str(Path(__file__).parent))

from services.core.aiva_core.ai_engine.bio_neuron_core import BioNeuronRAGAgent, ScalableBioNet
from services.core.aiva_core.ai_engine.cli_tools import get_all_tools


# è¨“ç·´æ•¸æ“šé›† - CLI å‘½ä»¤ -> æ­£ç¢ºå·¥å…·çš„é…å°
TRAINING_DATA = [
    # Scan ç›¸é—œ
    ("æƒæç›®æ¨™ç¶²ç«™ example.com", "ScanTrigger"),
    ("å•Ÿå‹•æƒæ https://test.com", "ScanTrigger"),
    ("é–‹å§‹ç¶²ç«™æƒæ", "ScanTrigger"),
    ("scan target url", "ScanTrigger"),
    
    # SQLi æª¢æ¸¬
    ("æª¢æ¸¬ SQL æ³¨å…¥æ¼æ´åœ¨ login é é¢çš„ username åƒæ•¸", "SQLiDetector"),
    ("æ¸¬è©¦ SQL injection", "SQLiDetector"),
    ("SQL æ³¨å…¥æ¸¬è©¦", "SQLiDetector"),
    ("æª¢æŸ¥ SQL æ¼æ´", "SQLiDetector"),
    
    # XSS æª¢æ¸¬
    ("æª¢æ¸¬ XSS æ¼æ´åœ¨ search é é¢çš„ q åƒæ•¸", "XSSDetector"),
    ("æ¸¬è©¦è·¨ç«™è…³æœ¬æ”»æ“Š", "XSSDetector"),
    ("XSS æ¼æ´æª¢æ¸¬", "XSSDetector"),
    ("æª¢æŸ¥ XSS", "XSSDetector"),
    
    # ä»£ç¢¼åˆ†æ
    ("åˆ†æ services/core ç›®éŒ„çš„ä»£ç¢¼çµæ§‹", "CodeAnalyzer"),
    ("åˆ†æä»£ç¢¼è³ªé‡", "CodeAnalyzer"),
    ("æª¢æŸ¥ä»£ç¢¼çµæ§‹", "CodeAnalyzer"),
    ("analyze code", "CodeAnalyzer"),
    
    # æ–‡ä»¶è®€å–
    ("è®€å– pyproject.toml é…ç½®æ–‡ä»¶", "CodeReader"),
    ("è®€å– README.md", "CodeReader"),
    ("æŸ¥çœ‹æ–‡ä»¶å…§å®¹", "CodeReader"),
    ("read file", "CodeReader"),
    
    # æ–‡ä»¶å¯«å…¥
    ("å¯«å…¥é…ç½®åˆ° config.json", "CodeWriter"),
    ("å‰µå»ºæ–°æ–‡ä»¶", "CodeWriter"),
    ("ä¿®æ”¹æ–‡ä»¶å…§å®¹", "CodeWriter"),
    ("write file", "CodeWriter"),
    
    # å ±å‘Šç”Ÿæˆ
    ("ç”Ÿæˆæƒæå ±å‘Š", "ReportGenerator"),
    ("å‰µå»ºåˆ†æå ±å‘Š", "ReportGenerator"),
    ("è¼¸å‡ºæª¢æ¸¬çµæœ", "ReportGenerator"),
    ("generate report", "ReportGenerator"),
]


def train_neural_network(agent: BioNeuronRAGAgent, epochs: int = 100, learning_rate: float = 0.01):
    """è¨“ç·´ç¥ç¶“ç¶²è·¯é…å° CLI å‘½ä»¤"""
    
    print("="*70)
    print("è¨“ç·´ç¥ç¶“ç¶²è·¯ - CLI å‘½ä»¤é…å°")
    print("="*70)
    
    # ç²å–å·¥å…·æ˜ å°„
    tool_names = [tool["name"] for tool in agent.tools]
    tool_to_idx = {name: idx for idx, name in enumerate(tool_names)}
    
    print(f"\n[è¨“ç·´é…ç½®]")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(TRAINING_DATA)}")
    print(f"  å·¥å…·æ•¸é‡: {len(tool_names)}")
    print(f"  è¨“ç·´è¼ªæ•¸: {epochs}")
    print(f"  å­¸ç¿’ç‡: {learning_rate}")
    
    # è¨“ç·´æ­·å²
    history = {
        "epochs": [],
        "losses": [],
        "accuracies": []
    }
    
    print(f"\n[é–‹å§‹è¨“ç·´]")
    
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        
        # éš¨æ©Ÿæ‰“äº‚è¨“ç·´æ•¸æ“š
        np.random.shuffle(TRAINING_DATA)
        
        for task_desc, correct_tool in TRAINING_DATA:
            # ç°¡å–®çš„æ–‡æœ¬åµŒå…¥ï¼ˆå¯¦éš›æ‡‰è©²ä½¿ç”¨æ›´å¥½çš„ç·¨ç¢¼ï¼‰
            # é€™è£¡ä½¿ç”¨ä»»å‹™æè¿°çš„å“ˆå¸Œå€¼ç”Ÿæˆå½éš¨æ©Ÿå‘é‡
            task_hash = hash(task_desc)
            np.random.seed(abs(task_hash) % (2**32))
            task_vector = np.random.randn(agent.input_vector_size).astype(np.float32)
            
            # å‰å‘å‚³æ’­
            raw_scores = agent.decision_core.forward(task_vector)
            
            # å‰µå»ºç›®æ¨™ï¼ˆone-hot ç·¨ç¢¼ï¼‰
            target = np.zeros(len(tool_names), dtype=np.float32)
            target[tool_to_idx[correct_tool]] = 1.0
            
            # è¨ˆç®—æå¤±ï¼ˆäº¤å‰ç†µï¼‰
            probabilities = np.exp(raw_scores) / np.sum(np.exp(raw_scores))
            loss = -np.sum(target * np.log(probabilities + 1e-10))
            total_loss += loss
            
            # æª¢æŸ¥æº–ç¢ºåº¦
            predicted_idx = np.argmax(raw_scores)
            if predicted_idx == tool_to_idx[correct_tool]:
                correct += 1
            
            # åå‘å‚³æ’­ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            # è¨ˆç®—è¼¸å‡ºå±¤æ¢¯åº¦
            grad_output = probabilities - target
            
            # æ›´æ–°è¼¸å‡ºå±¤æ¬Šé‡
            agent.decision_core.fc2.weight -= learning_rate * np.outer(grad_output, agent.decision_core.fc2_input)
            agent.decision_core.fc2.bias -= learning_rate * grad_output
        
        # è¨ˆç®—å¹³å‡æå¤±å’Œæº–ç¢ºåº¦
        avg_loss = total_loss / len(TRAINING_DATA)
        accuracy = correct / len(TRAINING_DATA)
        
        history["epochs"].append(epoch + 1)
        history["losses"].append(float(avg_loss))
        history["accuracies"].append(float(accuracy))
        
        # æ¯ 10 è¼ªé¡¯ç¤ºä¸€æ¬¡é€²åº¦
        if (epoch + 1) % 10 == 0:
            print(f"  è¼ªæ•¸ {epoch+1:3d}/{epochs}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2%}")
    
    print(f"\nâœ“ è¨“ç·´å®Œæˆ")
    print(f"  æœ€çµ‚æå¤±: {history['losses'][-1]:.4f}")
    print(f"  æœ€çµ‚æº–ç¢ºåº¦: {history['accuracies'][-1]:.2%}")
    
    return history


def test_trained_agent(agent: BioNeuronRAGAgent):
    """æ¸¬è©¦è¨“ç·´å¾Œçš„ä»£ç†"""
    
    print("\n" + "="*70)
    print("æ¸¬è©¦è¨“ç·´å¾Œçš„ AI")
    print("="*70)
    
    test_cases = [
        ("æƒæç›®æ¨™ç¶²ç«™ example.com", "ScanTrigger"),
        ("æª¢æ¸¬ SQL æ³¨å…¥æ¼æ´åœ¨ login é é¢çš„ username åƒæ•¸", "SQLiDetector"),
        ("æª¢æ¸¬ XSS æ¼æ´åœ¨ search é é¢çš„ q åƒæ•¸", "XSSDetector"),
        ("åˆ†æ services/core ç›®éŒ„çš„ä»£ç¢¼çµæ§‹", "CodeAnalyzer"),
        ("è®€å– pyproject.toml é…ç½®æ–‡ä»¶", "CodeReader"),
    ]
    
    print(f"\næ¸¬è©¦ {len(test_cases)} å€‹æ¡ˆä¾‹:\n")
    
    correct = 0
    for i, (task, expected_tool) in enumerate(test_cases, 1):
        result = agent.invoke(task)
        actual_tool = result.get('tool_used')
        confidence = result.get('confidence', 0)
        
        is_correct = actual_tool == expected_tool
        if is_correct:
            correct += 1
        
        status_icon = "âœ“" if is_correct else "âœ—"
        print(f"[æ¸¬è©¦ {i}] {status_icon}")
        print(f"  ä»»å‹™: {task}")
        print(f"  é æœŸ: {expected_tool}")
        print(f"  å¯¦éš›: {actual_tool}")
        print(f"  ä¿¡å¿ƒåº¦: {confidence:.1%}")
        print()
    
    accuracy = correct / len(test_cases)
    print(f"æ¸¬è©¦æº–ç¢ºåº¦: {correct}/{len(test_cases)} = {accuracy:.1%}")
    
    return accuracy


def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    
    print("\n" + "="*70)
    print("BioNeuronCore AI - CLI å‘½ä»¤é…å°è¨“ç·´")
    print("="*70)
    print(f"\næ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. åˆå§‹åŒ– AI
    print("[æ­¥é©Ÿ 1] åˆå§‹åŒ– AI")
    codebase = str(Path(__file__).parent)
    
    agent = BioNeuronRAGAgent(
        codebase_path=codebase,
        enable_planner=False,
        enable_tracer=False,
        enable_experience=False
    )
    
    # è¼‰å…¥ CLI å·¥å…·
    cli_tools_dict = get_all_tools()
    agent.tools = [
        {"name": tool_name, "instance": tool_obj}
        for tool_name, tool_obj in cli_tools_dict.items()
    ]
    agent.tool_map = {tool["name"]: tool for tool in agent.tools}
    
    # é‡æ–°å‰µå»ºæ±ºç­–æ ¸å¿ƒ
    agent.decision_core = ScalableBioNet(
        agent.input_vector_size, 
        len(agent.tools)
    )
    
    print(f"âœ“ AI åˆå§‹åŒ–å®Œæˆï¼ˆ{len(agent.tools)} å€‹å·¥å…·ï¼‰\n")
    
    # 2. è¨“ç·´ç¥ç¶“ç¶²è·¯
    print("[æ­¥é©Ÿ 2] è¨“ç·´ç¥ç¶“ç¶²è·¯")
    history = train_neural_network(agent, epochs=100, learning_rate=0.01)
    
    # 3. æ¸¬è©¦è¨“ç·´çµæœ
    print("[æ­¥é©Ÿ 3] æ¸¬è©¦è¨“ç·´çµæœ")
    test_accuracy = test_trained_agent(agent)
    
    # 4. ä¿å­˜è¨“ç·´çµæœ
    print("\n[æ­¥é©Ÿ 4] ä¿å­˜è¨“ç·´çµæœ")
    
    output_dir = Path("data/ai_training")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è¨“ç·´æ­·å²
    history_file = output_dir / "training_history.json"
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2)
    print(f"âœ“ è¨“ç·´æ­·å²å·²ä¿å­˜: {history_file}")
    
    # ä¿å­˜æ¨¡å‹æ¬Šé‡
    weights_file = output_dir / "model_weights.npz"
    np.savez(
        weights_file,
        fc1_weight=agent.decision_core.fc1.weight,
        fc1_bias=agent.decision_core.fc1.bias,
        fc2_weight=agent.decision_core.fc2.weight,
        fc2_bias=agent.decision_core.fc2.bias,
    )
    print(f"âœ“ æ¨¡å‹æ¬Šé‡å·²ä¿å­˜: {weights_file}")
    
    # ä¿å­˜è¨“ç·´å ±å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "training_samples": len(TRAINING_DATA),
        "epochs": len(history["epochs"]),
        "final_loss": history["losses"][-1],
        "final_training_accuracy": history["accuracies"][-1],
        "test_accuracy": test_accuracy,
        "tools": [tool["name"] for tool in agent.tools],
        "neural_network": {
            "input_size": agent.input_vector_size,
            "hidden_size": 2048,
            "output_size": len(agent.tools),
            "total_params": agent.decision_core.total_params
        }
    }
    
    report_file = output_dir / "matching_training_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_file}")
    
    # æœ€çµ‚ç¸½çµ
    print("\n" + "="*70)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    print("="*70)
    print(f"\nğŸ“Š è¨“ç·´çµæœ:")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(TRAINING_DATA)}")
    print(f"  è¨“ç·´è¼ªæ•¸: {len(history['epochs'])}")
    print(f"  æœ€çµ‚æå¤±: {history['losses'][-1]:.4f}")
    print(f"  è¨“ç·´æº–ç¢ºåº¦: {history['accuracies'][-1]:.1%}")
    print(f"  æ¸¬è©¦æº–ç¢ºåº¦: {test_accuracy:.1%}")
    
    print(f"\nğŸ’¾ å·²ä¿å­˜:")
    print(f"  {history_file}")
    print(f"  {weights_file}")
    print(f"  {report_file}")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹åŸ·è¡Œ CLI å‘½ä»¤")
    print(f"  2. æ”¶é›†æ›´å¤šè¨“ç·´æ•¸æ“šæé«˜æº–ç¢ºåº¦")
    print(f"  3. å¯¦éš›éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
