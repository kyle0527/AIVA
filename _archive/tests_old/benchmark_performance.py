#!/usr/bin/env python3
"""
æ€§èƒ½åŸºæº–æ¸¬è©¦ - BioNeuronCore vs å…¶ä»–æ–¹æ³•

å°æ¯”:
1. BioNeuronCore + ç°¡å–®åŒ¹é…å™¨
2. ç´”é—œéµå­—åŒ¹é…
3. ç´”éš¨æ©Ÿï¼ˆåŸºæº–ç·šï¼‰
"""
import sys
from pathlib import Path
import time
import random
from typing import List, Tuple
import json

sys.path.insert(0, str(Path(__file__).parent))

from services.core.aiva_core.ai_engine.bio_neuron_core import BioNeuronRAGAgent
from services.core.aiva_core.ai_engine.simple_matcher import SimpleTaskMatcher
from services.core.aiva_core.ai_engine.cli_tools import get_all_tools


class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦"""
        self.test_cases = [
            ("æƒæç›®æ¨™ç¶²ç«™ example.com", "ScanTrigger"),
            ("é–‹å§‹å®‰å…¨æƒæ https://test.com", "ScanTrigger"),
            ("æª¢æ¸¬ SQL æ³¨å…¥æ¼æ´åœ¨ç™»å…¥é é¢", "SQLiDetector"),
            ("æ¸¬è©¦ SQL injection åœ¨ç”¨æˆ¶è¡¨å–®", "SQLiDetector"),
            ("æª¢æ¸¬ XSS æ¼æ´åœ¨æœç´¢æ¡†", "XSSDetector"),
            ("æ¸¬è©¦è·¨ç«™è…³æœ¬æ”»æ“Š", "XSSDetector"),
            ("åˆ†æ services/core ç›®éŒ„ä»£ç¢¼", "CodeAnalyzer"),
            ("æª¢æŸ¥ä»£ç¢¼è³ªé‡å’Œçµæ§‹", "CodeAnalyzer"),
            ("è®€å– pyproject.toml é…ç½®", "CodeReader"),
            ("æŸ¥çœ‹ README.md æ–‡ä»¶å…§å®¹", "CodeReader"),
            ("å¯«å…¥é…ç½®åˆ° config.json", "CodeWriter"),
            ("å‰µå»ºæ–°çš„Pythonæ–‡ä»¶", "CodeWriter"),
            ("ç”Ÿæˆå®Œæ•´æƒæå ±å‘Š", "ReportGenerator"),
            ("è¼¸å‡ºæ¼æ´æª¢æ¸¬çµæœ", "ReportGenerator"),
        ]
        
        # åˆå§‹åŒ–å·¥å…·
        cli_tools = get_all_tools()
        self.tools = [{"name": n, "instance": o} for n, o in cli_tools.items()]
        self.tool_names = [t["name"] for t in self.tools]
    
    def test_random_baseline(self) -> dict:
        """æ¸¬è©¦éš¨æ©Ÿé¸æ“‡ï¼ˆåŸºæº–ç·šï¼‰"""
        print("\n[æ–¹æ³• 1] ç´”éš¨æ©Ÿé¸æ“‡ï¼ˆåŸºæº–ç·šï¼‰")
        print("-" * 70)
        
        correct = 0
        times = []
        
        for task, expected in self.test_cases:
            start = time.perf_counter()
            predicted = random.choice(self.tool_names)
            elapsed = time.perf_counter() - start
            
            times.append(elapsed)
            if predicted == expected:
                correct += 1
        
        accuracy = correct / len(self.test_cases)
        avg_time = sum(times) / len(times) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
        
        print(f"  æº–ç¢ºåº¦: {correct}/{len(self.test_cases)} = {accuracy:.1%}")
        print(f"  å¹³å‡å»¶é²: {avg_time:.4f} ms")
        print(f"  ååé‡: {1000/avg_time:.0f} æ¨ç†/ç§’")
        
        return {
            "method": "Random Baseline",
            "accuracy": accuracy,
            "avg_latency_ms": avg_time,
            "throughput_per_sec": 1000 / avg_time,
            "correct": correct,
            "total": len(self.test_cases)
        }
    
    def test_simple_matcher(self) -> dict:
        """æ¸¬è©¦ç°¡å–®åŒ¹é…å™¨"""
        print("\n[æ–¹æ³• 2] ç°¡å–®åŒ¹é…å™¨ï¼ˆé—œéµå­—ï¼‰")
        print("-" * 70)
        
        matcher = SimpleTaskMatcher(self.tools)
        
        correct = 0
        times = []
        confidences = []
        
        for task, expected in self.test_cases:
            start = time.perf_counter()
            predicted, confidence = matcher.match(task)
            elapsed = time.perf_counter() - start
            
            times.append(elapsed)
            confidences.append(confidence)
            if predicted == expected:
                correct += 1
        
        accuracy = correct / len(self.test_cases)
        avg_time = sum(times) / len(times) * 1000
        avg_confidence = sum(confidences) / len(confidences)
        
        print(f"  æº–ç¢ºåº¦: {correct}/{len(self.test_cases)} = {accuracy:.1%}")
        print(f"  å¹³å‡å»¶é²: {avg_time:.4f} ms")
        print(f"  ååé‡: {1000/avg_time:.0f} æ¨ç†/ç§’")
        print(f"  å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.1%}")
        
        return {
            "method": "Simple Matcher",
            "accuracy": accuracy,
            "avg_latency_ms": avg_time,
            "throughput_per_sec": 1000 / avg_time,
            "avg_confidence": avg_confidence,
            "correct": correct,
            "total": len(self.test_cases)
        }
    
    def test_bioneuron_core(self) -> dict:
        """æ¸¬è©¦ BioNeuronCore AI"""
        print("\n[æ–¹æ³• 3] BioNeuronCore AI")
        print("-" * 70)
        
        # å‰µå»º AI ä»£ç†
        codebase = str(Path(__file__).parent)
        agent = BioNeuronRAGAgent(
            codebase_path=codebase,
            enable_planner=False,
            enable_tracer=False,
            enable_experience=False
        )
        
        # æ›¿æ›å·¥å…·
        agent.tools = self.tools
        agent.tool_map = {tool["name"]: tool for tool in self.tools}
        
        # é‡æ–°å‰µå»ºæ±ºç­–æ ¸å¿ƒ
        from services.core.aiva_core.ai_engine.bio_neuron_core import ScalableBioNet
        agent.decision_core = ScalableBioNet(agent.input_vector_size, len(self.tools))
        
        correct = 0
        times = []
        confidences = []
        
        for task, expected in self.test_cases:
            start = time.perf_counter()
            result = agent.invoke(task)
            elapsed = time.perf_counter() - start
            
            times.append(elapsed)
            predicted = result.get('tool_used')
            confidence = result.get('confidence', 0)
            confidences.append(confidence)
            
            if predicted == expected:
                correct += 1
        
        accuracy = correct / len(self.test_cases)
        avg_time = sum(times) / len(times) * 1000
        avg_confidence = sum(confidences) / len(confidences)
        
        print(f"  æº–ç¢ºåº¦: {correct}/{len(self.test_cases)} = {accuracy:.1%}")
        print(f"  å¹³å‡å»¶é²: {avg_time:.4f} ms")
        print(f"  ååé‡: {1000/avg_time:.0f} æ¨ç†/ç§’")
        print(f"  å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.1%}")
        print(f"  ç¥ç¶“ç¶²è·¯åƒæ•¸: {agent.decision_core.total_params:,}")
        
        return {
            "method": "BioNeuronCore AI",
            "accuracy": accuracy,
            "avg_latency_ms": avg_time,
            "throughput_per_sec": 1000 / avg_time,
            "avg_confidence": avg_confidence,
            "neural_params": agent.decision_core.total_params,
            "correct": correct,
            "total": len(self.test_cases)
        }
    
    def test_hybrid_approach(self) -> dict:
        """æ¸¬è©¦æ··åˆæ–¹æ³•ï¼ˆåŒ¹é…å™¨ + ç¥ç¶“ç¶²è·¯ï¼‰"""
        print("\n[æ–¹æ³• 4] æ··åˆæ–¹æ³•ï¼ˆæ¨è–¦ï¼‰")
        print("-" * 70)
        
        # å‰µå»ºå…©å€‹çµ„ä»¶
        matcher = SimpleTaskMatcher(self.tools)
        
        codebase = str(Path(__file__).parent)
        agent = BioNeuronRAGAgent(
            codebase_path=codebase,
            enable_planner=False,
            enable_tracer=False,
            enable_experience=False
        )
        agent.tools = self.tools
        agent.tool_map = {tool["name"]: tool for tool in self.tools}
        
        from services.core.aiva_core.ai_engine.bio_neuron_core import ScalableBioNet
        agent.decision_core = ScalableBioNet(agent.input_vector_size, len(self.tools))
        
        correct = 0
        times = []
        confidences = []
        matcher_used = 0
        neural_used = 0
        
        for task, expected in self.test_cases:
            start = time.perf_counter()
            
            # 1. å˜—è©¦ç°¡å–®åŒ¹é…
            match_tool, match_conf = matcher.match(task)
            
            # 2. å¦‚æœä¿¡å¿ƒåº¦ä½ï¼Œä½¿ç”¨ç¥ç¶“ç¶²è·¯
            if match_conf >= 0.7:
                predicted = match_tool
                confidence = match_conf
                matcher_used += 1
            else:
                result = agent.invoke(task)
                predicted = result.get('tool_used')
                confidence = result.get('confidence', 0)
                neural_used += 1
            
            elapsed = time.perf_counter() - start
            
            times.append(elapsed)
            confidences.append(confidence)
            
            if predicted == expected:
                correct += 1
        
        accuracy = correct / len(self.test_cases)
        avg_time = sum(times) / len(times) * 1000
        avg_confidence = sum(confidences) / len(confidences)
        
        print(f"  æº–ç¢ºåº¦: {correct}/{len(self.test_cases)} = {accuracy:.1%}")
        print(f"  å¹³å‡å»¶é²: {avg_time:.4f} ms")
        print(f"  ååé‡: {1000/avg_time:.0f} æ¨ç†/ç§’")
        print(f"  å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.1%}")
        print(f"  ä½¿ç”¨åŒ¹é…å™¨: {matcher_used} æ¬¡")
        print(f"  ä½¿ç”¨ç¥ç¶“ç¶²è·¯: {neural_used} æ¬¡")
        
        return {
            "method": "Hybrid (Matcher + Neural)",
            "accuracy": accuracy,
            "avg_latency_ms": avg_time,
            "throughput_per_sec": 1000 / avg_time,
            "avg_confidence": avg_confidence,
            "matcher_used": matcher_used,
            "neural_used": neural_used,
            "correct": correct,
            "total": len(self.test_cases)
        }
    
    def generate_comparison_report(self, results: List[dict]):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        print("\n" + "="*70)
        print("æ€§èƒ½å°æ¯”ç¸½çµ")
        print("="*70)
        
        # æ’åºï¼ˆæŒ‰æº–ç¢ºåº¦ï¼‰
        results_sorted = sorted(results, key=lambda x: x['accuracy'], reverse=True)
        
        print("\n[æº–ç¢ºåº¦æ’å]")
        for i, r in enumerate(results_sorted, 1):
            print(f"  {i}. {r['method']}: {r['accuracy']:.1%}")
        
        # æ’åºï¼ˆæŒ‰é€Ÿåº¦ï¼‰
        results_by_speed = sorted(results, key=lambda x: x['avg_latency_ms'])
        
        print("\n[é€Ÿåº¦æ’åï¼ˆå»¶é²è¶Šä½è¶Šå¥½ï¼‰]")
        for i, r in enumerate(results_by_speed, 1):
            print(f"  {i}. {r['method']}: {r['avg_latency_ms']:.4f} ms")
        
        # ç¶œåˆè©•åˆ†
        print("\n[ç¶œåˆè©•åˆ†ï¼ˆæº–ç¢ºåº¦Ã—é€Ÿåº¦æ¬Šé‡ï¼‰]")
        for r in results:
            # ç¶œåˆåˆ†æ•¸ = æº–ç¢ºåº¦ * 0.7 + (1 - æ¨™æº–åŒ–å»¶é²) * 0.3
            max_latency = max(res['avg_latency_ms'] for res in results)
            normalized_latency = r['avg_latency_ms'] / max_latency
            score = r['accuracy'] * 0.7 + (1 - normalized_latency) * 0.3
            r['ç»¼åˆåˆ†æ•¸'] = score
        
        results_by_score = sorted(results, key=lambda x: x['ç»¼åˆåˆ†æ•¸'], reverse=True)
        for i, r in enumerate(results_by_score, 1):
            print(f"  {i}. {r['method']}: {r['ç»¼åˆåˆ†æ•¸']:.3f}")
        
        # ä¿å­˜å ±å‘Š
        output_file = Path("data/ai_training/benchmark_report.json")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "test_cases": len(self.test_cases),
                "results": results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ å®Œæ•´å ±å‘Šå·²ä¿å­˜: {output_file}")
        
        # å‹å‡ºè€…
        winner = results_by_score[0]
        print(f"\nğŸ† ç¶œåˆå† è»: {winner['method']}")
        print(f"   æº–ç¢ºåº¦: {winner['accuracy']:.1%}")
        print(f"   å»¶é²: {winner['avg_latency_ms']:.4f} ms")
        print(f"   ç¶œåˆåˆ†æ•¸: {winner['ç»¼åˆåˆ†æ•¸']:.3f}")


def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("="*70)
    print("BioNeuronCore AI æ€§èƒ½åŸºæº–æ¸¬è©¦")
    print("="*70)
    print(f"\næ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    benchmark = PerformanceBenchmark()
    print(f"æ¸¬è©¦æ¡ˆä¾‹æ•¸: {len(benchmark.test_cases)}")
    print(f"å·¥å…·æ•¸é‡: {len(benchmark.tools)}")
    
    results = []
    
    # æ¸¬è©¦ 1: éš¨æ©ŸåŸºæº–ç·š
    results.append(benchmark.test_random_baseline())
    
    # æ¸¬è©¦ 2: ç°¡å–®åŒ¹é…å™¨
    results.append(benchmark.test_simple_matcher())
    
    # æ¸¬è©¦ 3: BioNeuronCore
    results.append(benchmark.test_bioneuron_core())
    
    # æ¸¬è©¦ 4: æ··åˆæ–¹æ³•
    results.append(benchmark.test_hybrid_approach())
    
    # ç”Ÿæˆå°æ¯”å ±å‘Š
    benchmark.generate_comparison_report(results)
    
    print("\n" + "="*70)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
