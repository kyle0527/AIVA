"""å¯¦éš›åŸ·è¡Œå¤šèªè¨€èƒ½åŠ›åˆ†æ

ç›´æ¥é‹è¡Œå®Œæ•´çš„èƒ½åŠ›åˆ†æä¸¦ç”Ÿæˆå ±å‘Š
"""

import asyncio
import logging
from pathlib import Path
from collections import Counter
import json
from datetime import datetime

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)

logger = logging.getLogger(__name__)


async def main():
    """åŸ·è¡Œå®Œæ•´çš„èƒ½åŠ›åˆ†æ"""
    
    # å°å…¥æ¨¡çµ„
    from services.core.aiva_core.internal_exploration import (
        ModuleExplorer,
        CapabilityAnalyzer
    )
    
    logger.info("=" * 70)
    logger.info("ğŸš€ AIVA å¤šèªè¨€èƒ½åŠ›åˆ†æç³»çµ± v2.0 Enhanced")
    logger.info("=" * 70)
    logger.info(f"ğŸ“… åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # åˆå§‹åŒ–
    explorer = ModuleExplorer()
    analyzer = CapabilityAnalyzer()
    
    # 1. æ¢ç´¢æ¨¡çµ„
    logger.info("ğŸ” éšæ®µ 1: æ¢ç´¢æ¨¡çµ„çµæ§‹...")
    modules = await explorer.explore_all_modules()
    logger.info(f"   âœ… ç™¼ç¾ {len(modules)} å€‹æ¨¡çµ„\n")
    
    # 2. åˆ†æèƒ½åŠ›
    logger.info("ğŸ” éšæ®µ 2: åˆ†æå¤šèªè¨€èƒ½åŠ›...")
    capabilities = await analyzer.analyze_capabilities(modules)
    logger.info(f"   âœ… æå– {len(capabilities)} å€‹èƒ½åŠ›\n")
    
    # 3. çµ±è¨ˆåˆ†æ
    logger.info("=" * 70)
    logger.info("ğŸ“Š èªè¨€åˆ†å¸ƒçµ±è¨ˆ")
    logger.info("=" * 70)
    
    lang_counts = Counter(cap["language"] for cap in capabilities)
    
    # è¡¨æ ¼æ¨™é¡Œ
    logger.info(f"\n{'èªè¨€':<12} {'èƒ½åŠ›æ•¸':>8}  {'ä½”æ¯”':>8}  {'ç‹€æ…‹':>6}")
    logger.info("-" * 45)
    
    for lang, count in lang_counts.most_common():
        percentage = (count / len(capabilities)) * 100
        status = "âœ…"
        logger.info(f"{lang:<12} {count:>8}  {percentage:>7.1f}%  {status:>6}")
    
    logger.info("-" * 45)
    logger.info(f"{'ç¸½è¨ˆ':<12} {len(capabilities):>8}  {100.0:>7.1f}%  âœ…\n")
    
    # 4. Rust è©³ç´°åˆ†æ
    rust_caps = [cap for cap in capabilities if cap["language"] == "rust"]
    if rust_caps:
        logger.info("=" * 70)
        logger.info("ğŸ¦€ Rust èƒ½åŠ›è©³ç´°åˆ†æ")
        logger.info("=" * 70)
        
        methods = [cap for cap in rust_caps if cap.get("is_method")]
        functions = [cap for cap in rust_caps if not cap.get("is_method")]
        
        logger.info(f"\nç¸½è¨ˆ: {len(rust_caps)} å€‹èƒ½åŠ›")
        logger.info(f"  ğŸ“¦ çµæ§‹é«”æ–¹æ³•: {len(methods)}")
        logger.info(f"  ğŸ“ é ‚å±¤å‡½æ•¸:   {len(functions)}")
        
        if methods:
            logger.info(f"\nğŸ” ç†±é–€çµæ§‹é«” (Top 10 æ–¹æ³•):")
            
            # æŒ‰çµæ§‹é«”åˆ†çµ„
            by_struct = {}
            for cap in methods:
                struct = cap.get("struct", "Unknown")
                by_struct.setdefault(struct, []).append(cap)
            
            # æ’åºä¸¦é¡¯ç¤º
            sorted_structs = sorted(by_struct.items(), key=lambda x: len(x[1]), reverse=True)
            for i, (struct, caps) in enumerate(sorted_structs[:10], 1):
                logger.info(f"   {i:2}. {struct:<30} - {len(caps)} å€‹æ–¹æ³•")
        
        logger.info("")
    
    # 5. éŒ¯èª¤å ±å‘Š
    analyzer.print_extraction_report()
    
    # 6. æ¨¡çµ„çµ±è¨ˆ
    logger.info("=" * 70)
    logger.info("ğŸ“¦ æ¨¡çµ„èƒ½åŠ›åˆ†å¸ƒ")
    logger.info("=" * 70)
    
    grouped = analyzer.get_capabilities_by_module(capabilities)
    
    logger.info(f"\n{'æ¨¡çµ„':<25} {'èƒ½åŠ›æ•¸':>8}  {'ä¸»è¦èªè¨€':<15}")
    logger.info("-" * 55)
    
    for module, caps in sorted(grouped.items(), key=lambda x: len(x[1]), reverse=True):
        main_lang = Counter(c["language"] for c in caps).most_common(1)[0][0]
        logger.info(f"{module:<25} {len(caps):>8}  {main_lang:<15}")
    
    logger.info("")
    
    # 7. ä¿å­˜çµæœ
    logger.info("=" * 70)
    logger.info("ğŸ’¾ ä¿å­˜åˆ†æçµæœ")
    logger.info("=" * 70)
    
    output_dir = Path("analysis_results")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜å®Œæ•´æ•¸æ“š
    output_file = output_dir / f"capabilities_{timestamp}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(capabilities, f, indent=2, ensure_ascii=False)
    logger.info(f"   âœ… å®Œæ•´æ•¸æ“š: {output_file}")
    
    # ä¿å­˜çµ±è¨ˆæ‘˜è¦
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total_capabilities": len(capabilities),
        "language_distribution": dict(lang_counts),
        "module_distribution": {
            module: len(caps) for module, caps in grouped.items()
        },
        "rust_details": {
            "total": len(rust_caps),
            "methods": len(methods) if rust_caps else 0,
            "functions": len(functions) if rust_caps else 0
        },
        "extraction_report": analyzer.get_extraction_report()
    }
    
    summary_file = output_dir / f"summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    logger.info(f"   âœ… çµ±è¨ˆæ‘˜è¦: {summary_file}")
    
    # 8. å°æ¯”åˆ†æ (å¦‚æœæœ‰åŸºç·š)
    baseline_file = output_dir / "baseline.json"
    if baseline_file.exists():
        logger.info("\nğŸ“Š èˆ‡åŸºç·šå°æ¯”:")
        
        with open(baseline_file) as f:
            baseline = json.load(f)
        
        baseline_count = baseline.get("total_capabilities", 0)
        diff = len(capabilities) - baseline_count
        diff_pct = (diff / baseline_count * 100) if baseline_count > 0 else 0
        
        if diff > 0:
            logger.info(f"   ğŸ“ˆ èƒ½åŠ›æ•¸å¢åŠ : +{diff} (+{diff_pct:.1f}%)")
        elif diff < 0:
            logger.info(f"   ğŸ“‰ èƒ½åŠ›æ•¸æ¸›å°‘: {diff} ({diff_pct:.1f}%)")
        else:
            logger.info(f"   â¡ï¸  èƒ½åŠ›æ•¸ä¸è®Š: {len(capabilities)}")
        
        # èªè¨€å°æ¯”
        baseline_langs = baseline.get("language_distribution", {})
        for lang in set(list(lang_counts.keys()) + list(baseline_langs.keys())):
            current = lang_counts.get(lang, 0)
            previous = baseline_langs.get(lang, 0)
            if current != previous:
                diff = current - previous
                logger.info(f"   {lang}: {previous} â†’ {current} ({diff:+d})")
    else:
        # å‰µå»ºåŸºç·š
        logger.info(f"\n   ğŸ“ é¦–æ¬¡åŸ·è¡Œï¼Œå‰µå»ºåŸºç·š...")
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        logger.info(f"   âœ… åŸºç·šå·²ä¿å­˜")
    
    logger.info("")
    
    # 9. å®Œæˆ
    logger.info("=" * 70)
    logger.info("âœ… åˆ†æå®Œæˆï¼")
    logger.info("=" * 70)
    logger.info(f"\nğŸ¯ æ ¸å¿ƒæŒ‡æ¨™:")
    logger.info(f"   ç¸½èƒ½åŠ›æ•¸:   {len(capabilities)}")
    logger.info(f"   æˆåŠŸç‡:     {analyzer.get_extraction_report()['success_rate']:.1f}%")
    logger.info(f"   è¦†è“‹èªè¨€:   {len(lang_counts)}")
    logger.info(f"   è¦†è“‹æ¨¡çµ„:   {len(grouped)}")
    logger.info("")
    
    return capabilities


if __name__ == "__main__":
    try:
        capabilities = asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  åˆ†æè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
