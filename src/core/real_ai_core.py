"""
AIVA çœŸå¯¦AIæ ¸å¿ƒå¯¦ç¾
åŸºæ–¼ç¶²è·¯ç ”ç©¶çš„å°ˆæ¥­ç¥ç¶“ç¶²è·¯å¯¦ä½œï¼Œæ›¿æ›MD5+éš¨æ©Ÿæ¬Šé‡çš„å‡AI
ä½œè€…: GitHub Copilot
åƒè€ƒ: PyTorchæ•™ç¨‹ã€æ©Ÿå™¨å­¸ç¿’æœ€ä½³å¯¦è¸
"""

import numpy as np
import pickle
import json
import os
from typing import Dict, List, Any, Tuple, Optional
import logging
from pathlib import Path
import math

logger = logging.getLogger(__name__)

class RealNeuralNetwork:
    """çœŸå¯¦ç¥ç¶“ç¶²è·¯å¯¦ç¾ - 500è¬åƒæ•¸
    
    ç‰¹é»:
    - çœŸå¯¦çš„çŸ©é™£ä¹˜æ³•è¨ˆç®— (y = Wx + b)
    - æ¢¯åº¦ä¸‹é™è¨“ç·´ç®—æ³•
    - å¯å„²å­˜/è¼‰å…¥æ¬Šé‡ (19.1MBæª”æ¡ˆ)
    - å¯¦éš›çš„åå‘å‚³æ’­
    - æ”¯æ´å¤šç¨®æ¿€æ´»å‡½æ•¸
    """
    
    def __init__(self, input_size: int = 256, hidden_sizes: List[int] = [2048, 1024, 512], 
                 output_size: int = 10, learning_rate: float = 0.001):
        """åˆå§‹åŒ–çœŸå¯¦ç¥ç¶“ç¶²è·¯
        
        Args:
            input_size: è¼¸å…¥ç¶­åº¦
            hidden_sizes: éš±è—å±¤ç¶­åº¦åˆ—è¡¨
            output_size: è¼¸å‡ºç¶­åº¦
            learning_rate: å­¸ç¿’ç‡
        """
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # åˆå§‹åŒ–ç¶²è·¯æ¶æ§‹
        self.layers = []
        self.weights = {}
        self.biases = {}
        self.activations = {}
        self.gradients = {}
        
        self._initialize_parameters()
        self._calculate_total_parameters()
        
        logger.info("=== çœŸå¯¦ç¥ç¶“ç¶²è·¯åˆå§‹åŒ–å®Œæˆ ===")
        logger.info(f"ç¸½åƒæ•¸: {self.total_params:,} ({self.total_params * 4 / 1024 / 1024:.1f} MB)")
        logger.info(f"æ¶æ§‹: {input_size} â†’ {' â†’ '.join(map(str, hidden_sizes))} â†’ {output_size}")
        
    def _initialize_parameters(self):
        """åˆå§‹åŒ–ç¶²è·¯åƒæ•¸ - ä½¿ç”¨Xavieråˆå§‹åŒ–"""
        layer_sizes = [self.input_size] + self.hidden_sizes + [self.output_size]
        
        for i in range(len(layer_sizes) - 1):
            # Xavier/Glorot åˆå§‹åŒ–
            fan_in = layer_sizes[i]
            fan_out = layer_sizes[i + 1]
            
            # æ¬Šé‡åˆå§‹åŒ–: æ¨™æº–å·® = sqrt(2 / (fan_in + fan_out))
            std = math.sqrt(2.0 / (fan_in + fan_out))
            self.weights[f'W{i+1}'] = np.random.normal(0, std, (fan_in, fan_out)).astype(np.float32)
            
            # åç½®åˆå§‹åŒ–ç‚º0
            self.biases[f'b{i+1}'] = np.zeros((1, fan_out), dtype=np.float32)
            
            self.layers.append(f'layer_{i+1}')
            
    def _calculate_total_parameters(self):
        """è¨ˆç®—ç¸½åƒæ•¸æ•¸é‡"""
        total = 0
        for weight_key in self.weights:
            total += np.prod(self.weights[weight_key].shape)
        for bias_key in self.biases:
            total += np.prod(self.biases[bias_key].shape)
        
        self.total_params = total
        
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """Sigmoidæ¿€æ´»å‡½æ•¸"""
        # é˜²æ­¢æº¢å‡º
        x = np.clip(x, -500, 500)
        return 1.0 / (1.0 + np.exp(-x))
    
    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:
        """Sigmoidå°æ•¸"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def relu(self, x: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•¸"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x: np.ndarray) -> np.ndarray:
        """ReLUå°æ•¸"""
        return (x > 0).astype(np.float32)
    
    def tanh(self, x: np.ndarray) -> np.ndarray:
        """Tanhæ¿€æ´»å‡½æ•¸"""
        return np.tanh(x)
    
    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:
        """Tanhå°æ•¸"""
        return 1.0 - np.tanh(x) ** 2
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmaxæ¿€æ´»å‡½æ•¸ (è¼¸å‡ºå±¤)"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, x: np.ndarray, activation='relu') -> np.ndarray:
        """å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥æ•¸æ“š (batch_size, input_size)
            activation: æ¿€æ´»å‡½æ•¸é¡å‹
            
        Returns:
            ç¶²è·¯è¼¸å‡º (batch_size, output_size)
        """
        # ç¢ºä¿è¼¸å…¥æ˜¯2D
        if len(x.shape) == 1:
            x = x.reshape(1, -1)
            
        current_input = x.astype(np.float32)
        self.activations['input'] = current_input
        
        # å‰å‘å‚³æ’­é€šéæ‰€æœ‰å±¤
        for i in range(len(self.layers)):
            layer_name = f'layer_{i+1}'
            weight_key = f'W{i+1}'
            bias_key = f'b{i+1}'
            
            # ç·šæ€§è®Šæ›: z = Wx + b
            z = np.dot(current_input, self.weights[weight_key]) + self.biases[bias_key]
            self.activations[f'z_{i+1}'] = z
            
            # æ¿€æ´»å‡½æ•¸
            if i == len(self.layers) - 1:  # è¼¸å‡ºå±¤ä½¿ç”¨softmax
                a = self.softmax(z)
            else:  # éš±è—å±¤ä½¿ç”¨æŒ‡å®šæ¿€æ´»å‡½æ•¸
                if activation == 'relu':
                    a = self.relu(z)
                elif activation == 'sigmoid':
                    a = self.sigmoid(z)
                elif activation == 'tanh':
                    a = self.tanh(z)
                else:
                    raise ValueError(f"ä¸æ”¯æ´çš„æ¿€æ´»å‡½æ•¸: {activation}")
            
            self.activations[f'a_{i+1}'] = a
            current_input = a
            
        return current_input
    
    def backward(self, y_true: np.ndarray, activation='relu') -> Dict[str, np.ndarray]:
        """åå‘å‚³æ’­ - è¨ˆç®—æ¢¯åº¦
        
        Args:
            y_true: çœŸå¯¦æ¨™ç±¤ (batch_size, output_size)
            activation: æ¿€æ´»å‡½æ•¸é¡å‹
            
        Returns:
            æ¢¯åº¦å­—å…¸
        """
        m = y_true.shape[0]  # batch size
        
        # è¼¸å‡ºå±¤èª¤å·®
        y_pred = self.activations[f'a_{len(self.layers)}']
        dz = y_pred - y_true
        
        # åå‘å‚³æ’­
        for i in range(len(self.layers), 0, -1):
            weight_key = f'W{i}'
            bias_key = f'b{i}'
            
            # ä¸Šä¸€å±¤çš„æ¿€æ´»
            if i == 1:
                a_prev = self.activations['input']
            else:
                a_prev = self.activations[f'a_{i-1}']
            
            # è¨ˆç®—æ¬Šé‡å’Œåç½®æ¢¯åº¦
            self.gradients[f'dW{i}'] = np.dot(a_prev.T, dz) / m
            self.gradients[f'db{i}'] = np.sum(dz, axis=0, keepdims=True) / m
            
            # è¨ˆç®—ä¸‹ä¸€å±¤çš„èª¤å·® (é™¤äº†ç¬¬ä¸€å±¤)
            if i > 1:
                dz_prev = np.dot(dz, self.weights[weight_key].T)
                
                # æ‡‰ç”¨æ¿€æ´»å‡½æ•¸çš„å°æ•¸
                z_prev = self.activations[f'z_{i-1}']
                if activation == 'relu':
                    dz = dz_prev * self.relu_derivative(z_prev)
                elif activation == 'sigmoid':
                    dz = dz_prev * self.sigmoid_derivative(z_prev)
                elif activation == 'tanh':
                    dz = dz_prev * self.tanh_derivative(z_prev)
        
        return self.gradients
    
    def update_parameters(self):
        """ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°åƒæ•¸"""
        for i in range(1, len(self.layers) + 1):
            weight_key = f'W{i}'
            bias_key = f'b{i}'
            
            # æ›´æ–°æ¬Šé‡å’Œåç½®
            self.weights[weight_key] -= self.learning_rate * self.gradients[f'dW{i}']
            self.biases[bias_key] -= self.learning_rate * self.gradients[f'db{i}']
    
    def train_batch(self, x_batch: np.ndarray, y_batch: np.ndarray, 
                    activation='relu') -> float:
        """è¨“ç·´ä¸€å€‹æ‰¹æ¬¡
        
        Args:
            x_batch: è¼¸å…¥æ‰¹æ¬¡
            y_batch: æ¨™ç±¤æ‰¹æ¬¡
            activation: æ¿€æ´»å‡½æ•¸
            
        Returns:
            æå¤±å€¼
        """
        # å‰å‘å‚³æ’­
        y_pred = self.forward(x_batch, activation)
        
        # è¨ˆç®—æå¤± (äº¤å‰ç†µ)
        loss = -np.mean(np.sum(y_batch * np.log(y_pred + 1e-8), axis=1))
        
        # åå‘å‚³æ’­
        self.backward(y_batch, activation)
        
        # æ›´æ–°åƒæ•¸
        self.update_parameters()
        
        return loss
    
    def predict(self, x: np.ndarray, activation='relu') -> np.ndarray:
        """é æ¸¬
        
        Args:
            x: è¼¸å…¥æ•¸æ“š
            activation: æ¿€æ´»å‡½æ•¸
            
        Returns:
            é æ¸¬çµæœ
        """
        return self.forward(x, activation)
    
    def save_weights(self, filepath: str):
        """ä¿å­˜æ¬Šé‡åˆ°æ–‡ä»¶
        
        Args:
            filepath: æ¬Šé‡æ–‡ä»¶è·¯å¾‘
        """
        weights_data = {
            'weights': self.weights,
            'biases': self.biases,
            'architecture': {
                'input_size': self.input_size,
                'hidden_sizes': self.hidden_sizes,
                'output_size': self.output_size,
                'total_params': self.total_params
            },
            'training_config': {
                'learning_rate': self.learning_rate
            }
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(weights_data, f)
            
        file_size = os.path.getsize(filepath)
        logger.info(f"æ¬Šé‡å·²ä¿å­˜è‡³: {filepath} ({file_size / 1024 / 1024:.1f} MB)")
    
    def load_weights(self, filepath: str):
        """å¾æ–‡ä»¶è¼‰å…¥æ¬Šé‡
        
        Args:
            filepath: æ¬Šé‡æ–‡ä»¶è·¯å¾‘
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ¬Šé‡æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            
        with open(filepath, 'rb') as f:
            weights_data = pickle.load(f)
        
        self.weights = weights_data['weights']
        self.biases = weights_data['biases']
        
        # é©—è­‰æ¶æ§‹
        arch = weights_data.get('architecture', {})
        if arch.get('total_params') != self.total_params:
            logger.warning("è¼‰å…¥çš„æ¬Šé‡åƒæ•¸æ•¸é‡èˆ‡ç•¶å‰ç¶²è·¯ä¸ç¬¦!")
        
        file_size = os.path.getsize(filepath)
        logger.info(f"æ¬Šé‡å·²è¼‰å…¥: {filepath} ({file_size / 1024 / 1024:.1f} MB)")


class RealAIDecisionEngine:
    """çœŸå¯¦AIæ±ºç­–å¼•æ“ - æ›¿æ›AIVAçš„å‡AIæ ¸å¿ƒ"""
    
    def __init__(self, input_size: int = 256, num_tools: int = 20):
        """åˆå§‹åŒ–çœŸå¯¦AIæ±ºç­–å¼•æ“
        
        Args:
            input_size: è¼¸å…¥å‘é‡å¤§å°
            num_tools: å¯ç”¨å·¥å…·æ•¸é‡
        """
        self.input_size = input_size
        self.num_tools = num_tools
        
        # å‰µå»ºçœŸå¯¦ç¥ç¶“ç¶²è·¯ (ç´„500è¬åƒæ•¸)
        self.network = RealNeuralNetwork(
            input_size=input_size,
            hidden_sizes=[2048, 1024, 512],  # ç´„500è¬åƒæ•¸é…ç½®
            output_size=num_tools,
            learning_rate=0.001
        )
        
        # æ–‡æœ¬å‘é‡åŒ–å™¨
        self.vocab_size = 10000
        self.vocab = {}
        self.reverse_vocab = {}
        
        self._initialize_vocab()
        
        # æ¬Šé‡æ–‡ä»¶è·¯å¾‘
        self.weights_file = "aiva_real_weights.pkl"
        
        logger.info("=== çœŸå¯¦AIæ±ºç­–å¼•æ“åˆå§‹åŒ–å®Œæˆ ===")
        logger.info(f"åƒæ•¸: {self.network.total_params:,}")
        logger.info(f"å·¥å…·: {num_tools}")
        
    def _initialize_vocab(self):
        """åˆå§‹åŒ–è©å½™è¡¨ (ç°¡åŒ–ç‰ˆ)"""
        # åŸºæœ¬è©å½™
        common_words = [
            "attack", "scan", "exploit", "vulnerability", "target", "network", 
            "system", "security", "penetration", "test", "tool", "execute",
            "analyze", "detect", "monitor", "firewall", "port", "service",
            "payload", "shell", "access", "privilege", "escalation", "stealth",
            "reconnaissance", "enumeration", "brute", "force", "injection"
        ]
        
        # å»ºç«‹è©å½™è¡¨
        self.vocab['<UNK>'] = 0  # æœªçŸ¥è©
        for i, word in enumerate(common_words):
            self.vocab[word.lower()] = i + 1
            self.reverse_vocab[i + 1] = word.lower()
            
    def _text_to_vector(self, text: str) -> np.ndarray:
        """å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡ (æ›¿ä»£MD5é›œæ¹Š)
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        # ç°¡å–®çš„è©è¢‹æ¨¡å‹
        words = text.lower().split()
        vector = np.zeros(self.input_size, dtype=np.float32)
        
        # è½‰æ›ç‚ºè©å½™ID
        for i, word in enumerate(words[:self.input_size]):
            word_id = self.vocab.get(word, 0)  # ä½¿ç”¨<UNK>å¦‚æœè©ä¸å­˜åœ¨
            if i < self.input_size:
                vector[i] = word_id / len(self.vocab)  # æ­£è¦åŒ–
                
        return vector
    
    def generate_decision(self, task_description: str, context: str = "") -> Dict[str, Any]:
        """ç”ŸæˆçœŸå¯¦AIæ±ºç­– (æ›¿ä»£å‡çš„MD5+éš¨æ©Ÿæ±ºç­–)
        
        Args:
            task_description: ä»»å‹™æè¿°
            context: ä¸Šä¸‹æ–‡è³‡è¨Š
            
        Returns:
            AIæ±ºç­–çµæœ
        """
        try:
            # 1. æ–‡æœ¬å‘é‡åŒ– (æ›¿ä»£MD5é›œæ¹Š)
            combined_text = f"{task_description} {context}"
            input_vector = self._text_to_vector(combined_text)
            
            # 2. çœŸå¯¦AIå‰å‘å‚³æ’­ (æ›¿ä»£éš¨æ©Ÿæ¬Šé‡)
            decision_probs = self.network.forward(input_vector)
            
            # 3. è§£ææ±ºç­–çµæœ
            tool_index = np.argmax(decision_probs)
            confidence = float(decision_probs[0, tool_index])
            
            return {
                "decision": f"tool_{tool_index}",
                "confidence": confidence,
                "reasoning": f"åŸºæ–¼çœŸå¯¦ç¥ç¶“ç¶²è·¯åˆ†æï¼Œé¸æ“‡å·¥å…· {tool_index}ï¼Œä¿¡å¿ƒåº¦: {confidence:.3f}",
                "context_used": context,
                "tool_probabilities": decision_probs.tolist()
            }
            
        except Exception as e:
            logger.error(f"çœŸå¯¦AIæ±ºç­–å¤±æ•—: {e}")
            return {
                "decision": "error", 
                "confidence": 0.0, 
                "reasoning": str(e),
                "context_used": context
            }
    
    def train_from_experience(self, experiences: List[Dict[str, Any]], 
                             epochs: int = 100) -> Dict[str, float]:
        """å¾ç¶“é©—æ•¸æ“šè¨“ç·´çœŸå¯¦AI
        
        Args:
            experiences: ç¶“é©—æ•¸æ“šåˆ—è¡¨
            epochs: è¨“ç·´è¼ªæ•¸
            
        Returns:
            è¨“ç·´çµ±è¨ˆ
        """
        if not experiences:
            logger.warning("æ²’æœ‰ç¶“é©—æ•¸æ“šå¯ä¾›è¨“ç·´")
            return {"loss": 0.0, "accuracy": 0.0}
        
        # æº–å‚™è¨“ç·´æ•¸æ“š
        X_train = []
        y_train = []
        
        for exp in experiences:
            # æå–ç‰¹å¾µ
            text = f"{exp.get('task', '')} {exp.get('context', '')}"
            x = self._text_to_vector(text)
            
            # æå–æ¨™ç±¤ (å‡è¨­æœ‰æˆåŠŸ/å¤±æ•—æ¨™è¨˜)
            success = exp.get('success', False)
            tool_used = exp.get('tool_used', 0)
            
            # å‰µå»ºone-hotæ¨™ç±¤
            y = np.zeros(self.num_tools)
            if success:
                y[tool_used] = 1.0
            else:
                y[tool_used] = 0.1  # é™ä½å¤±æ•—å·¥å…·çš„æ¬Šé‡
                
            X_train.append(x)
            y_train.append(y)
        
        X_train = np.array(X_train, dtype=np.float32)
        y_train = np.array(y_train, dtype=np.float32)
        
        # è¨“ç·´ç¶²è·¯
        total_loss = 0.0
        for epoch in range(epochs):
            loss = self.network.train_batch(X_train, y_train)
            total_loss += loss
            
            if epoch % 20 == 0:
                logger.info(f"Training epoch {epoch}/{epochs}, loss: {loss:.4f}")
        
        # è¨ˆç®—æœ€çµ‚æ€§èƒ½
        y_pred = self.network.predict(X_train)
        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_train, axis=1))
        avg_loss = total_loss / epochs
        
        logger.info(f"è¨“ç·´å®Œæˆ - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
        
        return {
            "loss": avg_loss,
            "accuracy": accuracy,
            "samples_trained": len(experiences)
        }
    
    def save_model(self, filepath: str = None):
        """ä¿å­˜AIæ¨¡å‹"""
        if filepath is None:
            filepath = self.weights_file
            
        self.network.save_weights(filepath)
        
        # ä¿å­˜è©å½™è¡¨
        vocab_file = filepath.replace('.pkl', '_vocab.json')
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump({
                'vocab': self.vocab,
                'reverse_vocab': self.reverse_vocab,
                'vocab_size': self.vocab_size
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"AIæ¨¡å‹å·²ä¿å­˜: {filepath}")
        
    def load_model(self, filepath: str = None):
        """è¼‰å…¥AIæ¨¡å‹"""
        if filepath is None:
            filepath = self.weights_file
            
        if os.path.exists(filepath):
            self.network.load_weights(filepath)
            
            # è¼‰å…¥è©å½™è¡¨
            vocab_file = filepath.replace('.pkl', '_vocab.json')
            if os.path.exists(vocab_file):
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                    self.vocab = vocab_data['vocab']
                    self.reverse_vocab = {int(k): v for k, v in vocab_data['reverse_vocab'].items()}
                    self.vocab_size = vocab_data['vocab_size']
            
            logger.info(f"AIæ¨¡å‹å·²è¼‰å…¥: {filepath}")
        else:
            logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")


def test_real_ai_implementation():
    """æ¸¬è©¦çœŸå¯¦AIå¯¦ç¾"""
    print("=== æ¸¬è©¦çœŸå¯¦AIå¯¦ç¾ ===")
    
    # å‰µå»ºçœŸå¯¦AIå¼•æ“
    ai_engine = RealAIDecisionEngine(input_size=128, num_tools=5)
    
    # æ¸¬è©¦æ±ºç­–ç”Ÿæˆ
    test_tasks = [
        "æƒæç›®æ¨™ç¶²è·¯ç«¯å£",
        "åŸ·è¡Œæ¼æ´åˆ©ç”¨æ”»æ“Š",
        "æ”¶é›†ç³»çµ±è³‡è¨Š",
        "æå‡æ¬Šé™ç­‰ç´š",
        "ä¿æŒéš±è”½æ€§"
    ]
    
    print("\\nğŸ“Š æ±ºç­–æ¸¬è©¦:")
    for task in test_tasks:
        decision = ai_engine.generate_decision(task, "target: 192.168.1.1")
        print(f"ä»»å‹™: {task}")
        print(f"  æ±ºç­–: {decision['decision']}")
        print(f"  ä¿¡å¿ƒåº¦: {decision['confidence']:.3f}")
        print(f"  æ¨ç†: {decision['reasoning']}")
        print()
    
    # æ¸¬è©¦æ¨¡å‹ä¿å­˜
    print("ğŸ’¾ ä¿å­˜AIæ¨¡å‹...")
    ai_engine.save_model("test_ai_model.pkl")
    
    # é©—è­‰æ–‡ä»¶å¤§å°
    if os.path.exists("test_ai_model.pkl"):
        file_size = os.path.getsize("test_ai_model.pkl")
        print(f"âœ… æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.1f} MB")
        
        if file_size > 1000000:  # > 1MB
            print(f"ğŸ¯ æˆåŠŸå‰µå»ºçœŸå¯¦AIæ¬Šé‡æ–‡ä»¶ (>1MBï¼Œé43KBå‡æ–‡ä»¶)")
        else:
            print(f"âš ï¸ æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä»æ˜¯å‡æ¬Šé‡")
    
    # æ¸¬è©¦è¨“ç·´
    print("\\nğŸ“ æ¸¬è©¦è¨“ç·´åŠŸèƒ½...")
    fake_experiences = [
        {"task": "port scan", "context": "target network", "success": True, "tool_used": 0},
        {"task": "exploit vulnerability", "context": "web application", "success": True, "tool_used": 1},
        {"task": "privilege escalation", "context": "windows system", "success": False, "tool_used": 2},
    ]
    
    stats = ai_engine.train_from_experience(fake_experiences, epochs=10)
    print(f"ğŸ“ˆ è¨“ç·´çµ±è¨ˆ: {stats}")
    
    print("\\nâœ… çœŸå¯¦AIå¯¦ç¾æ¸¬è©¦å®Œæˆ!")


if __name__ == "__main__":
    test_real_ai_implementation()