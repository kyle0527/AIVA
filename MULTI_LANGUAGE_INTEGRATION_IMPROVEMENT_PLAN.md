# AIVA å¤šèªè¨€æ•´åˆæ”¹å–„å»ºè­°å ±å‘Š

**æ—¥æœŸ**: 2025-11-16  
**ç‹€æ…‹**: ğŸ”„ æ”¹å–„å»ºè­°  
**åŸºæ–¼**: MULTI_LANGUAGE_ANALYSIS_INTEGRATION_REPORT.md åˆ†æ

---

## ğŸ“Š ç¾æ³è©•ä¼°

### ç•¶å‰æ¶æ§‹å„ªå‹¢ âœ…

1. **æ¸…æ™°çš„æ¨¡çµ„åŠƒåˆ†**
   - **äº”å¤§é ‚å±¤æ¨¡çµ„** (services/):
     - `core/` - æ ¸å¿ƒå¼•æ“ (AIVA Core)
     - `scan/` - æƒææœå‹™ (Go, Rust, TypeScript)
     - `features/` - åŠŸèƒ½æ¨¡çµ„ (å¤šèªè¨€å¯¦ç¾)
     - `integration/` - æ•´åˆæœå‹™
     - `aiva_common/` - å…±äº«è¦ç¯„å’Œ Schema

   - **å…­å¤§æ ¸å¿ƒå­æ¨¡çµ„** (core/aiva_core/):
     - `cognitive_core/` - èªçŸ¥æ ¸å¿ƒ (AI å¤§è…¦)
     - `core_capabilities/` - æ ¸å¿ƒèƒ½åŠ›
     - `task_planning/` - ä»»å‹™è¦åŠƒ
     - `service_backbone/` - æœå‹™éª¨å¹¹
     - `internal_exploration/` - å…§éƒ¨æ¢ç´¢ âœ¨
     - `external_learning/` - å¤–éƒ¨å­¸ç¿’ âœ¨

2. **å¤šèªè¨€æ”¯æ´å·²å¯¦ç¾**
   - Python: 410 å€‹èƒ½åŠ› (AST ç²¾ç¢ºè§£æ)
   - Go: 88 å€‹èƒ½åŠ› (æ­£å‰‡æå–)
   - TypeScript: 78 å€‹èƒ½åŠ› (æ­£å‰‡æå–)
   - ç¸½è¨ˆ: 576 å€‹èƒ½åŠ›è¦†è“‹

3. **çµ±ä¸€çš„æ•¸æ“šåˆç´„**
   - `aiva_common` æä¾›è·¨èªè¨€ Schema
   - Protocol Buffers / JSON Schema å®šç¾©
   - ç¢ºä¿é¡å‹å®‰å…¨å’Œä¸€è‡´æ€§

### å·²çŸ¥å•é¡Œèˆ‡é™åˆ¶ âš ï¸

| å•é¡Œ | å½±éŸ¿ç¯„åœ | åš´é‡ç¨‹åº¦ | ç¾æ³ |
|------|---------|---------|-----|
| Rust çµæ§‹é«”æ–¹æ³•æœªæå– | 7 å€‹ Rust æ–‡ä»¶ | P3 ä½ | 0 å€‹èƒ½åŠ› |
| JavaScript é›¶æå– | 8 å€‹ JS æ–‡ä»¶ | P4 å¾ˆä½ | å¯èƒ½ç‚ºé…ç½®æ–‡ä»¶ |
| ç¼ºä¹æ¸¬è©¦è¦†è“‹ | å¤šèªè¨€æå–é‚è¼¯ | P1 é«˜ | æ‰‹å‹•é©—è­‰ç‚ºä¸» |
| æ€§èƒ½æœªå„ªåŒ– | 380 æ–‡ä»¶åŒæ­¥æƒæ | P2 ä¸­ | å–®ç·šç¨‹è™•ç† |
| éŒ¯èª¤è™•ç†ä¸å®Œå–„ | æ–‡ä»¶è®€å–å¤±æ•— | P2 ä¸­ | ç°¡å–® try-catch |

---

## ğŸ¯ æ”¹å–„å»ºè­° (ç¶­æŒäº”å¤§æ¨¡çµ„ + å…­å¤§æ ¸å¿ƒæ¶æ§‹)

### Phase 1: å¼·åŒ–å¤šèªè¨€åˆ†æèƒ½åŠ› (P0 - ç«‹å³åŸ·è¡Œ)

#### 1.1 å¢å¼· Rust æå–å™¨

**å•é¡Œ**: ç•¶å‰ `RustExtractor` åªåŒ¹é…é ‚å±¤ `pub fn`,ç„¡æ³•æå– `impl` å€å¡Šå…§çš„æ–¹æ³•

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# services/core/aiva_core/internal_exploration/language_extractors.py

class RustExtractor(LanguageExtractor):
    """Rust èªè¨€å‡½æ•¸å’Œæ–¹æ³•æå–å™¨ (å¢å¼·ç‰ˆ)"""
    
    # âœ… æ–°å¢: impl å…§éƒ¨æ–¹æ³•æ¨¡å¼
    IMPL_METHOD_PATTERN = re.compile(
        r'impl\s+(?:<[^>]*>\s+)?(\w+)\s*(?:<[^>]*>)?\s*\{([^}]*(?:\{[^}]*\}[^}]*)*)\}',
        re.DOTALL | re.MULTILINE
    )
    
    # åŸæœ‰é ‚å±¤å‡½æ•¸æ¨¡å¼ä¿æŒä¸è®Š
    FUNCTION_PATTERN = re.compile(
        r'pub\s+(?:async\s+)?fn\s+(\w+)\s*\(([^)]*)\)\s*(?:->\s*([^{;]+))?',
        re.MULTILINE
    )
    
    def extract_capabilities(self, content: str, file_path: str) -> list[dict[str, Any]]:
        """å¾ Rust æºç¢¼æå–å…¬é–‹å‡½æ•¸å’Œæ–¹æ³•"""
        capabilities = []
        
        # 1. æå–é ‚å±¤ pub fn (ä¿æŒåŸæœ‰é‚è¼¯)
        capabilities.extend(self._extract_top_level_functions(content, file_path))
        
        # 2. âœ… æ–°å¢: æå– impl å€å¡Šæ–¹æ³•
        capabilities.extend(self._extract_impl_methods(content, file_path))
        
        logger.debug(f"Extracted {len(capabilities)} Rust capabilities from {file_path}")
        return capabilities
    
    def _extract_impl_methods(self, content: str, file_path: str) -> list[dict[str, Any]]:
        """æå– impl å€å¡Šå…§çš„å…¬é–‹æ–¹æ³•
        
        è™•ç†æ¨¡å¼:
        impl SensitiveInfoScanner {
            pub fn scan_content(&self, ...) -> Result<...> { ... }
        }
        """
        capabilities = []
        
        for impl_match in self.IMPL_METHOD_PATTERN.finditer(content):
            struct_name = impl_match.group(1)
            impl_body = impl_match.group(2)
            
            # åœ¨ impl å€å¡Šå…§æŸ¥æ‰¾ pub fn
            method_pattern = re.compile(
                r'pub\s+(?:async\s+)?fn\s+(\w+)\s*\(([^)]*)\)\s*(?:->\s*([^{;]+))?',
                re.MULTILINE
            )
            
            for method_match in method_pattern.finditer(impl_body):
                method_name = method_match.group(1)
                params = method_match.group(2)
                return_type = method_match.group(3)
                
                # è·³é new å’Œç§æœ‰æ–¹æ³•
                if method_name.startswith('_'):
                    continue
                
                capability = {
                    "name": f"{struct_name}::{method_name}",  # å®Œæ•´è·¯å¾‘
                    "language": "rust",
                    "file_path": file_path,
                    "struct": struct_name,
                    "method": method_name,
                    "parameters": self._parse_rust_params(params),
                    "return_type": return_type.strip() if return_type else None,
                    "description": f"Rust method: {struct_name}::{method_name}",
                    "is_async": 'async' in method_match.group(0),
                    "is_method": True,  # æ¨™è¨˜ç‚ºæ–¹æ³•
                }
                
                capabilities.append(capability)
        
        return capabilities
```

**é æœŸæ•ˆæœ**:
- Rust æ–‡ä»¶èƒ½åŠ›æ•¸: 0 â†’ 40+ (ä¼°è¨ˆ)
- è¦†è“‹ `SensitiveInfoScanner`, `SecretDetector`, `Verifier` ç­‰é¡

#### 1.2 é©—è­‰ JavaScript æ–‡ä»¶

**è¡Œå‹•é …**:
```powershell
# 1. æª¢æŸ¥ JS æ–‡ä»¶é¡å‹
Get-ChildItem -Path "C:\D\fold7\AIVA-git\services" -Recurse -Filter "*.js" | 
    Select-Object Name, Directory | Format-Table

# 2. æœå°‹å°å‡ºæ¨¡å¼
Select-String -Path "C:\D\fold7\AIVA-git\services\**\*.js" `
    -Pattern "(export |module\.exports)" | 
    Select-Object Path, LineNumber
```

**æ¢ä»¶æ€§å¢å¼·**:
- å¦‚æœæ˜¯é…ç½®æ–‡ä»¶ (`.config.js`, `.spec.js`) â†’ è·³é
- å¦‚æœæ˜¯ CommonJS æ¨¡çµ„ â†’ å¢åŠ  `module.exports` æ¨¡å¼
```python
# TypeScriptExtractor å¢åŠ  CommonJS æ”¯æ´
COMMONJS_PATTERN = re.compile(
    r'module\.exports\s*=\s*\{[^}]*(\w+)\s*:',
    re.MULTILINE
)
```

---

### Phase 2: æå‡å¯é æ€§èˆ‡å¯ç¶­è­·æ€§ (P1 - 1-2 é€±)

#### 2.1 å®Œå–„æ¸¬è©¦æ¡†æ¶

**å‰µå»ºå®Œæ•´æ¸¬è©¦å¥—ä»¶**:
```python
# services/core/aiva_core/tests/test_multi_language_extraction.py

import pytest
from pathlib import Path
from internal_exploration.capability_analyzer import CapabilityAnalyzer
from internal_exploration.module_explorer import ModuleExplorer

class TestMultiLanguageExtraction:
    """å¤šèªè¨€èƒ½åŠ›æå–æ¸¬è©¦"""
    
    @pytest.fixture
    def analyzer(self):
        return CapabilityAnalyzer()
    
    @pytest.fixture
    def explorer(self):
        return ModuleExplorer()
    
    @pytest.mark.asyncio
    async def test_python_ast_extraction(self, analyzer):
        """æ¸¬è©¦ Python AST æå–"""
        # æ¸¬è©¦å¸¶ @capability è£é£¾å™¨çš„å‡½æ•¸
        test_file = Path(__file__).parent / "fixtures" / "test_python.py"
        caps = analyzer._extract_python_capabilities(test_file, "test_module")
        
        assert len(caps) > 0
        assert all(cap["language"] == "python" for cap in caps)
        assert all("name" in cap for cap in caps)
    
    @pytest.mark.asyncio
    async def test_go_extraction(self, analyzer):
        """æ¸¬è©¦ Go å‡½æ•¸æå–"""
        test_file = Path(__file__).parent / "fixtures" / "test_scanner.go"
        caps = analyzer._extract_non_python_capabilities(
            test_file, "test_module", "go"
        )
        
        assert len(caps) > 0
        assert all(cap["language"] == "go" for cap in caps)
        # Go åªæå–å¤§å¯«é–‹é ­ (å°å‡ºå‡½æ•¸)
        assert all(cap["name"][0].isupper() for cap in caps)
    
    @pytest.mark.asyncio
    async def test_rust_impl_methods(self, analyzer):
        """æ¸¬è©¦ Rust impl æ–¹æ³•æå–"""
        test_file = Path(__file__).parent / "fixtures" / "test_scanner.rs"
        caps = analyzer._extract_non_python_capabilities(
            test_file, "test_module", "rust"
        )
        
        # æ‡‰è©²æå– impl å€å¡Šå…§çš„ pub fn
        assert len(caps) > 0
        method_caps = [c for c in caps if c.get("is_method")]
        assert len(method_caps) > 0
    
    @pytest.mark.asyncio
    async def test_typescript_export_patterns(self, analyzer):
        """æ¸¬è©¦ TypeScript å¤šç¨®å°å‡ºæ¨¡å¼"""
        test_cases = [
            ("export function test() {}", True),
            ("export const test = () => {}", True),
            ("private test() {}", False),
            ("function internal() {}", False),
        ]
        
        for code, should_extract in test_cases:
            caps = analyzer._extract_non_python_capabilities(
                Path("test.ts"), "test", "typescript"
            )
            # é©—è­‰æå–é‚è¼¯
    
    def test_language_detection(self, analyzer):
        """æ¸¬è©¦èªè¨€æª¢æ¸¬"""
        test_cases = [
            ("test.py", "python"),
            ("test.go", "go"),
            ("test.rs", "rust"),
            ("test.ts", "typescript"),
            ("test.js", "javascript"),
        ]
        
        for filename, expected_lang in test_cases:
            detected = analyzer._detect_language(Path(filename))
            assert detected == expected_lang
    
    @pytest.mark.asyncio
    async def test_error_handling(self, analyzer):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†"""
        # ä¸å­˜åœ¨çš„æ–‡ä»¶
        caps = await analyzer._extract_capabilities_from_file(
            Path("nonexistent.py"), "test"
        )
        assert caps == []
        
        # èªæ³•éŒ¯èª¤çš„ Python æ–‡ä»¶
        # ... (å‰µå»ºæ¸¬è©¦ fixture)
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_full_workspace_scan(self, explorer, analyzer):
        """æ•´åˆæ¸¬è©¦: å®Œæ•´å·¥ä½œå€æƒæ"""
        modules = await explorer.explore_all_modules()
        capabilities = await analyzer.analyze_capabilities(modules)
        
        # é©—è­‰çµ±è¨ˆæ•¸æ“š
        assert len(capabilities) > 500  # ç¸½èƒ½åŠ›æ•¸
        
        languages = {cap["language"] for cap in capabilities}
        assert "python" in languages
        assert "go" in languages
        assert "typescript" in languages
        
        # é©—è­‰æ¯ç¨®èªè¨€éƒ½æœ‰æå–
        lang_counts = {}
        for cap in capabilities:
            lang = cap["language"]
            lang_counts[lang] = lang_counts.get(lang, 0) + 1
        
        assert lang_counts["python"] > 400
        assert lang_counts["go"] > 50
```

**æ¸¬è©¦å›ºå®šè£ç½® (Fixtures)**:
```python
# tests/fixtures/test_python.py
from aiva_core.core_capabilities import register_capability

@register_capability(
    name="test_sqli_scan",
    description="æ¸¬è©¦ SQL æ³¨å…¥æƒæ"
)
async def test_scan(target: str) -> dict:
    return {"status": "success"}
```

```go
// tests/fixtures/test_scanner.go
package scanner

// DetectSSRF æª¢æ¸¬ SSRF æ¼æ´ (å°å‡ºå‡½æ•¸)
func DetectSSRF(target string) (*Finding, error) {
    return &Finding{}, nil
}

// internal_helper å…§éƒ¨è¼”åŠ©å‡½æ•¸ (ä¸æ‡‰æå–)
func internal_helper() {}
```

```rust
// tests/fixtures/test_scanner.rs
pub struct TestScanner {
    patterns: Vec<Pattern>,
}

impl TestScanner {
    pub fn scan_content(&self, content: &str) -> Vec<Finding> {
        vec![]
    }
    
    fn internal_method(&self) {} // ç§æœ‰æ–¹æ³•,ä¸æå–
}
```

**åŸ·è¡Œè¦†è“‹ç‡å ±å‘Š**:
```powershell
# å®‰è£ pytest-cov
pip install pytest-cov

# åŸ·è¡Œæ¸¬è©¦ä¸¦ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š
pytest tests/test_multi_language_extraction.py `
    --cov=services/core/aiva_core/internal_exploration `
    --cov-report=html `
    --cov-report=term

# æŸ¥çœ‹å ±å‘Š
Start-Process .\htmlcov\index.html
```

#### 2.2 å¢å¼·éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ

**æ”¹é€² capability_analyzer.py**:
```python
import logging
from typing import Optional
from pathlib import Path
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ExtractionError:
    """æå–éŒ¯èª¤è¨˜éŒ„"""
    file_path: str
    language: str
    error_type: str
    error_message: str
    timestamp: str

class CapabilityAnalyzer:
    def __init__(self):
        self.capabilities_cache: dict[str, list[dict]] = {}
        self.extraction_errors: list[ExtractionError] = []  # âœ… æ–°å¢éŒ¯èª¤è¿½è¹¤
        logger.info("CapabilityAnalyzer initialized")
    
    async def _extract_capabilities_from_file(
        self, 
        file_path: Path, 
        module: str
    ) -> list[dict]:
        """å¾æ–‡ä»¶ä¸­æå–èƒ½åŠ› (å¢å¼·éŒ¯èª¤è™•ç†)"""
        try:
            # é©—è­‰æ–‡ä»¶å­˜åœ¨
            if not file_path.exists():
                logger.error(f"  File not found: {file_path}")
                self._record_error(
                    file_path, "unknown", "FileNotFoundError", 
                    f"File does not exist: {file_path}"
                )
                return []
            
            # é©—è­‰æ–‡ä»¶å¤§å° (è·³ééå¤§æ–‡ä»¶)
            file_size = file_path.stat().st_size
            if file_size > 5 * 1024 * 1024:  # 5MB
                logger.warning(f"  Skipping large file: {file_path} ({file_size} bytes)")
                return []
            
            # æª¢æ¸¬èªè¨€
            language = self._detect_language(file_path)
            if language == "unknown":
                logger.warning(f"  Unknown file type: {file_path.suffix}")
                return []
            
            # æå–èƒ½åŠ›
            if language == "python":
                return self._extract_python_capabilities(file_path, module)
            else:
                return self._extract_non_python_capabilities(file_path, module, language)
                
        except PermissionError as e:
            logger.error(f"  Permission denied: {file_path}")
            self._record_error(file_path, language, "PermissionError", str(e))
            return []
        
        except UnicodeDecodeError as e:
            logger.error(f"  Encoding error: {file_path}")
            self._record_error(file_path, language, "UnicodeDecodeError", str(e))
            return []
        
        except Exception as e:
            logger.exception(f"  Unexpected error extracting from {file_path}: {e}")
            self._record_error(file_path, language, type(e).__name__, str(e))
            return []
    
    def _record_error(
        self, 
        file_path: Path, 
        language: str, 
        error_type: str, 
        error_message: str
    ):
        """è¨˜éŒ„æå–éŒ¯èª¤"""
        from datetime import datetime, timezone
        
        error = ExtractionError(
            file_path=str(file_path),
            language=language,
            error_type=error_type,
            error_message=error_message,
            timestamp=datetime.now(timezone.utc).isoformat()
        )
        self.extraction_errors.append(error)
    
    def get_extraction_report(self) -> dict:
        """ç²å–æå–å ±å‘Š"""
        return {
            "total_errors": len(self.extraction_errors),
            "errors_by_type": self._group_errors_by_type(),
            "errors_by_language": self._group_errors_by_language(),
            "error_details": [
                {
                    "file": err.file_path,
                    "language": err.language,
                    "type": err.error_type,
                    "message": err.error_message[:100]  # æˆªæ–·é•·è¨Šæ¯
                }
                for err in self.extraction_errors[:10]  # å‰ 10 å€‹éŒ¯èª¤
            ]
        }
    
    def _group_errors_by_type(self) -> dict[str, int]:
        """æŒ‰éŒ¯èª¤é¡å‹åˆ†çµ„"""
        error_counts = {}
        for err in self.extraction_errors:
            error_counts[err.error_type] = error_counts.get(err.error_type, 0) + 1
        return error_counts
    
    def _group_errors_by_language(self) -> dict[str, int]:
        """æŒ‰èªè¨€åˆ†çµ„éŒ¯èª¤"""
        lang_counts = {}
        for err in self.extraction_errors:
            lang_counts[err.language] = lang_counts.get(err.language, 0) + 1
        return lang_counts
```

---

### Phase 3: æ€§èƒ½å„ªåŒ– (P2 - ä¸­æœŸ)

#### 3.1 ä¸¦è¡Œè™•ç†

**å•é¡Œ**: ç•¶å‰ 380 å€‹æ–‡ä»¶åŒæ­¥è™•ç†,è€—æ™‚è¼ƒé•·

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨ asyncio ä¸¦è¡Œè™•ç†
```python
import asyncio
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

class CapabilityAnalyzer:
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ProcessPoolExecutor(max_workers=max_workers)
    
    async def analyze_capabilities(self, modules_info: dict) -> list[dict[str, Any]]:
        """ä¸¦è¡Œåˆ†ææ¨¡çµ„èƒ½åŠ›"""
        logger.info(f"ğŸ” Starting parallel capability analysis (workers={self.max_workers})...")
        
        # æ”¶é›†æ‰€æœ‰å¾…è™•ç†æ–‡ä»¶
        file_tasks = []
        for module_name, module_data in modules_info.items():
            module_path = Path(module_data["path"])
            
            for file_info in module_data["files"]:
                file_path = module_path / file_info["path"]
                
                if file_path.name != "__init__.py":
                    file_tasks.append((file_path, module_name))
        
        # æ‰¹æ¬¡ä¸¦è¡Œè™•ç† (é¿å…éå¤šå”ç¨‹)
        batch_size = 50
        all_capabilities = []
        
        for i in range(0, len(file_tasks), batch_size):
            batch = file_tasks[i:i + batch_size]
            
            # ä¸¦è¡Œæå–
            tasks = [
                self._extract_capabilities_from_file(file_path, module)
                for file_path, module in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ”¶é›†çµæœ (éæ¿¾ç•°å¸¸)
            for result in batch_results:
                if isinstance(result, list):
                    all_capabilities.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"Batch extraction failed: {result}")
            
            logger.info(f"  Processed batch {i//batch_size + 1}/{len(file_tasks)//batch_size + 1}")
        
        logger.info(f"âœ… Parallel analysis completed: {len(all_capabilities)} capabilities")
        return all_capabilities
```

**é æœŸæ•ˆæœ**:
- è™•ç†æ™‚é–“: 30s â†’ 8s (4 workers)
- CPU ä½¿ç”¨ç‡æå‡: 25% â†’ 80%

#### 3.2 æ™ºèƒ½å¿«å–

```python
import hashlib
import json
from pathlib import Path

class CapabilityAnalyzer:
    CACHE_DIR = Path(".aiva_cache/capabilities")
    
    def __init__(self):
        self.CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _get_cache_path(self, file_path: Path) -> Path:
        """ç²å–å¿«å–æ–‡ä»¶è·¯å¾‘"""
        file_hash = self._get_file_hash(file_path)
        return self.CACHE_DIR / f"{file_hash}.json"
    
    async def _extract_capabilities_from_file(
        self, 
        file_path: Path, 
        module: str
    ) -> list[dict]:
        """æå–èƒ½åŠ› (å¸¶å¿«å–)"""
        cache_path = self._get_cache_path(file_path)
        
        # æª¢æŸ¥å¿«å–
        if cache_path.exists():
            try:
                with open(cache_path) as f:
                    cached = json.load(f)
                    logger.debug(f"  Cache hit: {file_path.name}")
                    return cached
            except Exception as e:
                logger.warning(f"  Cache read failed: {e}")
        
        # æå–èƒ½åŠ›
        capabilities = await self._do_extract(file_path, module)
        
        # å¯«å…¥å¿«å–
        try:
            with open(cache_path, 'w') as f:
                json.dump(capabilities, f, indent=2)
        except Exception as e:
            logger.warning(f"  Cache write failed: {e}")
        
        return capabilities
```

---

### Phase 4: æ¶æ§‹å¢å¼· (P3 - é•·æœŸ)

#### 4.1 èƒ½åŠ›åˆ†é¡å’Œæ¨™ç±¤

**ç›®æ¨™**: è‡ªå‹•å°‡èƒ½åŠ›åˆ†é¡ç‚ºã€Œæƒæã€ã€Œåˆ†æã€ã€Œæ”»æ“Šã€ã€Œæ•´åˆã€ç­‰

```python
from enum import Enum
from typing import Optional

class CapabilityCategory(str, Enum):
    """èƒ½åŠ›é¡åˆ¥"""
    SCANNING = "scanning"
    ANALYSIS = "analysis"
    ATTACK = "attack"
    INTEGRATION = "integration"
    UTILITY = "utility"
    UNKNOWN = "unknown"

class CapabilityClassifier:
    """èƒ½åŠ›åˆ†é¡å™¨"""
    
    KEYWORD_MAPPING = {
        CapabilityCategory.SCANNING: [
            "scan", "detect", "discover", "crawl", "probe", "enumerate"
        ],
        CapabilityCategory.ANALYSIS: [
            "analyze", "parse", "evaluate", "assess", "inspect", "verify"
        ],
        CapabilityCategory.ATTACK: [
            "exploit", "inject", "bypass", "execute", "payload", "xss", "sqli"
        ],
        CapabilityCategory.INTEGRATION: [
            "connect", "interface", "adapter", "bridge", "client", "api"
        ],
        CapabilityCategory.UTILITY: [
            "format", "convert", "serialize", "encode", "decode", "helper"
        ],
    }
    
    def classify(self, capability: dict) -> CapabilityCategory:
        """åˆ†é¡èƒ½åŠ›"""
        name = capability.get("name", "").lower()
        description = capability.get("description", "").lower()
        text = f"{name} {description}"
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„åŒ¹é…åˆ†æ•¸
        scores = {}
        for category, keywords in self.KEYWORD_MAPPING.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                scores[category] = score
        
        # è¿”å›æœ€é«˜åˆ†é¡åˆ¥
        if scores:
            return max(scores, key=scores.get)
        return CapabilityCategory.UNKNOWN
    
    def add_tags(self, capability: dict) -> dict:
        """æ·»åŠ æ¨™ç±¤"""
        capability["category"] = self.classify(capability).value
        
        # æ·»åŠ èªè¨€æ¨™ç±¤
        lang = capability.get("language", "unknown")
        capability["tags"] = [
            f"lang:{lang}",
            f"category:{capability['category']}",
        ]
        
        # æ·»åŠ æ¨¡çµ„æ¨™ç±¤
        if "module" in capability:
            capability["tags"].append(f"module:{capability['module']}")
        
        return capability

# æ•´åˆåˆ° CapabilityAnalyzer
class CapabilityAnalyzer:
    def __init__(self):
        self.classifier = CapabilityClassifier()
    
    async def analyze_capabilities(self, modules_info: dict) -> list[dict[str, Any]]:
        """åˆ†æä¸¦åˆ†é¡èƒ½åŠ›"""
        capabilities = await self._extract_all_capabilities(modules_info)
        
        # æ·»åŠ åˆ†é¡å’Œæ¨™ç±¤
        classified_capabilities = [
            self.classifier.add_tags(cap)
            for cap in capabilities
        ]
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        self._print_classification_report(classified_capabilities)
        
        return classified_capabilities
    
    def _print_classification_report(self, capabilities: list[dict]):
        """æ‰“å°åˆ†é¡å ±å‘Š"""
        from collections import Counter
        
        categories = [cap["category"] for cap in capabilities]
        category_counts = Counter(categories)
        
        logger.info("ğŸ“Š Capability Classification Report:")
        for category, count in category_counts.most_common():
            percentage = (count / len(capabilities)) * 100
            logger.info(f"  {category}: {count} ({percentage:.1f}%)")
```

#### 4.2 è·¨èªè¨€èª¿ç”¨åœ–ç”Ÿæˆ

**ç›®æ¨™**: å¯è¦–åŒ–èƒ½åŠ›ä¹‹é–“çš„ä¾è³´é—œä¿‚

```python
import networkx as nx
from typing import Dict, List, Set

class CapabilityGraph:
    """èƒ½åŠ›ä¾è³´åœ–"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
    
    def build_from_capabilities(self, capabilities: List[dict]):
        """å¾èƒ½åŠ›åˆ—è¡¨æ§‹å»ºä¾è³´åœ–"""
        # æ·»åŠ ç¯€é»
        for cap in capabilities:
            self.graph.add_node(
                cap["name"],
                language=cap.get("language"),
                category=cap.get("category"),
                module=cap.get("module")
            )
        
        # æ·»åŠ é‚Š (åŸºæ–¼æ–‡ä»¶å…§çš„ import/use èªå¥)
        # ... (éœ€è¦é€²ä¸€æ­¥è§£ææºç¢¼)
    
    def find_critical_capabilities(self, top_n: int = 10) -> List[str]:
        """æŸ¥æ‰¾é—œéµèƒ½åŠ› (å…¥åº¦æœ€é«˜)"""
        degrees = dict(self.graph.in_degree())
        sorted_caps = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        return [cap for cap, degree in sorted_caps[:top_n]]
    
    def export_to_mermaid(self) -> str:
        """å°å‡ºç‚º Mermaid åœ–è¡¨"""
        lines = ["graph TD"]
        
        for node, data in self.graph.nodes(data=True):
            lang = data.get("language", "unknown")
            category = data.get("category", "unknown")
            lines.append(f'    {node}["{node}<br/>{lang}"]:::category_{category}')
        
        for src, dst in self.graph.edges():
            lines.append(f'    {src} --> {dst}')
        
        # æ¨£å¼å®šç¾©
        lines.extend([
            "",
            "classDef category_scanning fill:#e1f5ff",
            "classDef category_analysis fill:#fff3e0",
            "classDef category_attack fill:#ffebee",
        ])
        
        return "\n".join(lines)
```

#### 4.3 AI è¼”åŠ©èƒ½åŠ›æè¿°ç”Ÿæˆ

**ç›®æ¨™**: ä½¿ç”¨ LLM è‡ªå‹•ç”Ÿæˆæ›´è©³ç´°çš„èƒ½åŠ›æè¿°

```python
from openai import AsyncOpenAI

class CapabilityEnhancer:
    """èƒ½åŠ›å¢å¼·å™¨ (ä½¿ç”¨ AI)"""
    
    def __init__(self, openai_client: AsyncOpenAI):
        self.client = openai_client
    
    async def enhance_description(self, capability: dict) -> dict:
        """ä½¿ç”¨ AI å¢å¼·èƒ½åŠ›æè¿°"""
        prompt = f"""
Analyze the following code capability and provide a detailed description:

Name: {capability['name']}
Language: {capability['language']}
Parameters: {capability.get('parameters', [])}
Return Type: {capability.get('return_type', 'N/A')}

Original Description: {capability.get('description', 'None')}

Please provide:
1. A clear, concise description of what this capability does
2. Common use cases
3. Potential security implications (if any)

Format as JSON with keys: description, use_cases, security_notes
"""
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            enhanced = json.loads(response.choices[0].message.content)
            capability.update(enhanced)
            
        except Exception as e:
            logger.warning(f"AI enhancement failed for {capability['name']}: {e}")
        
        return capability
```

---

## ğŸ“‹ å¯¦æ–½è·¯ç·šåœ–

### âœ… Sprint 1 (Week 1-2): åŸºç¤å¼·åŒ– - å·²å®Œæˆ

| ä»»å‹™ | å·¥æ™‚ | è² è²¬æ¨¡çµ„ | å„ªå…ˆç´š | ç‹€æ…‹ |
|------|------|---------|--------|------|
| å¢å¼· Rust æå–å™¨ (impl æ–¹æ³•) | 2 å¤© | `internal_exploration` | P0 | âœ… å®Œæˆ |
| é©—è­‰ JavaScript æ–‡ä»¶æƒ…æ³ | 0.5 å¤© | `internal_exploration` | P0 | âœ… å®Œæˆ |
| å‰µå»ºæ¸¬è©¦å¥—ä»¶ (Phase 2.1) | 3 å¤© | `tests/` | P1 | â³ éƒ¨åˆ†å®Œæˆ |
| å¢å¼·éŒ¯èª¤è™•ç† (Phase 2.2) | 2 å¤© | `internal_exploration` | P1 | âœ… å®Œæˆ |
| æ–‡æª”æ›´æ–° | 0.5 å¤© | - | P1 | âœ… å®Œæˆ |

**äº¤ä»˜ç‰©**:
- âœ… Rust èƒ½åŠ›æ•¸: 0 â†’ 115 (ç›®æ¨™ 40+, å¯¦éš›é”æˆ 287.5%)
- âœ… ç¸½èƒ½åŠ›æ•¸: 576 â†’ 692 (+20.1%)
- âœ… éŒ¯èª¤å ±å‘Šæ©Ÿåˆ¶å®Œæ•´ (ExtractionError + çµ±è¨ˆè¿½è¹¤)
- âœ… æ¸¬è©¦è…³æœ¬: test_enhanced_extraction.py
- âœ… æˆåŠŸç‡: 100.0%
- âš ï¸  æ¸¬è©¦è¦†è“‹ç‡: å¾…å¯¦æ–½ pytest æ¡†æ¶

**å¯¦éš›æˆæœ**:
```
èªè¨€åˆ†å¸ƒ (æ”¹é€²å¾Œ):
  Python:      411 capabilities (59.4%)
  Rust:        115 capabilities (16.6%)  â† å¾ 0 æå‡
  Go:           88 capabilities (12.7%)
  TypeScript:   78 capabilities (11.3%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç¸½è¨ˆ:         692 capabilities (100%)
```

**è©³ç´°å ±å‘Š**: è«‹åƒé–± `P0_IMPLEMENTATION_COMPLETION_REPORT.md`

### Sprint 2 (Week 3-4): æ€§èƒ½å„ªåŒ–

| ä»»å‹™ | å·¥æ™‚ | è² è²¬æ¨¡çµ„ | å„ªå…ˆç´š |
|------|------|---------|--------|
| å¯¦ç¾ä¸¦è¡Œè™•ç† (Phase 3.1) | 2 å¤© | `internal_exploration` | P2 |
| å¯¦ç¾æ™ºèƒ½å¿«å– (Phase 3.2) | 1 å¤© | `internal_exploration` | P2 |
| æ€§èƒ½åŸºæº–æ¸¬è©¦ | 1 å¤© | `tests/` | P2 |
| å„ªåŒ–æ­£å‰‡è¡¨é”å¼ | 1 å¤© | `language_extractors` | P2 |

**äº¤ä»˜ç‰©**:
- âœ… è™•ç†æ™‚é–“æ¸›å°‘ 60%+
- âœ… å¿«å–å‘½ä¸­ç‡ > 70%

### Sprint 3 (Month 2): æ¶æ§‹å¢å¼·

| ä»»å‹™ | å·¥æ™‚ | è² è²¬æ¨¡çµ„ | å„ªå…ˆç´š |
|------|------|---------|--------|
| å¯¦ç¾èƒ½åŠ›åˆ†é¡å™¨ (Phase 4.1) | 3 å¤© | `internal_exploration` | P3 |
| æ§‹å»ºä¾è³´åœ– (Phase 4.2) | 3 å¤© | `internal_exploration` | P3 |
| AI è¼”åŠ©æè¿° (Phase 4.3) | 2 å¤© | `cognitive_core` æ•´åˆ | P3 |

---

## ğŸ¯ æˆåŠŸæŒ‡æ¨™ (KPI)

### æŠ€è¡“æŒ‡æ¨™

| æŒ‡æ¨™ | ç•¶å‰ | ç›®æ¨™ | è¡¡é‡æ–¹å¼ |
|------|------|------|---------|
| **èƒ½åŠ›è¦†è“‹ç‡** | 576 (152%) | 650+ (170%) | ç¸½èƒ½åŠ›æ•¸ / ç¸½æ–‡ä»¶æ•¸ |
| **Rust æå–** | 0 | 40+ | Rust èƒ½åŠ›æ•¸ |
| **æ¸¬è©¦è¦†è“‹ç‡** | 0% | 85%+ | pytest-cov å ±å‘Š |
| **è™•ç†æ™‚é–“** | 30s | < 10s | å…¨é‡æƒææ™‚é–“ |
| **éŒ¯èª¤ç‡** | æœªè¿½è¹¤ | < 1% | å¤±æ•—æ–‡ä»¶æ•¸ / ç¸½æ–‡ä»¶æ•¸ |

### è³ªé‡æŒ‡æ¨™

- âœ… æ‰€æœ‰æå–å™¨æœ‰å–®å…ƒæ¸¬è©¦
- âœ… éŒ¯èª¤è™•ç†å®Œå–„ (ç„¡éœé»˜å¤±æ•—)
- âœ… æ—¥èªŒç´šåˆ¥æ­£ç¢º (INFO/WARNING/ERROR)
- âœ… æ–‡æª”èˆ‡ä»£ç¢¼åŒæ­¥æ›´æ–°

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç«‹å³åŸ·è¡Œ (P0 ä»»å‹™)

```powershell
# 1. æ›´æ–° Rust æå–å™¨
code C:\D\fold7\AIVA-git\services\core\aiva_core\internal_exploration\language_extractors.py

# 2. åŸ·è¡Œé©—è­‰æ¸¬è©¦
cd C:\D\fold7\AIVA-git
python -c "
from services.core.aiva_core.internal_exploration import CapabilityAnalyzer, ModuleExplorer
import asyncio

async def test():
    explorer = ModuleExplorer()
    analyzer = CapabilityAnalyzer()
    
    modules = await explorer.explore_all_modules()
    capabilities = await analyzer.analyze_capabilities(modules)
    
    # çµ±è¨ˆ
    from collections import Counter
    lang_counts = Counter(cap['language'] for cap in capabilities)
    
    print('ğŸ“Š èªè¨€åˆ†å¸ƒ:')
    for lang, count in lang_counts.most_common():
        print(f'  {lang}: {count}')
    
    print(f'\nâœ… ç¸½è¨ˆ: {len(capabilities)} å€‹èƒ½åŠ›')

asyncio.run(test())
"

# 3. æŸ¥çœ‹ Rust æ–‡ä»¶
Get-ChildItem -Path "C:\D\fold7\AIVA-git\services" -Recurse -Filter "*.rs" | 
    Select-String -Pattern "impl\s+\w+\s*\{" | 
    Select-Object Path, LineNumber | 
    Format-Table -AutoSize
```

---

## ğŸ“š ç›¸é—œè³‡æº

### å…§éƒ¨æ–‡æª”
- [MULTI_LANGUAGE_ANALYSIS_INTEGRATION_REPORT.md](./MULTI_LANGUAGE_ANALYSIS_INTEGRATION_REPORT.md)
- [ARCHITECTURE_GAPS_ANALYSIS.md](./services/core/aiva_core/ARCHITECTURE_GAPS_ANALYSIS.md)
- [AI_SELF_OPTIMIZATION_DUAL_LOOP_DESIGN.md](./AI_SELF_OPTIMIZATION_DUAL_LOOP_DESIGN.md)

### å¤–éƒ¨åƒè€ƒ
- **å¾®æœå‹™æœ€ä½³å¯¦è¸**: 
  - æ¯å€‹æœå‹™ç¨ç«‹èªè¨€é¸æ“‡æ¬Š
  - é€šéæ¨™æº–åŒ– API (REST/gRPC) é€šä¿¡
  - Schema-first è¨­è¨ˆ (Protocol Buffers)

- **å¤šèªè¨€å°ˆæ¡ˆç®¡ç†**:
  - Bazel/Buck çµ±ä¸€æ§‹å»ºç³»çµ±
  - Docker å®¹å™¨åŒ–éƒ¨ç½²
  - å…±äº« Schema ç¢ºä¿ä¸€è‡´æ€§

- **ä»£ç¢¼åˆ†æå·¥å…·åƒè€ƒ**:
  - [tree-sitter](https://tree-sitter.github.io/tree-sitter/) - å¤šèªè¨€ AST è§£æ
  - [sourcegraph](https://about.sourcegraph.com/) - ä»£ç¢¼æœç´¢å’Œåˆ†æ
  - [kythe](https://kythe.io/) - ä»£ç¢¼ç´¢å¼•å’Œäº¤å‰å¼•ç”¨

---

## ğŸ“ é—œéµæ±ºç­–è¨˜éŒ„

### ADR-001: ä¿æŒäº”å¤§æ¨¡çµ„æ¶æ§‹ä¸è®Š

**æ±ºç­–**: ç¶­æŒç•¶å‰ `core/`, `scan/`, `features/`, `integration/`, `aiva_common/` æ¶æ§‹

**ç†ç”±**:
1. æ¶æ§‹æ¸…æ™°,è·è²¬åˆ†æ˜
2. å¤šèªè¨€åˆ†ä½ˆåˆç† (scan å’Œ features å¤šèªè¨€,core ç‚º Python)
3. å·²æœ‰å®Œæ•´çš„ aiva_common Schema å®šç¾©
4. è®Šæ›´æˆæœ¬éé«˜,æ”¶ç›Šä¸æ˜é¡¯

### ADR-002: ä½¿ç”¨æ­£å‰‡è€Œéå®Œæ•´ AST è§£æé Python èªè¨€

**æ±ºç­–**: Go/Rust/TypeScript ä½¿ç”¨æ­£å‰‡æå–,ä¸å¼•å…¥å®Œæ•´è§£æå™¨

**ç†ç”±**:
1. é™ä½ä¾è³´è¤‡é›œåº¦ (é¿å…å¼•å…¥ tree-sitter ç­‰é‡å‹åº«)
2. 90% æ¡ˆä¾‹æ­£å‰‡è¶³å¤  (åªéœ€æå–å…¬é–‹å‡½æ•¸/æ–¹æ³•)
3. æ€§èƒ½æ›´å„ª (æ­£å‰‡æ¯”å®Œæ•´ AST å¿« 10x+)
4. ç¶­è­·æˆæœ¬æ›´ä½

**æ¬Šè¡¡**: ç„¡æ³•è™•ç†è¤‡é›œèªæ³•çµæ§‹ (å¯æ¥å—)

### ADR-003: èƒ½åŠ›åˆ†é¡æ¡ç”¨å•Ÿç™¼å¼è¦å‰‡è€Œéæ©Ÿå™¨å­¸ç¿’

**æ±ºç­–**: ä½¿ç”¨é—œéµå­—åŒ¹é…é€²è¡Œèƒ½åŠ›åˆ†é¡

**ç†ç”±**:
1. å¯è§£é‡‹æ€§å¼·
2. ç„¡éœ€è¨“ç·´æ•¸æ“š
3. æº–ç¢ºåº¦è¶³å¤  (ç›®æ¨™ 85%+)
4. å¯éš¨æ™‚èª¿æ•´è¦å‰‡

**æœªä¾†**: å¦‚æœåˆ†é¡éœ€æ±‚è¤‡é›œåŒ–,å¯è€ƒæ…® ML æ¨¡å‹

---

## ğŸ¤ è²¢ç»æŒ‡å—

### æ–°å¢èªè¨€æ”¯æ´

1. åœ¨ `language_extractors.py` æ·»åŠ æå–å™¨é¡
2. å¯¦ç¾ `extract_capabilities()` æ–¹æ³•
3. åœ¨ `get_extractor()` è¨»å†Šèªè¨€æ˜ å°„
4. æ·»åŠ æ¸¬è©¦ç”¨ä¾‹å’Œ fixture
5. æ›´æ–°æ–‡æª”

### æäº¤ Pull Request

```bash
# 1. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/enhance-rust-extractor

# 2. å¯¦ç¾åŠŸèƒ½ä¸¦æ¸¬è©¦
pytest tests/test_multi_language_extraction.py -v

# 3. æäº¤è®Šæ›´
git add .
git commit -m "feat(internal_exploration): enhance Rust impl method extraction

- Add IMPL_METHOD_PATTERN to extract methods inside impl blocks
- Improve capability coverage from 0 to 40+
- Add test cases for Rust extraction
"

# 4. æ¨é€ä¸¦å‰µå»º PR
git push origin feature/enhance-rust-extractor
```

---

**å ±å‘Šç‰ˆæœ¬**: v2.0  
**æœ€å¾Œæ›´æ–°**: 2025-11-16  
**ç¶­è­·è€…**: AIVA æ¶æ§‹åœ˜éšŠ  
**ä¸‹æ¬¡å¯©æŸ¥**: 2025-12-01
