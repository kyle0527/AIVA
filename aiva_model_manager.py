#!/usr/bin/env python3
"""
AIVA æ¨¡å‹ç®¡ç†å™¨åŸå‹ - ç«‹å³å¯ç”¨çš„æ¬Šé‡æ•´åˆæ–¹æ¡ˆ
åŸºæ–¼åˆ†æå ±å‘Šçš„å¯¦éš›å¯¦ç¾ï¼Œå¯ç›´æ¥æ•´åˆåˆ°ç¾æœ‰ç³»çµ±

ä½œè€…: GitHub Copilot
ç›®æ¨™: æä¾›ç”Ÿç”¢ç´šçš„ AI æ¨¡å‹è¼‰å…¥å’Œç®¡ç†åŠŸèƒ½
"""

import os
import torch
import logging
import asyncio
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import json

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """æ¨¡å‹é¡å‹æšèˆ‰"""
    STRUCTURED = "structured_model"  # åŒ…å«å…ƒæ•¸æ“šçš„å®Œæ•´æ¨¡å‹
    RAW_WEIGHTS = "raw_weights"     # ç´”æ¬Šé‡æ–‡ä»¶
    UNKNOWN = "unknown"

class LoadStatus(Enum):
    """è¼‰å…¥ç‹€æ…‹"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    file_path: str
    model_type: ModelType
    total_params: int
    layers: int
    file_size_mb: float
    architecture: Dict[str, Any]
    timestamp: Optional[str] = None
    load_time_ms: Optional[float] = None
    status: LoadStatus = LoadStatus.NOT_LOADED
    error_message: Optional[str] = None

class AIVAModelManager:
    """AIVA AI æ¨¡å‹ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    - è‡ªå‹•ç™¼ç¾å’Œè¼‰å…¥ PyTorch æ¬Šé‡æ–‡ä»¶
    - æ”¯æ´å¤šæ¨¡å‹ä¸¦å­˜å’Œåˆ‡æ›
    - æä¾›å¥åº·ç›£æ§å’ŒéŒ¯èª¤è™•ç†
    - èˆ‡ç¾æœ‰ real_ai_core.py æ•´åˆ
    """
    
    def __init__(self, models_dir: str = "."):
        self.models_dir = Path(models_dir)
        self.models: Dict[str, ModelInfo] = {}
        self.active_model: Optional[str] = None
        self.model_data: Dict[str, Any] = {}
        
        # é…ç½®æ–‡ä»¶
        self.config = {
            "primary_model": "aiva_real_ai_core",
            "backup_models": ["aiva_real_weights", "aiva_5M_weights"],
            "auto_fallback": True,
            "health_check_interval": 30
        }
        
        # å·²çŸ¥çš„æ¨¡å‹æ–‡ä»¶
        self.known_models = {
            "aiva_real_ai_core": "aiva_real_ai_core.pth",
            "aiva_real_weights": "aiva_real_weights.pth",
            "aiva_5M_weights": "aiva_5M_weights.pth"
        }
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ– AIVA AI æ¨¡å‹ç®¡ç†å™¨...")
        
        try:
            # ç™¼ç¾å¯ç”¨æ¨¡å‹
            await self.discover_models()
            
            # è¼‰å…¥ä¸»è¦æ¨¡å‹
            success = await self.load_primary_model()
            
            if success:
                logger.info("âœ… AIVA AI æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ!")
                return True
            else:
                logger.error("âŒ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–ç•°å¸¸: {e}")
            return False
    
    async def discover_models(self) -> List[ModelInfo]:
        """ç™¼ç¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
        logger.info("ğŸ” æ­£åœ¨ç™¼ç¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶...")
        
        discovered_models = []
        
        for model_name, filename in self.known_models.items():
            file_path = self.models_dir / filename
            
            if file_path.exists():
                try:
                    model_info = await self._analyze_model_file(model_name, str(file_path))
                    self.models[model_name] = model_info
                    discovered_models.append(model_info)
                    logger.info(f"âœ… ç™¼ç¾æ¨¡å‹: {model_name} ({model_info.file_size_mb:.1f}MB)")
                    
                except Exception as e:
                    logger.error(f"âŒ åˆ†ææ¨¡å‹ {model_name} å¤±æ•—: {e}")
            else:
                logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        logger.info(f"ğŸ¯ ç¸½è¨ˆç™¼ç¾ {len(discovered_models)} å€‹å¯ç”¨æ¨¡å‹")
        return discovered_models
    
    async def _analyze_model_file(self, model_name: str, file_path: str) -> ModelInfo:
        """åˆ†ææ¨¡å‹æ–‡ä»¶"""
        import time
        
        start_time = time.time()
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        
        # è¼‰å…¥ PyTorch æ¨¡å‹
        data = torch.load(file_path, map_location='cpu')
        load_time_ms = (time.time() - start_time) * 1000
        
        # åˆ†ææ¨¡å‹çµæ§‹
        if isinstance(data, dict) and 'model_state_dict' in data:
            # çµæ§‹åŒ–æ¨¡å‹ (æ¨è–¦æ ¼å¼)
            state_dict = data['model_state_dict']
            architecture = data.get('architecture', {})
            total_params = data.get('total_params', 0)
            timestamp = data.get('timestamp')
            model_type = ModelType.STRUCTURED
            
        elif isinstance(data, dict):
            # ç´”æ¬Šé‡å­—å…¸
            state_dict = data
            total_params = sum(tensor.numel() for tensor in state_dict.values() 
                             if isinstance(tensor, torch.Tensor))
            architecture = self._infer_architecture_from_weights(state_dict)
            timestamp = None
            model_type = ModelType.RAW_WEIGHTS
            
        else:
            # æœªçŸ¥æ ¼å¼
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹æ ¼å¼: {type(data)}")
        
        layers = len(state_dict)
        
        return ModelInfo(
            name=model_name,
            file_path=file_path,
            model_type=model_type,
            total_params=total_params,
            layers=layers,
            file_size_mb=file_size_mb,
            architecture=architecture,
            timestamp=timestamp,
            load_time_ms=load_time_ms
        )
    
    def _infer_architecture_from_weights(self, state_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """å¾æ¬Šé‡æ¨æ–·æ¶æ§‹ä¿¡æ¯"""
        layer_shapes = []
        total_params = 0
        
        for name, tensor in state_dict.items():
            layer_shapes.append({
                "name": name,
                "shape": list(tensor.shape),
                "params": tensor.numel()
            })
            total_params += tensor.numel()
        
        # å˜—è©¦æ¨æ–·ç¶²è·¯çµæ§‹
        weight_layers = [info for info in layer_shapes if 'weight' in info['name']]
        
        if len(weight_layers) >= 2:
            input_size = weight_layers[0]['shape'][0] if len(weight_layers[0]['shape']) >= 2 else None
            output_size = weight_layers[-1]['shape'][1] if len(weight_layers[-1]['shape']) >= 2 else None
            hidden_sizes = [layer['shape'][1] for layer in weight_layers[:-1] 
                          if len(layer['shape']) >= 2]
        else:
            input_size = output_size = None
            hidden_sizes = []
        
        return {
            "type": "inferred",
            "input_size": input_size,
            "output_size": output_size,
            "hidden_sizes": hidden_sizes,
            "total_params": total_params,
            "layer_details": layer_shapes[:5]  # åªä¿ç•™å‰5å±¤çš„è©³ç´°ä¿¡æ¯
        }
    
    async def load_primary_model(self) -> bool:
        """è¼‰å…¥ä¸»è¦æ¨¡å‹"""
        primary_name = self.config["primary_model"]
        
        if primary_name in self.models:
            return await self.load_model(primary_name)
        else:
            logger.warning(f"âš ï¸ ä¸»è¦æ¨¡å‹ {primary_name} ä¸å¯ç”¨ï¼Œå˜—è©¦è¼‰å…¥å‚™ç”¨æ¨¡å‹...")
            return await self.load_fallback_model()
    
    async def load_fallback_model(self) -> bool:
        """è¼‰å…¥å‚™ç”¨æ¨¡å‹"""
        for backup_name in self.config["backup_models"]:
            if backup_name in self.models:
                logger.info(f"ğŸ”„ å˜—è©¦è¼‰å…¥å‚™ç”¨æ¨¡å‹: {backup_name}")
                if await self.load_model(backup_name):
                    return True
        
        logger.error("âŒ æ‰€æœ‰å‚™ç”¨æ¨¡å‹éƒ½ç„¡æ³•è¼‰å…¥")
        return False
    
    async def load_model(self, model_name: str) -> bool:
        """è¼‰å…¥æŒ‡å®šæ¨¡å‹"""
        if model_name not in self.models:
            logger.error(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return False
        
        model_info = self.models[model_name]
        model_info.status = LoadStatus.LOADING
        
        try:
            logger.info(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
            
            # è¼‰å…¥æ¨¡å‹æ•¸æ“š
            data = torch.load(model_info.file_path, map_location='cpu')
            
            # æå–æ¬Šé‡
            if model_info.model_type == ModelType.STRUCTURED:
                weights = data['model_state_dict']
            else:
                weights = data
            
            # å­˜å„²æ¨¡å‹æ•¸æ“š
            self.model_data[model_name] = {
                "weights": weights,
                "raw_data": data,
                "info": model_info
            }
            
            # æ›´æ–°ç‹€æ…‹
            model_info.status = LoadStatus.LOADED
            self.active_model = model_name
            
            logger.info(f"âœ… æ¨¡å‹ {model_name} è¼‰å…¥æˆåŠŸ!")
            logger.info(f"   åƒæ•¸é‡: {model_info.total_params:,}")
            logger.info(f"   æ¨¡å‹é¡å‹: {model_info.model_type.value}")
            
            return True
            
        except Exception as e:
            model_info.status = LoadStatus.ERROR
            model_info.error_message = str(e)
            logger.error(f"âŒ æ¨¡å‹ {model_name} è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def get_active_model_weights(self) -> Optional[Dict[str, torch.Tensor]]:
        """ç²å–ç•¶å‰æ´»èºæ¨¡å‹çš„æ¬Šé‡"""
        if not self.active_model or self.active_model not in self.model_data:
            return None
        
        return self.model_data[self.active_model]["weights"]
    
    def get_active_model_info(self) -> Optional[ModelInfo]:
        """ç²å–ç•¶å‰æ´»èºæ¨¡å‹çš„ä¿¡æ¯"""
        if not self.active_model or self.active_model not in self.models:
            return None
        
        return self.models[self.active_model]
    
    def get_model_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ç®¡ç†å™¨ç‹€æ…‹"""
        return {
            "active_model": self.active_model,
            "total_models": len(self.models),
            "loaded_models": len(self.model_data),
            "model_list": {
                name: {
                    "status": info.status.value,
                    "params": info.total_params,
                    "size_mb": info.file_size_mb,
                    "type": info.model_type.value
                }
                for name, info in self.models.items()
            },
            "timestamp": datetime.now().isoformat()
        }
    
    async def switch_model(self, model_name: str) -> bool:
        """åˆ‡æ›åˆ°æŒ‡å®šæ¨¡å‹"""
        if model_name == self.active_model:
            logger.info(f"â„¹ï¸ æ¨¡å‹ {model_name} å·²ç¶“æ˜¯æ´»èºæ¨¡å‹")
            return True
        
        if model_name not in self.models:
            logger.error(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return False
        
        # å¦‚æœæ¨¡å‹æœªè¼‰å…¥ï¼Œå…ˆè¼‰å…¥
        if model_name not in self.model_data:
            if not await self.load_model(model_name):
                return False
        
        # åˆ‡æ›æ´»èºæ¨¡å‹
        old_model = self.active_model
        self.active_model = model_name
        
        logger.info(f"ğŸ”„ æ¨¡å‹åˆ‡æ›: {old_model} â†’ {model_name}")
        return True
    
    def export_model_info(self, output_file: str = "model_status.json") -> bool:
        """å°å‡ºæ¨¡å‹ä¿¡æ¯åˆ° JSON æ–‡ä»¶"""
        try:
            status = self.get_model_status()
            
            # æ·»åŠ è©³ç´°çš„æ¨¡å‹ä¿¡æ¯
            detailed_info = {
                "manager_status": status,
                "detailed_models": {}
            }
            
            for name, info in self.models.items():
                detailed_info["detailed_models"][name] = asdict(info)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_info, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ“„ æ¨¡å‹ä¿¡æ¯å·²å°å‡ºåˆ°: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å°å‡ºæ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return False

async def demo_model_manager():
    """ç¤ºç¯„æ¨¡å‹ç®¡ç†å™¨çš„ä½¿ç”¨"""
    print("=" * 70)
    print("ğŸ§  AIVA æ¨¡å‹ç®¡ç†å™¨åŸå‹ç¤ºç¯„ ğŸ§ ")
    print("=" * 70)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    manager = AIVAModelManager()
    
    print("\nğŸ“¦ ç¬¬ä¸€æ­¥: åˆå§‹åŒ–ç®¡ç†å™¨")
    print("-" * 40)
    success = await manager.initialize()
    
    if not success:
        print("âŒ åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¬Šé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        return
    
    # é¡¯ç¤ºæ¨¡å‹ç‹€æ…‹
    print("\nğŸ“Š ç¬¬äºŒæ­¥: æ¨¡å‹ç‹€æ…‹æ¦‚è¦½")
    print("-" * 40)
    status = manager.get_model_status()
    
    print(f"ğŸ¯ æ´»èºæ¨¡å‹: {status['active_model']}")
    print(f"ğŸ“ˆ ç¸½è¨ˆæ¨¡å‹: {status['total_models']}")
    print(f"ğŸ’¾ å·²è¼‰å…¥æ¨¡å‹: {status['loaded_models']}")
    print()
    
    for name, info in status['model_list'].items():
        status_emoji = "âœ…" if info['status'] == 'loaded' else "â³"
        print(f"{status_emoji} {name}:")
        print(f"   ç‹€æ…‹: {info['status']}")
        print(f"   åƒæ•¸: {info['params']:,}")
        print(f"   å¤§å°: {info['size_mb']:.1f} MB")
        print(f"   é¡å‹: {info['type']}")
        print()
    
    # æ¸¬è©¦æ¨¡å‹åˆ‡æ›
    print("\nğŸ”„ ç¬¬ä¸‰æ­¥: æ¸¬è©¦æ¨¡å‹åˆ‡æ›")
    print("-" * 40)
    
    available_models = [name for name in manager.models.keys() if name != manager.active_model]
    
    if available_models:
        test_model = available_models[0]
        print(f"ğŸ¯ åˆ‡æ›åˆ°æ¨¡å‹: {test_model}")
        
        switch_success = await manager.switch_model(test_model)
        if switch_success:
            print("âœ… æ¨¡å‹åˆ‡æ›æˆåŠŸ!")
            
            # ç²å–æ–°æ¨¡å‹çš„æ¬Šé‡
            weights = manager.get_active_model_weights()
            if weights:
                print(f"ğŸ“Š æ–°æ¨¡å‹æ¬Šé‡å±¤æ•¸: {len(weights)}")
                print(f"ğŸ“Š å‰3å±¤: {list(weights.keys())[:3]}")
        else:
            print("âŒ æ¨¡å‹åˆ‡æ›å¤±æ•—")
    
    # æ¸¬è©¦æ¬Šé‡è¨ªå•
    print("\nğŸ” ç¬¬å››æ­¥: æ¸¬è©¦æ¬Šé‡è¨ªå•")
    print("-" * 40)
    
    active_info = manager.get_active_model_info()
    if active_info:
        print(f"ğŸ“ ç•¶å‰æ¨¡å‹: {active_info.name}")
        print(f"âš™ï¸ æ¶æ§‹: {active_info.architecture.get('type', 'unknown')}")
        print(f"ğŸ“Š åƒæ•¸: {active_info.total_params:,}")
        
        weights = manager.get_active_model_weights()
        if weights:
            print(f"ğŸ”¢ æ¬Šé‡éµ: {len(weights)} å€‹")
            print(f"ğŸ¯ ç¤ºä¾‹å±¤: {list(weights.keys())[:2]}")
    
    # å°å‡ºç‹€æ…‹
    print("\nğŸ“„ ç¬¬äº”æ­¥: å°å‡ºæ¨¡å‹ä¿¡æ¯")
    print("-" * 40)
    
    export_success = manager.export_model_info("aiva_model_status.json")
    if export_success:
        print("âœ… æ¨¡å‹ä¿¡æ¯å·²å°å‡ºåˆ° aiva_model_status.json")
    
    print("\nğŸ‰ æ¨¡å‹ç®¡ç†å™¨ç¤ºç¯„å®Œæˆ!")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ•´åˆå»ºè­°:")
    print("  1. å°‡æ­¤ç®¡ç†å™¨æ•´åˆåˆ° real_ai_core.py")
    print("  2. ä¿®æ”¹ RealAIDecisionEngine ä½¿ç”¨çœŸå¯¦æ¬Šé‡")
    print("  3. æ·»åŠ åˆ° services/core/ai/ æ¨¡çµ„ç³»çµ±")
    print("  4. å»ºç«‹ A/B æ¸¬è©¦æ¡†æ¶")
    print("=" * 70)

if __name__ == "__main__":
    # è¨­å®šæ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # é‹è¡Œç¤ºç¯„
    asyncio.run(demo_model_manager())