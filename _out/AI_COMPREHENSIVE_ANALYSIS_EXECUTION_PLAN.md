# ğŸ¤– AIVA AI å…¨é¢åˆ†æåŸ·è¡Œè¨ˆåŠƒ

**å»ºç«‹æ—¥æœŸ**: 2025-11-16  
**ç›®æ¨™**: AI èƒ½å®Œæ•´äº†è§£æ•´å€‹ç¨‹å¼çš„åŠŸèƒ½(åŒ…å«ä¸åŒèªè¨€)åŠè­˜åˆ¥æ½›åœ¨å•é¡Œ  
**ç¯„åœ**: æ•´åˆç¾æœ‰è…³æœ¬ + çµ±ä¸€æ•¸æ“šåˆç´„ + å¤šèªè¨€èƒ½åŠ›åˆ†æ  

---

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### ğŸ¯ æ ¸å¿ƒç›®æ¨™

1. **å®Œæ•´åŠŸèƒ½ç†è§£**: AI èƒ½ç†è§£æ‰€æœ‰æ¨¡çµ„åŠŸèƒ½åŠä½¿ç”¨æ–¹å¼
2. **å¤šèªè¨€æ”¯æ´**: æ¶µè“‹ Python, Go, Rust, TypeScript, JavaScript
3. **å•é¡Œè­˜åˆ¥**: è‡ªå‹•ç™¼ç¾æ¶æ§‹ã€ä»£ç¢¼ã€æ•¸æ“šæµå•é¡Œ
4. **çµ±ä¸€è¦–åœ–**: åŸºæ–¼çµ±ä¸€æ•¸æ“šåˆç´„å»ºç«‹å…¨å±€çŸ¥è­˜åœ–è­œ

### âœ… ç¾æœ‰è³‡ç”¢ç›¤é»

| é¡åˆ¥ | è…³æœ¬/å·¥å…· | åŠŸèƒ½ | ç‹€æ…‹ |
|------|----------|------|------|
| **AI æ¢ç´¢å™¨** | `ai_system_explorer_v2.py` | å¤šèªè¨€å¢é‡åˆ†æã€é€²åº¦æŒä¹…åŒ– | âœ… å®Œæ•´ |
| | `ai_component_explorer.py` | AI çµ„ä»¶è­˜åˆ¥èˆ‡åˆ†æ | âœ… å®Œæ•´ |
| | `module_explorer.py` | Python æ¨¡çµ„æ¢ç´¢ | âš ï¸ åƒ… Python |
| | `capability_analyzer.py` | Python èƒ½åŠ›åˆ†æ | âš ï¸ åƒ… Python |
| **æ•¸æ“šåˆç´„** | `unified_schema_manager.py` | çµ±ä¸€ Schema ç®¡ç† | âœ… å®Œæ•´ |
| | `schema_codegen_tool.py` | è·¨èªè¨€ä»£ç¢¼ç”Ÿæˆ | âœ… å®Œæ•´ |
| | `core_schema_sot.yaml` | å–®ä¸€äº‹å¯¦ä¾†æº | âœ… å­˜åœ¨ |
| | `cross_language_validator.py` | è·¨èªè¨€é©—è­‰ | âœ… å®Œæ•´ |
| **æµç¨‹åœ–** | `py2mermaid.py` | Python AST è½‰ Mermaid | âœ… å®Œæ•´ |
| **å…§é–‰ç’°** | `update_self_awareness.py` | RAG çŸ¥è­˜æ³¨å…¥ | âœ… å®Œæ•´ |
| **è¨ºæ–·** | `ai_functionality_validator.py` | AI åŠŸèƒ½é©—è­‰ | âœ… å®Œæ•´ |

---

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹åˆ†æ

### 1. ç¾æœ‰ AI åˆ†æèƒ½åŠ›çŸ©é™£

```mermaid
graph TB
    subgraph "ğŸ” æ¢ç´¢å±¤"
        A1[ai_system_explorer_v2.py<br/>å¤šèªè¨€æ¢ç´¢]
        A2[ai_component_explorer.py<br/>AIçµ„ä»¶è­˜åˆ¥]
        A3[module_explorer.py<br/>Pythonæ¨¡çµ„æƒæ]
    end
    
    subgraph "ğŸ“Š åˆ†æå±¤"
        B1[capability_analyzer.py<br/>èƒ½åŠ›åˆ†æ]
        B2[py2mermaid.py<br/>æµç¨‹åœ–ç”Ÿæˆ]
        B3[ASTè§£æå™¨<br/>ä»£ç¢¼çµæ§‹]
    end
    
    subgraph "ğŸ“¦ æ•¸æ“šåˆç´„å±¤"
        C1[unified_schema_manager.py<br/>Schemaç®¡ç†]
        C2[core_schema_sot.yaml<br/>å”¯ä¸€ä¾†æº]
        C3[schema_codegen_tool.py<br/>ä»£ç¢¼ç”Ÿæˆ]
    end
    
    subgraph "ğŸ§  çŸ¥è­˜å±¤"
        D1[RAG Engine<br/>å‘é‡æª¢ç´¢]
        D2[Knowledge Base<br/>çŸ¥è­˜åº«]
        D3[Internal Loop<br/>å°å…§é–‰ç’°]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    
    B1 --> D2
    B2 --> D2
    B3 --> B1
    
    C1 --> C2
    C2 --> C3
    C3 --> D2
    
    D2 --> D1
    D3 --> D1
    
    style A1 fill:#e1f5ff
    style C2 fill:#fff4e1
    style D1 fill:#f0f0ff
```

### 2. æ•¸æ“šæµå‘åˆ†æ

```
éšæ®µ1: ä»£ç¢¼æ¢ç´¢
    â”œâ”€ ai_system_explorer_v2.py 
    â”‚   â”œâ”€ æƒæ Python (.py)         âœ… æ”¯æ´
    â”‚   â”œâ”€ æƒæ Go (.go)             âœ… æ”¯æ´
    â”‚   â”œâ”€ æƒæ Rust (.rs)           âœ… æ”¯æ´
    â”‚   â”œâ”€ æƒæ TypeScript (.ts)     âœ… æ”¯æ´
    â”‚   â””â”€ æƒæ JavaScript (.js)     âœ… æ”¯æ´
    â”‚
    â””â”€ è¼¸å‡º: ExplorationSnapshot (æŒä¹…åŒ–åˆ° SQLite)
        â”œâ”€ æ¨¡çµ„åˆ†æ (ModuleAnalysis)
        â”œâ”€ æª”æ¡ˆåˆ†æ (FileAnalysis)
        â””â”€ å¥åº·åˆ†æ•¸ (health_score)

éšæ®µ2: èƒ½åŠ›æå–
    â”œâ”€ capability_analyzer.py
    â”‚   â”œâ”€ Python AST è§£æ           âœ… æ”¯æ´
    â”‚   â”œâ”€ Go è§£æ                   âŒ æœªæ”¯æ´
    â”‚   â”œâ”€ Rust è§£æ                 âŒ æœªæ”¯æ´
    â”‚   â””â”€ TypeScript è§£æ           âŒ æœªæ”¯æ´
    â”‚
    â””â”€ è¼¸å‡º: èƒ½åŠ›åˆ—è¡¨ (capabilities)
        â”œâ”€ å‡½æ•¸èƒ½åŠ›
        â”œâ”€ é¡åˆ¥èƒ½åŠ›
        â””â”€ è£é£¾å™¨æ¨™è¨˜èƒ½åŠ›

éšæ®µ3: æ•¸æ“šåˆç´„æ˜ å°„
    â”œâ”€ unified_schema_manager.py
    â”‚   â”œâ”€ é©—è­‰ Enums                âœ… æ”¯æ´
    â”‚   â”œâ”€ é©—è­‰ Schemas              âœ… æ”¯æ´
    â”‚   â”œâ”€ ç”Ÿæˆå¤šèªè¨€å®šç¾©            âœ… æ”¯æ´
    â”‚   â””â”€ è·¨èªè¨€ä¸€è‡´æ€§é©—è­‰          âœ… æ”¯æ´
    â”‚
    â””â”€ è¼¸å‡º: çµ±ä¸€æ•¸æ“šæ¨¡å‹
        â”œâ”€ Python (Pydantic)
        â”œâ”€ TypeScript (Interfaces)
        â”œâ”€ Go (Structs)
        â””â”€ Rust (Serde)

éšæ®µ4: çŸ¥è­˜æ³¨å…¥
    â”œâ”€ update_self_awareness.py
    â”‚   â”œâ”€ ModuleExplorer            âœ… é‹è¡Œ
    â”‚   â”œâ”€ CapabilityAnalyzer        âœ… é‹è¡Œ
    â”‚   â”œâ”€ InternalLoopConnector     âœ… é‹è¡Œ
    â”‚   â””â”€ RAG å‘é‡åŒ–               âœ… é‹è¡Œ
    â”‚
    â””â”€ è¼¸å‡º: RAG çŸ¥è­˜åº«
        â””â”€ 405 å€‹ Python èƒ½åŠ› (å·²æ³¨å…¥)
            âŒ ç¼ºå°‘ Go/Rust/TS èƒ½åŠ›
```

---

## ğŸ” å•é¡Œåˆ†æ

### ğŸš¨ ç™¼ç¾çš„é—œéµå•é¡Œ

#### å•é¡Œ 1: èƒ½åŠ›åˆ†æå™¨åƒ…æ”¯æ´ Python (P0)

**ç¾ç‹€**:
```python
# capability_analyzer.py (ç•¶å‰å¯¦ç¾)
async def _extract_capabilities_from_file(self, file_path: Path, module: str):
    tree = ast.parse(content)  # âŒ åªèƒ½è§£æ Python
    
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):  # âŒ Python ç‰¹å®š
            # ...
```

**å½±éŸ¿**:
- âŒ 75+ å€‹ Go/Rust/TS æ–‡ä»¶æœªè¢«åˆ†æ
- âŒ ç³»çµ±èƒ½åŠ›è¦†è“‹ç‡åƒ… 81%
- âŒ AI ç„¡æ³•æ¨è–¦é Python åŠŸèƒ½

**è§£æ±ºæ–¹æ¡ˆ**: è¦‹ä¸‹æ–¹ã€Œå¤šèªè¨€èƒ½åŠ›åˆ†æå™¨è¨­è¨ˆã€

---

#### å•é¡Œ 2: çµ±ä¸€æ•¸æ“šåˆç´„æœªæ•´åˆåˆ°æ¢ç´¢æµç¨‹ (P1)

**ç¾ç‹€**:
- âœ… `core_schema_sot.yaml` å­˜åœ¨
- âœ… `schema_codegen_tool.py` å¯ç”Ÿæˆå¤šèªè¨€ä»£ç¢¼
- âŒ æ¢ç´¢å™¨æœªè­˜åˆ¥æ•¸æ“šåˆç´„å®šç¾©çš„çµæ§‹
- âŒ èƒ½åŠ›åˆ†ææœªæ˜ å°„åˆ°çµ±ä¸€ Schema

**æ‡‰æœ‰æ¶æ§‹**:
```python
# ç†æƒ³æµç¨‹
æ¢ç´¢éšæ®µ â†’ æå–å‡½æ•¸ç°½å
    â†“
æ˜ å°„éšæ®µ â†’ åŒ¹é… core_schema_sot.yaml å®šç¾©
    â†“
é©—è­‰éšæ®µ â†’ æª¢æŸ¥æ˜¯å¦ç¬¦åˆçµ±ä¸€åˆç´„
    â†“
æ³¨å…¥éšæ®µ â†’ æ¨™è¨˜åˆç´„é¡å‹åˆ° RAG
```

**ç•¶å‰ç¼ºå¤±**:
- æ²’æœ‰å°‡å‡½æ•¸åƒæ•¸/è¿”å›å€¼æ˜ å°„åˆ° Schema å®šç¾©
- ç„¡æ³•è­˜åˆ¥ä½¿ç”¨äº†å“ªäº›çµ±ä¸€åˆç´„
- è·¨èªè¨€æ•¸æ“šæµç„¡æ³•è¿½è¹¤

---

#### å•é¡Œ 3: æµç¨‹åœ–ç”Ÿæˆåƒ…æ”¯æ´ Python (P1)

**ç¾ç‹€**:
```python
# py2mermaid.py
def generate_flowchart(py_file):
    tree = ast.parse(source)  # âŒ Python only
    # ...
```

**å½±éŸ¿**:
- Go/Rust/TS çš„é‚è¼¯æµç¨‹ç„¡æ³•å¯è¦–åŒ–
- AI ç„¡æ³•ç†è§£é Python ä»£ç¢¼çš„åŸ·è¡Œæµ

**éœ€æ±‚**:
- Go â†’ Mermaid æµç¨‹åœ–ç”Ÿæˆ
- Rust â†’ Mermaid æµç¨‹åœ–ç”Ÿæˆ  
- TypeScript â†’ Mermaid æµç¨‹åœ–ç”Ÿæˆ

---

#### å•é¡Œ 4: è·¨èªè¨€èª¿ç”¨é—œä¿‚æœªè­˜åˆ¥ (P2)

**ç¤ºä¾‹**:
```python
# Python èª¿ç”¨ Go æœå‹™ (HTTP/gRPC)
async def call_go_scanner(target: str):
    response = await http_client.post(
        "http://go-scanner:8080/scan",  # âŒ æœªè¢«è¿½è¹¤
        json={"target": target}
    )
```

```go
// Go ç™¼é€ MQ æ¶ˆæ¯çµ¦ Python
func PublishScanResult(result ScanResult) error {
    msg := amqp.Publishing{
        Body: json.Marshal(result),  // âŒ æœªè¢«è¿½è¹¤
    }
    ch.Publish("aiva.results", "scan.completed", false, false, msg)
}
```

**éœ€æ±‚**:
- è­˜åˆ¥ HTTP/gRPC èª¿ç”¨
- è¿½è¹¤ MQ è¨Šæ¯æµ
- å»ºç«‹è·¨èªè¨€ä¾è³´åœ–

---

## ğŸ› ï¸ è§£æ±ºæ–¹æ¡ˆè¨­è¨ˆ

### æ–¹æ¡ˆ 1: å¤šèªè¨€èƒ½åŠ›åˆ†æå™¨ (P0)

#### æ¶æ§‹è¨­è¨ˆ

```python
# multi_language_capability_analyzer.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict, Any
import re

@dataclass
class UnifiedCapability:
    """çµ±ä¸€èƒ½åŠ›å…ƒæ•¸æ“š"""
    name: str
    language: str  # python, go, rust, typescript
    file_path: str
    capability_type: str  # function, method, class, service
    
    # å‡½æ•¸ä¿¡æ¯
    parameters: List[Dict[str, str]]  # [{"name": "target", "type": "str"}]
    return_type: str
    
    # èªç¾©ä¿¡æ¯
    description: str
    is_async: bool
    is_exported: bool  # Go/Rust/TS exported
    
    # åˆç´„æ˜ å°„
    uses_contracts: List[str]  # ä½¿ç”¨äº†å“ªäº›çµ±ä¸€ Schema
    input_contract: str | None  # è¼¸å…¥åˆç´„é¡å‹
    output_contract: str | None  # è¼¸å‡ºåˆç´„é¡å‹
    
    # è£é£¾å™¨/å±¬æ€§
    decorators: List[str]  # Python: @capability, Rust: #[capability]
    
    # èªè¨€ç‰¹å®š
    language_specific: Dict[str, Any]


class BaseLanguageAnalyzer(ABC):
    """èªè¨€åˆ†æå™¨åŸºé¡"""
    
    def __init__(self, schema_manager):
        self.schema_manager = schema_manager
    
    @abstractmethod
    async def extract_capabilities(self, file_path: str) -> List[UnifiedCapability]:
        """æå–èƒ½åŠ›"""
        pass
    
    @abstractmethod
    def detect_contract_usage(self, content: str) -> List[str]:
        """æª¢æ¸¬ä½¿ç”¨çš„çµ±ä¸€åˆç´„"""
        pass


class PythonCapabilityAnalyzer(BaseLanguageAnalyzer):
    """Python èƒ½åŠ›åˆ†æå™¨"""
    
    async def extract_capabilities(self, file_path: str) -> List[UnifiedCapability]:
        import ast
        
        with open(file_path, 'r') as f:
            content = f.read()
        
        tree = ast.parse(content)
        capabilities = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                capability = UnifiedCapability(
                    name=node.name,
                    language="python",
                    file_path=file_path,
                    capability_type="function",
                    parameters=self._extract_parameters(node),
                    return_type=self._extract_return_type(node),
                    description=ast.get_docstring(node) or "",
                    is_async=isinstance(node, ast.AsyncFunctionDef),
                    is_exported=not node.name.startswith('_'),
                    decorators=[d.id for d in node.decorator_list if hasattr(d, 'id')],
                    uses_contracts=self.detect_contract_usage(content),
                    input_contract=self._detect_input_contract(node),
                    output_contract=self._detect_output_contract(node),
                    language_specific={}
                )
                capabilities.append(capability)
        
        return capabilities
    
    def detect_contract_usage(self, content: str) -> List[str]:
        """æª¢æ¸¬ä½¿ç”¨çš„çµ±ä¸€åˆç´„"""
        # æª¢æ¸¬ import èªå¥
        pattern = r'from aiva_common\.schemas import ([\w, ]+)'
        matches = re.findall(pattern, content)
        
        contracts = []
        for match in matches:
            contracts.extend([s.strip() for s in match.split(',')])
        
        return contracts
    
    def _detect_input_contract(self, node: ast.FunctionDef) -> str | None:
        """æª¢æ¸¬è¼¸å…¥åƒæ•¸ä½¿ç”¨çš„åˆç´„é¡å‹"""
        for arg in node.args.args:
            if arg.annotation:
                type_name = ast.unparse(arg.annotation)
                if self._is_contract_type(type_name):
                    return type_name
        return None
    
    def _is_contract_type(self, type_name: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºåˆç´„é¡å‹"""
        contract_prefixes = [
            'TaskPayload', 'ScanResult', 'Finding',
            'Vulnerability', 'Asset', 'Authentication'
        ]
        return any(type_name.startswith(prefix) for prefix in contract_prefixes)


class GoCapabilityAnalyzer(BaseLanguageAnalyzer):
    """Go èƒ½åŠ›åˆ†æå™¨"""
    
    async def extract_capabilities(self, file_path: str) -> List[UnifiedCapability]:
        with open(file_path, 'r') as f:
            content = f.read()
        
        capabilities = []
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå– Go å‡½æ•¸
        # func (receiver *Type) FunctionName(params) (returns) { }
        pattern = r'func\s+(?:\((\w+)\s+\*?(\w+)\)\s+)?(\w+)\s*\((.*?)\)\s*(?:\((.*?)\)|(\w+))?\s*\{'
        
        for match in re.finditer(pattern, content, re.MULTILINE):
            receiver_name, receiver_type, func_name, params, returns1, returns2 = match.groups()
            
            # åªæå– exported å‡½æ•¸ (é¦–å­—æ¯å¤§å¯«)
            if not func_name[0].isupper():
                continue
            
            capability = UnifiedCapability(
                name=func_name,
                language="go",
                file_path=file_path,
                capability_type="method" if receiver_type else "function",
                parameters=self._parse_go_params(params),
                return_type=returns1 or returns2 or "void",
                description=self._extract_go_comment(content, match.start()),
                is_async=False,  # Go ç”¨ goroutine,è¦–ç‚º sync
                is_exported=True,
                decorators=[],
                uses_contracts=self.detect_contract_usage(content),
                input_contract=self._detect_go_contract(params),
                output_contract=self._detect_go_contract(returns1 or returns2 or ""),
                language_specific={
                    "receiver": receiver_type,
                    "receiver_name": receiver_name
                }
            )
            capabilities.append(capability)
        
        return capabilities
    
    def detect_contract_usage(self, content: str) -> List[str]:
        """æª¢æ¸¬ Go ä½¿ç”¨çš„åˆç´„é¡å‹"""
        # æª¢æ¸¬ import è·¯å¾‘
        import_pattern = r'import\s+(?:[\w\s]+\s+)?"([^"]+/schemas[^"]*)"'
        matches = re.findall(import_pattern, content)
        
        contracts = []
        for match in matches:
            # æå–æœ€å¾Œä¸€æ®µä½œç‚ºåŒ…å
            package = match.split('/')[-1]
            contracts.append(package)
        
        return contracts
    
    def _parse_go_params(self, params_str: str) -> List[Dict[str, str]]:
        """è§£æ Go å‡½æ•¸åƒæ•¸"""
        if not params_str.strip():
            return []
        
        params = []
        # ç°¡åŒ–ç‰ˆ: name type, name type
        for param in params_str.split(','):
            parts = param.strip().split()
            if len(parts) >= 2:
                params.append({"name": parts[0], "type": ' '.join(parts[1:])})
        
        return params
    
    def _extract_go_comment(self, content: str, func_pos: int) -> str:
        """æå–å‡½æ•¸å‰çš„è¨»è§£"""
        lines = content[:func_pos].split('\n')
        comments = []
        
        for line in reversed(lines[-5:]):  # æœ€å¤šå¾€å‰çœ‹5è¡Œ
            line = line.strip()
            if line.startswith('//'):
                comments.insert(0, line[2:].strip())
            elif line:
                break
        
        return ' '.join(comments)
    
    def _detect_go_contract(self, type_str: str) -> str | None:
        """æª¢æ¸¬ Go é¡å‹æ˜¯å¦ç‚ºåˆç´„é¡å‹"""
        if not type_str:
            return None
        
        contract_types = [
            'TaskPayload', 'ScanResult', 'Finding',
            'Vulnerability', 'Asset', 'schemas.'
        ]
        
        for contract in contract_types:
            if contract in type_str:
                return type_str.strip()
        
        return None


class RustCapabilityAnalyzer(BaseLanguageAnalyzer):
    """Rust èƒ½åŠ›åˆ†æå™¨"""
    
    async def extract_capabilities(self, file_path: str) -> List[UnifiedCapability]:
        with open(file_path, 'r') as f:
            content = f.read()
        
        capabilities = []
        
        # Rust å‡½æ•¸æ¨¡å¼: pub fn function_name(params) -> return_type { }
        pattern = r'((?:#\[[\w\s,=\(\)]+\]\s*)*)(?:pub\s+)?(?:async\s+)?fn\s+(\w+)\s*(?:<[^>]*>)?\s*\((.*?)\)\s*(?:->\s*([^{]+))?\s*\{'
        
        for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL):
            attributes, func_name, params, return_type = match.groups()
            
            # æª¢æŸ¥æ˜¯å¦ pub
            func_start = content[max(0, match.start()-50):match.start()]
            is_pub = 'pub' in func_start
            
            if not is_pub:
                continue
            
            capability = UnifiedCapability(
                name=func_name,
                language="rust",
                file_path=file_path,
                capability_type="function",
                parameters=self._parse_rust_params(params),
                return_type=(return_type or "()").strip(),
                description=self._extract_rust_comment(content, match.start()),
                is_async='async' in func_start,
                is_exported=is_pub,
                decorators=self._parse_rust_attributes(attributes),
                uses_contracts=self.detect_contract_usage(content),
                input_contract=self._detect_rust_contract(params),
                output_contract=self._detect_rust_contract(return_type or ""),
                language_specific={}
            )
            capabilities.append(capability)
        
        return capabilities
    
    def detect_contract_usage(self, content: str) -> List[str]:
        """æª¢æ¸¬ Rust ä½¿ç”¨çš„åˆç´„"""
        # use aiva_common::schemas::{Type1, Type2};
        pattern = r'use\s+aiva_common::schemas::(?:\{([^}]+)\}|(\w+))'
        matches = re.findall(pattern, content)
        
        contracts = []
        for match in matches:
            if match[0]:  # å¤šå€‹å°å…¥
                contracts.extend([s.strip() for s in match[0].split(',')])
            elif match[1]:  # å–®å€‹å°å…¥
                contracts.append(match[1])
        
        return contracts
    
    def _parse_rust_params(self, params_str: str) -> List[Dict[str, str]]:
        """è§£æ Rust åƒæ•¸"""
        if not params_str.strip():
            return []
        
        params = []
        # name: type
        for param in params_str.split(','):
            if ':' in param:
                parts = param.split(':', 1)
                params.append({
                    "name": parts[0].strip(),
                    "type": parts[1].strip()
                })
        
        return params
    
    def _parse_rust_attributes(self, attrs_str: str) -> List[str]:
        """è§£æ Rust å±¬æ€§ #[...]"""
        if not attrs_str:
            return []
        
        attrs = re.findall(r'#\[([\w\s,=\(\)]+)\]', attrs_str)
        return attrs
    
    def _extract_rust_comment(self, content: str, func_pos: int) -> str:
        """æå– Rust æ–‡æª”è¨»è§£"""
        lines = content[:func_pos].split('\n')
        comments = []
        
        for line in reversed(lines[-10:]):
            line = line.strip()
            if line.startswith('///'):
                comments.insert(0, line[3:].strip())
            elif line.startswith('//!'):
                comments.insert(0, line[3:].strip())
            elif line and not line.startswith('//'):
                break
        
        return ' '.join(comments)
    
    def _detect_rust_contract(self, type_str: str) -> str | None:
        """æª¢æ¸¬ Rust åˆç´„é¡å‹"""
        if not type_str:
            return None
        
        if 'schemas::' in type_str or any(
            contract in type_str for contract in [
                'TaskPayload', 'ScanResult', 'Finding'
            ]
        ):
            return type_str.strip()
        
        return None


class TypeScriptCapabilityAnalyzer(BaseLanguageAnalyzer):
    """TypeScript èƒ½åŠ›åˆ†æå™¨"""
    
    async def extract_capabilities(self, file_path: str) -> List[UnifiedCapability]:
        with open(file_path, 'r') as f:
            content = f.read()
        
        capabilities = []
        
        # åŒ¹é… export function/async function
        pattern1 = r'export\s+(?:async\s+)?function\s+(\w+)\s*(<[^>]*>)?\s*\((.*?)\)\s*:\s*([^{]+)\s*\{'
        
        # åŒ¹é… export const arrow function
        pattern2 = r'export\s+const\s+(\w+)\s*=\s*(?:async\s+)?\((.*?)\)\s*:\s*([^=]+)\s*=>'
        
        for pattern in [pattern1, pattern2]:
            for match in re.finditer(pattern, content, re.MULTILINE):
                if pattern == pattern1:
                    func_name, generics, params, return_type = match.groups()
                else:
                    func_name, params, return_type = match.groups()
                
                func_start = content[max(0, match.start()-100):match.start()]
                is_async = 'async' in func_start
                
                capability = UnifiedCapability(
                    name=func_name,
                    language="typescript",
                    file_path=file_path,
                    capability_type="function",
                    parameters=self._parse_ts_params(params),
                    return_type=return_type.strip(),
                    description=self._extract_ts_jsdoc(content, match.start()),
                    is_async=is_async,
                    is_exported=True,
                    decorators=[],
                    uses_contracts=self.detect_contract_usage(content),
                    input_contract=self._detect_ts_contract(params),
                    output_contract=self._detect_ts_contract(return_type),
                    language_specific={}
                )
                capabilities.append(capability)
        
        return capabilities
    
    def detect_contract_usage(self, content: str) -> List[str]:
        """æª¢æ¸¬ TypeScript åˆç´„å°å…¥"""
        # import { Type1, Type2 } from 'aiva-common/schemas'
        pattern = r'import\s+\{([^}]+)\}\s+from\s+[\'"](?:.*?schemas[^\'"]*)[\'"]'
        matches = re.findall(pattern, content)
        
        contracts = []
        for match in matches:
            contracts.extend([s.strip() for s in match.split(',')])
        
        return contracts
    
    def _parse_ts_params(self, params_str: str) -> List[Dict[str, str]]:
        """è§£æ TypeScript åƒæ•¸"""
        if not params_str.strip():
            return []
        
        params = []
        # name: type
        for param in params_str.split(','):
            if ':' in param:
                parts = param.split(':', 1)
                params.append({
                    "name": parts[0].strip().rstrip('?'),
                    "type": parts[1].strip()
                })
        
        return params
    
    def _extract_ts_jsdoc(self, content: str, func_pos: int) -> str:
        """æå– JSDoc è¨»è§£"""
        lines = content[:func_pos].split('\n')
        
        # æ‰¾ /** ... */ è¨»è§£å¡Š
        jsdoc_pattern = r'/\*\*(.*?)\*/'
        jsdoc_match = re.search(jsdoc_pattern, '\n'.join(lines[-20:]), re.DOTALL)
        
        if jsdoc_match:
            doc = jsdoc_match.group(1)
            # ç§»é™¤æ¯è¡Œé–‹é ­çš„ *
            lines = [line.strip().lstrip('*').strip() for line in doc.split('\n')]
            return ' '.join(lines)
        
        return ""
    
    def _detect_ts_contract(self, type_str: str) -> str | None:
        """æª¢æ¸¬ TypeScript åˆç´„é¡å‹"""
        if not type_str:
            return None
        
        contract_keywords = [
            'TaskPayload', 'ScanResult', 'Finding',
            'Vulnerability', 'Asset', 'Promise<'
        ]
        
        for keyword in contract_keywords:
            if keyword in type_str:
                return type_str.strip()
        
        return None


class MultiLanguageCapabilityAnalyzer:
    """å¤šèªè¨€èƒ½åŠ›åˆ†æå™¨çµ±ä¸€æ¥å£"""
    
    def __init__(self, schema_manager):
        self.analyzers = {
            '.py': PythonCapabilityAnalyzer(schema_manager),
            '.go': GoCapabilityAnalyzer(schema_manager),
            '.rs': RustCapabilityAnalyzer(schema_manager),
            '.ts': TypeScriptCapabilityAnalyzer(schema_manager),
            '.js': TypeScriptCapabilityAnalyzer(schema_manager),  # JS ä½¿ç”¨ TS åˆ†æå™¨
        }
    
    async def analyze_file(self, file_path: str) -> List[UnifiedCapability]:
        """åˆ†æå–®å€‹æ–‡ä»¶"""
        from pathlib import Path
        
        ext = Path(file_path).suffix
        analyzer = self.analyzers.get(ext)
        
        if analyzer:
            return await analyzer.extract_capabilities(file_path)
        
        return []
    
    async def analyze_workspace(self, workspace_root: str) -> Dict[str, List[UnifiedCapability]]:
        """åˆ†ææ•´å€‹å·¥ä½œå€"""
        from pathlib import Path
        
        all_capabilities = {}
        
        for ext, analyzer in self.analyzers.items():
            files = Path(workspace_root).rglob(f"*{ext}")
            
            for file_path in files:
                if self._should_skip(file_path):
                    continue
                
                capabilities = await analyzer.extract_capabilities(str(file_path))
                
                if capabilities:
                    all_capabilities[str(file_path)] = capabilities
        
        return all_capabilities
    
    def _should_skip(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦è·³éæ–‡ä»¶"""
        skip_patterns = [
            '__pycache__', 'node_modules', 'target',
            'test_', '_test.', '.test.', 'spec.'
        ]
        
        return any(pattern in str(file_path) for pattern in skip_patterns)
```

#### ä½¿ç”¨ç¯„ä¾‹

```python
from multi_language_capability_analyzer import MultiLanguageCapabilityAnalyzer
from unified_schema_manager import UnifiedSchemaManager

# åˆå§‹åŒ–
schema_manager = UnifiedSchemaManager()
analyzer = MultiLanguageCapabilityAnalyzer(schema_manager)

# åˆ†ææ•´å€‹å·¥ä½œå€
capabilities = await analyzer.analyze_workspace("C:/D/fold7/AIVA-git")

# çµ±è¨ˆ
stats = {
    "python": 0,
    "go": 0,
    "rust": 0,
    "typescript": 0
}

for file_path, caps in capabilities.items():
    for cap in caps:
        stats[cap.language] += 1

print(f"ç™¼ç¾èƒ½åŠ›ç¸½æ•¸: {sum(stats.values())}")
print(f"Python: {stats['python']}")
print(f"Go: {stats['go']}")
print(f"Rust: {stats['rust']}")
print(f"TypeScript: {stats['typescript']}")

# åˆ†æåˆç´„ä½¿ç”¨æƒ…æ³
contract_usage = {}
for file_path, caps in capabilities.items():
    for cap in caps:
        for contract in cap.uses_contracts:
            if contract not in contract_usage:
                contract_usage[contract] = []
            contract_usage[contract].append({
                "file": file_path,
                "function": cap.name,
                "language": cap.language
            })

print(f"\nä½¿ç”¨æœ€å¤šçš„åˆç´„:")
for contract, usages in sorted(contract_usage.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
    print(f"  {contract}: {len(usages)} æ¬¡")
```

---

### æ–¹æ¡ˆ 2: çµ±ä¸€æ•¸æ“šåˆç´„æ•´åˆ (P1)

#### ç›®æ¨™

å°‡ `core_schema_sot.yaml` å®šç¾©çš„çµ±ä¸€æ•¸æ“šåˆç´„æ•´åˆåˆ°æ¢ç´¢æµç¨‹ä¸­ã€‚

#### å¯¦ç¾æ­¥é©Ÿ

**Step 1: å¢å¼· Schema Manager**

```python
# enhanced_schema_manager.py

from unified_schema_manager import UnifiedSchemaManager
from typing import Dict, List, Any

class EnhancedSchemaManager(UnifiedSchemaManager):
    """å¢å¼·çš„ Schema ç®¡ç†å™¨ - æ”¯æ´èƒ½åŠ›æ˜ å°„"""
    
    def __init__(self):
        super().__init__()
        self.contract_index = self._build_contract_index()
    
    def _build_contract_index(self) -> Dict[str, Any]:
        """å»ºç«‹åˆç´„ç´¢å¼•"""
        index = {
            "schemas": {},
            "enums": {},
            "relationships": {}
        }
        
        # è¼‰å…¥æ‰€æœ‰ Schema
        schemas = self.list_schemas()
        for module_name, schema_list in schemas.items():
            for schema_name in schema_list:
                index["schemas"][schema_name] = {
                    "module": module_name,
                    "fields": self._get_schema_fields(module_name, schema_name),
                    "used_by": []
                }
        
        # è¼‰å…¥æ‰€æœ‰ Enum
        enums = self.list_enums()
        for module_name, enum_list in enums.items():
            for enum_name in enum_list:
                index["enums"][enum_name] = {
                    "module": module_name,
                    "values": self._get_enum_values(module_name, enum_name),
                    "used_by": []
                }
        
        return index
    
    def find_contract_for_type(self, type_str: str) -> Dict[str, Any] | None:
        """æ ¹æ“šé¡å‹å­—ä¸²æŸ¥æ‰¾å°æ‡‰çš„åˆç´„"""
        # ç§»é™¤æ³›å‹åƒæ•¸
        base_type = type_str.split('[')[0].split('<')[0].strip()
        
        if base_type in self.contract_index["schemas"]:
            return {
                "type": "schema",
                "name": base_type,
                **self.contract_index["schemas"][base_type]
            }
        
        if base_type in self.contract_index["enums"]:
            return {
                "type": "enum",
                "name": base_type,
                **self.contract_index["enums"][base_type]
            }
        
        return None
    
    def record_usage(self, contract_name: str, used_by: Dict[str, str]):
        """è¨˜éŒ„åˆç´„ä½¿ç”¨æƒ…æ³"""
        if contract_name in self.contract_index["schemas"]:
            self.contract_index["schemas"][contract_name]["used_by"].append(used_by)
        elif contract_name in self.contract_index["enums"]:
            self.contract_index["enums"][contract_name]["used_by"].append(used_by)
    
    def get_contract_usage_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆç´„ä½¿ç”¨å ±å‘Š"""
        report = {
            "total_schemas": len(self.contract_index["schemas"]),
            "total_enums": len(self.contract_index["enums"]),
            "most_used": [],
            "unused": []
        }
        
        # çµ±è¨ˆä½¿ç”¨æ¬¡æ•¸
        all_contracts = []
        
        for name, info in self.contract_index["schemas"].items():
            all_contracts.append({
                "name": name,
                "type": "schema",
                "usage_count": len(info["used_by"]),
                "used_by": info["used_by"]
            })
        
        for name, info in self.contract_index["enums"].items():
            all_contracts.append({
                "name": name,
                "type": "enum",
                "usage_count": len(info["used_by"]),
                "used_by": info["used_by"]
            })
        
        # æ’åº
        all_contracts.sort(key=lambda x: x["usage_count"], reverse=True)
        
        report["most_used"] = all_contracts[:20]
        report["unused"] = [c for c in all_contracts if c["usage_count"] == 0]
        
        return report
```

**Step 2: å°‡åˆç´„æ˜ å°„æ•´åˆåˆ°èƒ½åŠ›åˆ†æ**

```python
# åœ¨ MultiLanguageCapabilityAnalyzer ä¸­

async def analyze_with_contracts(self, file_path: str) -> List[UnifiedCapability]:
    """åˆ†ææ–‡ä»¶ä¸¦æ˜ å°„åˆç´„"""
    capabilities = await self.analyze_file(file_path)
    
    # ç‚ºæ¯å€‹èƒ½åŠ›æ·»åŠ åˆç´„æ˜ å°„
    for cap in capabilities:
        # æª¢æŸ¥è¼¸å…¥åƒæ•¸
        if cap.parameters:
            for param in cap.parameters:
                contract = self.schema_manager.find_contract_for_type(param["type"])
                if contract:
                    self.schema_manager.record_usage(
                        contract["name"],
                        {
                            "file": file_path,
                            "function": cap.name,
                            "language": cap.language,
                            "usage_type": "input_parameter"
                        }
                    )
        
        # æª¢æŸ¥è¿”å›å€¼
        if cap.return_type:
            contract = self.schema_manager.find_contract_for_type(cap.return_type)
            if contract:
                cap.output_contract = contract["name"]
                self.schema_manager.record_usage(
                    contract["name"],
                    {
                        "file": file_path,
                        "function": cap.name,
                        "language": cap.language,
                        "usage_type": "return_type"
                    }
                )
    
    return capabilities
```

---

### æ–¹æ¡ˆ 3: è·¨èªè¨€èª¿ç”¨è¿½è¹¤ (P2)

#### è­˜åˆ¥ HTTP/gRPC èª¿ç”¨

```python
# cross_language_call_detector.py

from dataclasses import dataclass
from typing import List, Dict
import re

@dataclass
class CrossLanguageCall:
    """è·¨èªè¨€èª¿ç”¨"""
    caller_file: str
    caller_language: str
    caller_function: str
    
    callee_service: str
    callee_endpoint: str
    call_type: str  # http, grpc, mq
    
    request_contract: str | None
    response_contract: str | None


class HTTPCallDetector:
    """HTTP èª¿ç”¨æª¢æ¸¬å™¨"""
    
    def detect_python_http_calls(self, content: str) -> List[Dict]:
        """æª¢æ¸¬ Python HTTP èª¿ç”¨"""
        calls = []
        
        # requests.post/get
        pattern1 = r'requests\.(post|get|put|delete)\s*\(\s*["\']([^"\']+)["\']'
        
        # aiohttp
        pattern2 = r'session\.(post|get|put|delete)\s*\(\s*["\']([^"\']+)["\']'
        
        # httpx
        pattern3 = r'httpx\.(post|get|put|delete)\s*\(\s*["\']([^"\']+)["\']'
        
        for pattern in [pattern1, pattern2, pattern3]:
            for match in re.finditer(pattern, content):
                method, url = match.groups()
                calls.append({
                    "method": method,
                    "url": url,
                    "type": "http"
                })
        
        return calls
    
    def detect_go_http_calls(self, content: str) -> List[Dict]:
        """æª¢æ¸¬ Go HTTP èª¿ç”¨"""
        calls = []
        
        # http.Post/Get
        pattern1 = r'http\.(Post|Get|Put|Delete)\s*\(\s*"([^"]+)"'
        
        # client.Do
        pattern2 = r'NewRequest\s*\(\s*"(\w+)"\s*,\s*"([^"]+)"'
        
        for pattern in [pattern1, pattern2]:
            for match in re.finditer(pattern, content):
                if pattern == pattern1:
                    method, url = match.groups()
                else:
                    method, url = match.groups()
                
                calls.append({
                    "method": method,
                    "url": url,
                    "type": "http"
                })
        
        return calls


class MQCallDetector:
    """MQ èª¿ç”¨æª¢æ¸¬å™¨"""
    
    def detect_python_mq_publishes(self, content: str) -> List[Dict]:
        """æª¢æ¸¬ Python MQ ç™¼å¸ƒ"""
        publishes = []
        
        # channel.basic_publish
        pattern = r'basic_publish\s*\([^)]*routing_key\s*=\s*["\']([^"\']+)["\']'
        
        for match in re.finditer(pattern, content):
            routing_key = match.group(1)
            publishes.append({
                "routing_key": routing_key,
                "type": "mq_publish"
            })
        
        return publishes
    
    def detect_go_mq_publishes(self, content: str) -> List[Dict]:
        """æª¢æ¸¬ Go MQ ç™¼å¸ƒ"""
        publishes = []
        
        # ch.Publish
        pattern = r'Publish\s*\([^)]*"([^"]+)"\s*,\s*"([^"]+)"'
        
        for match in re.finditer(pattern, content):
            exchange, routing_key = match.groups()
            publishes.append({
                "exchange": exchange,
                "routing_key": routing_key,
                "type": "mq_publish"
            })
        
        return publishes


class CrossLanguageCallAnalyzer:
    """è·¨èªè¨€èª¿ç”¨åˆ†æå™¨"""
    
    def __init__(self):
        self.http_detector = HTTPCallDetector()
        self.mq_detector = MQCallDetector()
        self.call_graph = {}
    
    async def analyze_calls(self, capabilities: Dict[str, List['UnifiedCapability']]) -> List[CrossLanguageCall]:
        """åˆ†ææ‰€æœ‰è·¨èªè¨€èª¿ç”¨"""
        all_calls = []
        
        for file_path, caps in capabilities.items():
            with open(file_path, 'r') as f:
                content = f.read()
            
            language = caps[0].language if caps else None
            
            if language == "python":
                http_calls = self.http_detector.detect_python_http_calls(content)
                mq_calls = self.mq_detector.detect_python_mq_publishes(content)
            elif language == "go":
                http_calls = self.http_detector.detect_go_http_calls(content)
                mq_calls = self.mq_detector.detect_go_mq_publishes(content)
            else:
                continue
            
            # è½‰æ›ç‚º CrossLanguageCall
            for call in http_calls:
                cross_call = CrossLanguageCall(
                    caller_file=file_path,
                    caller_language=language,
                    caller_function="",  # éœ€è¦æ›´ç²¾ç¢ºçš„å®šä½
                    callee_service=self._extract_service_name(call["url"]),
                    callee_endpoint=call["url"],
                    call_type="http",
                    request_contract=None,
                    response_contract=None
                )
                all_calls.append(cross_call)
        
        return all_calls
    
    def _extract_service_name(self, url: str) -> str:
        """å¾ URL æå–æœå‹™å"""
        # http://go-scanner:8080/scan -> go-scanner
        match = re.search(r'://([^:/]+)', url)
        if match:
            return match.group(1)
        return "unknown"
```

---

## ğŸ“‹ å®Œæ•´åŸ·è¡Œè¨ˆåŠƒ

### Phase 1: åŸºç¤è¨­æ–½ (1-2 é€±)

**ç›®æ¨™**: å»ºç«‹å¤šèªè¨€åˆ†æåŸºç¤è¨­æ–½

#### ä»»å‹™æ¸…å–®

- [ ] **Task 1.1**: å¯¦ç¾ `multi_language_capability_analyzer.py`
  - [ ] Python åˆ†æå™¨ (å·²æœ‰,éœ€å¢å¼·)
  - [ ] Go åˆ†æå™¨ (æ–°å¢)
  - [ ] Rust åˆ†æå™¨ (æ–°å¢)
  - [ ] TypeScript åˆ†æå™¨ (æ–°å¢)
  - [ ] çµ±ä¸€èƒ½åŠ›å…ƒæ•¸æ“šæ ¼å¼

- [ ] **Task 1.2**: å¢å¼· `unified_schema_manager.py`
  - [ ] å»ºç«‹åˆç´„ç´¢å¼•
  - [ ] å¯¦ç¾é¡å‹æŸ¥æ‰¾
  - [ ] è¿½è¹¤åˆç´„ä½¿ç”¨
  - [ ] ç”Ÿæˆä½¿ç”¨å ±å‘Š

- [ ] **Task 1.3**: å¯¦ç¾ `cross_language_call_detector.py`
  - [ ] HTTP èª¿ç”¨æª¢æ¸¬
  - [ ] gRPC èª¿ç”¨æª¢æ¸¬
  - [ ] MQ ç™¼å¸ƒ/è¨‚é–±æª¢æ¸¬
  - [ ] å»ºç«‹èª¿ç”¨åœ–

#### é©—æ”¶æ¨™æº–

```bash
# æ¸¬è©¦å¤šèªè¨€åˆ†æ
python scripts/ai_analysis/test_multi_language_analyzer.py

# é æœŸè¼¸å‡º:
# âœ… Python åˆ†æ: 405 å€‹èƒ½åŠ›
# âœ… Go åˆ†æ: 150+ å€‹èƒ½åŠ›
# âœ… Rust åˆ†æ: 80+ å€‹èƒ½åŠ›
# âœ… TypeScript åˆ†æ: 120+ å€‹èƒ½åŠ›
# âœ… ç¸½è¨ˆ: 755+ å€‹èƒ½åŠ›
```

---

### Phase 2: çŸ¥è­˜åœ–è­œæ§‹å»º (1 é€±)

**ç›®æ¨™**: å»ºç«‹å®Œæ•´çš„ç³»çµ±çŸ¥è­˜åœ–è­œ

#### ä»»å‹™æ¸…å–®

- [ ] **Task 2.1**: æ•´åˆå¤šèªè¨€èƒ½åŠ›åˆ° RAG
  - [ ] ä¿®æ”¹ `InternalLoopConnector` æ”¯æ´å¤šèªè¨€
  - [ ] æ›´æ–° `update_self_awareness.py`
  - [ ] ç”Ÿæˆçµ±ä¸€å‘é‡è¡¨ç¤º

- [ ] **Task 2.2**: å»ºç«‹è·¨èªè¨€ä¾è³´åœ–
  - [ ] åˆ†æ HTTP èª¿ç”¨éˆ
  - [ ] åˆ†æ MQ æ¶ˆæ¯æµ
  - [ ] è­˜åˆ¥åˆç´„ä½¿ç”¨é—œä¿‚
  - [ ] ç”Ÿæˆä¾è³´åœ–å¯è¦–åŒ–

- [ ] **Task 2.3**: åˆç´„æ˜ å°„èˆ‡é©—è­‰
  - [ ] è‡ªå‹•æ˜ å°„å‡½æ•¸åˆ°åˆç´„
  - [ ] é©—è­‰è·¨èªè¨€ä¸€è‡´æ€§
  - [ ] æª¢æ¸¬åˆç´„é•è¦

#### é©—æ”¶æ¨™æº–

```bash
# åŸ·è¡Œå®Œæ•´çŸ¥è­˜æ³¨å…¥
python scripts/internal_loop/update_self_awareness_v2.py

# é æœŸè¼¸å‡º:
# âœ… æ¢ç´¢ 430 å€‹æ–‡ä»¶
# âœ… æå– 755+ å€‹èƒ½åŠ›
# âœ… æ˜ å°„ 120+ å€‹åˆç´„ä½¿ç”¨
# âœ… è­˜åˆ¥ 85+ å€‹è·¨èªè¨€èª¿ç”¨
# âœ… æ³¨å…¥åˆ° RAG: 100% æˆåŠŸ
```

---

### Phase 3: AI è¨ºæ–·èˆ‡å„ªåŒ– (1 é€±)

**ç›®æ¨™**: è®“ AI è‡ªå‹•ç™¼ç¾å•é¡Œä¸¦çµ¦å‡ºå»ºè­°

#### ä»»å‹™æ¸…å–®

- [ ] **Task 3.1**: å¯¦ç¾æ™ºèƒ½å•é¡Œæª¢æ¸¬
  - [ ] åˆç´„ä¸ä¸€è‡´æª¢æ¸¬
  - [ ] æ­»ä»£ç¢¼æª¢æ¸¬
  - [ ] è·¨èªè¨€é¡å‹ä¸åŒ¹é…
  - [ ] ç¼ºå¤±æ–‡æª”æª¢æ¸¬

- [ ] **Task 3.2**: ç”Ÿæˆå„ªåŒ–å»ºè­°
  - [ ] ä»£ç¢¼é‡æ§‹å»ºè­°
  - [ ] åˆç´„è¦ç¯„å»ºè­°
  - [ ] æ¶æ§‹æ”¹é€²å»ºè­°
  - [ ] æ€§èƒ½å„ªåŒ–å»ºè­°

- [ ] **Task 3.3**: å»ºç«‹ AI æŸ¥è©¢æ¥å£
  - [ ] è‡ªç„¶èªè¨€æŸ¥è©¢åŠŸèƒ½
  - [ ] ä»£ç¢¼æœç´¢å¢å¼·
  - [ ] è·¨èªè¨€ç¤ºä¾‹ç”Ÿæˆ

#### é©—æ”¶æ¨™æº–

```python
# AI æŸ¥è©¢æ¸¬è©¦
from ai_enhanced_rag import AIEnhancedRAG

rag = AIEnhancedRAG()

# æ¸¬è©¦ 1: æŸ¥æ‰¾åŠŸèƒ½
result = rag.query("å¦‚ä½•ä½¿ç”¨ Go æœå‹™æƒæç›®æ¨™?")
# é æœŸ: è¿”å› Go scanner ç›¸é—œå‡½æ•¸åŠä½¿ç”¨ç¯„ä¾‹

# æ¸¬è©¦ 2: è·¨èªè¨€æŸ¥è©¢
result = rag.query("Python å¦‚ä½•èª¿ç”¨ Rust çš„é«˜æ€§èƒ½æ¨¡çµ„?")
# é æœŸ: è¿”å› FFI èª¿ç”¨æ–¹å¼åŠç¤ºä¾‹

# æ¸¬è©¦ 3: å•é¡Œæª¢æ¸¬
issues = rag.detect_issues()
# é æœŸ: è¿”å›æ¶æ§‹å•é¡Œåˆ—è¡¨
```

---

## ğŸ“Š é æœŸæˆæœ

### 1. å…¨å±€èƒ½åŠ›çŸ¥è­˜åº«

```
ç³»çµ±ç¸½èƒ½åŠ›: 755+
â”œâ”€ Python: 405 (53.6%)
â”œâ”€ Go: 150 (19.9%)
â”œâ”€ Rust: 80 (10.6%)
â””â”€ TypeScript: 120 (15.9%)

åˆç´„ä½¿ç”¨: 120+ è™•
â”œâ”€ TaskPayload: 45 è™•
â”œâ”€ ScanResult: 38 è™•
â”œâ”€ Finding: 28 è™•
â””â”€ å…¶ä»–: 9 è™•

è·¨èªè¨€èª¿ç”¨: 85+
â”œâ”€ HTTP: 52
â”œâ”€ MQ: 28
â””â”€ gRPC: 5
```

### 2. AI èƒ½åŠ›æå‡

**ç¾åœ¨ (Before)**:
```
User: "å¦‚ä½•æƒæ SQL æ³¨å…¥?"
AI: "ä½¿ç”¨ sqlmap_scanner.py çš„ scan_sql_injection å‡½æ•¸"
   âŒ ä¸çŸ¥é“æœ‰ Go çš„é«˜æ€§èƒ½ç‰ˆæœ¬
   âŒ ä¸çŸ¥é“åˆç´„å®šç¾©
```

**æœªä¾† (After)**:
```
User: "å¦‚ä½•æƒæ SQL æ³¨å…¥?"
AI: "ç³»çµ±æä¾› 3 ç¨®å¯¦ç¾:
     1. Python: attack_engine/sqlmap_scanner.py::scan_sql_injection()
        è¼¸å…¥: SQLInjectionTaskPayload
        è¼¸å‡º: FindingPayload[]
        
     2. Go: services/scan/sql_scanner.go::ScanSQL()
        æ€§èƒ½: æ¯” Python å¿« 3x
        é©åˆ: å¤§è¦æ¨¡æƒæ
        
     3. Rust: modules/sql_engine/lib.rs::fast_sql_scan()
        æ€§èƒ½: æ¯” Python å¿« 10x
        é©åˆ: æ¥µé«˜æ€§èƒ½éœ€æ±‚
        
     å»ºè­°: ä¸€èˆ¬ç”¨é€”ä½¿ç”¨ Python,å¤§è¦æ¨¡æƒæä½¿ç”¨ Go"
   âœ… çŸ¥é“æ‰€æœ‰å¯¦ç¾
   âœ… äº†è§£åˆç´„å®šç¾©
   âœ… çµ¦å‡ºæ€§èƒ½å°æ¯”
   âœ… æä¾›ä½¿ç”¨å»ºè­°
```

### 3. å•é¡Œè‡ªå‹•ç™¼ç¾

```
æª¢æ¸¬åˆ°çš„å•é¡Œ:
âœ… åˆç´„ä¸ä¸€è‡´: 12 è™•
   - Python ä½¿ç”¨ `target_url`, Go ä½¿ç”¨ `target` â†’ éœ€çµ±ä¸€

âœ… æœªä½¿ç”¨åˆç´„: 8 å€‹ Schema
   - SecurityHeader Schema æœªè¢«ä»»ä½•æ¨¡çµ„ä½¿ç”¨ â†’ å¯ç§»é™¤

âœ… è·¨èªè¨€é¡å‹ä¸åŒ¹é…: 5 è™•
   - Python int vs Go int64 â†’ éœ€æ˜ç¢ºç¯„åœ

âœ… æ­»ä»£ç¢¼: 23 å€‹å‡½æ•¸
   - old_scanner.py ä¸­çš„å‡½æ•¸æœªè¢«èª¿ç”¨ â†’ å¯æ¸…ç†

âœ… ç¼ºå¤±æ–‡æª”: 145 å€‹å‡½æ•¸
   - 85% çš„ Go å‡½æ•¸ç¼ºå°‘è¨»è§£ â†’ éœ€è£œå……
```

---

## ğŸš€ ç«‹å³è¡Œå‹•

### æœ¬é€±ä»»å‹™ (Week 1)

1. **Day 1-2**: å¯¦ç¾ Go èƒ½åŠ›åˆ†æå™¨
2. **Day 3-4**: å¯¦ç¾ Rust èƒ½åŠ›åˆ†æå™¨
3. **Day 5**: å¯¦ç¾ TypeScript èƒ½åŠ›åˆ†æå™¨
4. **Day 6-7**: æ•´åˆæ¸¬è©¦ + æ–‡æª”

### ä¸‹é€±ä»»å‹™ (Week 2)

1. **Day 8-10**: å¢å¼· Schema Manager
2. **Day 11-12**: å¯¦ç¾è·¨èªè¨€èª¿ç”¨æª¢æ¸¬
3. **Day 13-14**: æ•´åˆåˆ° RAG ç³»çµ±

### ç¬¬ä¸‰é€± (Week 3)

1. **Day 15-17**: å¯¦ç¾æ™ºèƒ½è¨ºæ–·
2. **Day 18-19**: AI æŸ¥è©¢æ¥å£
3. **Day 20-21**: å®Œæ•´æ¸¬è©¦èˆ‡å„ªåŒ–

---

## ğŸ“š åƒè€ƒæ–‡æª”

- [AIVA çµ±ä¸€é€šä¿¡æ¶æ§‹æŠ€è¡“æ•´åˆæŒ‡å—](../guides/contracts/AIVA_çµ±ä¸€é€šä¿¡æ¶æ§‹æŠ€è¡“æ•´åˆæŒ‡å—.md)
- [AIVA Common é–‹ç™¼è¦ç¯„](../services/aiva_common/README.md)
- [å…§é–‰ç’°åŸ·è¡Œåˆ†æå ±å‘Š](./INTERNAL_LOOP_EXECUTION_ANALYSIS.md)
- [èªè¨€è¦†è“‹å•é¡Œåˆ†æ](./INTERNAL_LOOP_LANGUAGE_COVERAGE_AND_ISSUES_ANALYSIS.md)
- [AI è‡ªæˆ‘å„ªåŒ–é›™é‡é–‰ç’°è¨­è¨ˆ](../AI_SELF_OPTIMIZATION_DUAL_LOOP_DESIGN.md)

---

**å»ºç«‹è€…**: GitHub Copilot  
**å¯©æ ¸è€…**: AIVA Development Team  
**ç‹€æ…‹**: å¾…åŸ·è¡Œ  
**å„ªå…ˆç´š**: P0 - Critical
